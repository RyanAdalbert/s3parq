{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-08 18:51:26,128 - core.helpers.session_helper.SessionHelper - INFO - Creating session for dev environment...\n",
      "2019-08-08 18:51:26,150 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating administrator mocks.\n",
      "2019-08-08 18:51:26,183 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating administrator mocks.\n",
      "2019-08-08 18:51:26,184 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pharmaceutical company mocks.\n",
      "2019-08-08 18:51:26,188 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pharmaceutical company mocks.\n",
      "2019-08-08 18:51:26,189 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating brand mocks.\n",
      "2019-08-08 18:51:26,191 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating brand mocks.\n",
      "2019-08-08 18:51:26,192 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating segment mocks.\n",
      "2019-08-08 18:51:26,195 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating segment mocks.\n",
      "2019-08-08 18:51:26,196 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline type mocks.\n",
      "2019-08-08 18:51:26,200 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline type mocks.\n",
      "2019-08-08 18:51:26,201 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline mocks.\n",
      "2019-08-08 18:51:26,206 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline mocks.\n",
      "2019-08-08 18:51:26,208 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline state type mocks.\n",
      "2019-08-08 18:51:26,211 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline state type mocks.\n",
      "2019-08-08 18:51:26,212 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline state mocks.\n",
      "2019-08-08 18:51:26,217 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline state mocks.\n",
      "2019-08-08 18:51:26,218 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating tag mocks.\n",
      "2019-08-08 18:51:26,221 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating tag mocks.\n",
      "2019-08-08 18:51:26,223 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation_template mocks.\n",
      "2019-08-08 18:51:26,225 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_template mocks.\n",
      "2019-08-08 18:51:26,226 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating bridge table mocks for transformation_templates <=> tags.\n",
      "2019-08-08 18:51:26,229 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_templates_tags mocks.\n",
      "2019-08-08 18:51:26,231 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation mocks.\n",
      "2019-08-08 18:51:26,235 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation mocks.\n",
      "2019-08-08 18:51:26,237 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation_variables mocks\n",
      "2019-08-08 18:51:26,240 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_variables mocks.\n",
      "2019-08-08 18:51:26,241 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating run_events mocks.\n",
      "2019-08-08 18:51:26,245 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating run_events mocks.\n",
      "2019-08-08 18:51:26,246 - core.helpers.session_helper.SessionHelper - INFO - Done. Created dev session with mock data.\n"
     ]
    }
   ],
   "source": [
    "from core.helpers.session_helper import SessionHelper\n",
    "session = SessionHelper().session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ CONFIGURATION - PLEASE TOUCH **************\n",
    "Pipeline Builder configuration: creates configurations from variables specified here!!\n",
    "This cell will be off in production as configurations will come from the configuration postgres DB.\n",
    "\"\"\"\n",
    "# config vars: this dataset\n",
    "config_pharma = \"sun\" # the pharmaceutical company which owns {brand}\n",
    "config_brand = \"ilumya\" # the brand this pipeline operates on\n",
    "config_state = \"enrich\" # the state this transform runs in\n",
    "config_name = \"fill_rate_schema\" # the name of this transform, which is the name of this notebook without .ipynb\n",
    "\n",
    "# input vars: dataset to fetch. Recall that a contract published to S3 has a key format branch/pharma/brand/state/name\n",
    "input_pharma = \"sun\"\n",
    "input_brand = \"ilumya\"\n",
    "input_state = \"ingest\"\n",
    "input_name = \"symphony_health_association_ingest_column_mapping\"\n",
    "input_branch = \"sun-extract-validation\" # if None, input_branch is automagically set to your working branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-08 18:51:26,292 - core.logging - DEBUG - Adding/getting mocks for specified configurations...\n",
      "2019-08-08 18:51:26,321 - core.logging - DEBUG - Done. Creating mock run event and committing results to configuration mocker.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "Populating config mocker based on config parameters...\n",
    "\"\"\"\n",
    "import core.helpers.pipeline_builder as builder\n",
    "\n",
    "ids = builder.build(config_pharma, config_brand, config_state, config_name, session)\n",
    "transform_id = ids[0]\n",
    "run_id = ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "This section imports data from the configuration database\n",
    "and should not need to be altered or otherwise messed with. \n",
    "~~These are not the droids you are looking for~~\n",
    "\"\"\"\n",
    "from core.constants import BRANCH_NAME, ENV_BUCKET\n",
    "from core.models.configuration import Transformation\n",
    "from dataclasses import dataclass\n",
    "from core.dataset_contract import DatasetContract\n",
    "\n",
    "db_transform = session.query(Transformation).filter(Transformation.id == transform_id).one()\n",
    "\n",
    "@dataclass\n",
    "class DbTransform:\n",
    "    id: int = db_transform.id ## the instance id of the transform in the config app\n",
    "    name: str = db_transform.transformation_template.name ## the transform name in the config app\n",
    "    state: str = db_transform.pipeline_state.pipeline_state_type.name ## the pipeline state, one of raw, ingest, master, enhance, enrich, metrics, dimensional\n",
    "    branch:str = BRANCH_NAME ## the git branch for this execution \n",
    "    brand: str = db_transform.pipeline_state.pipeline.brand.name ## the pharma brand name\n",
    "    pharmaceutical_company: str = db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name # the pharma company name\n",
    "    publish_contract: DatasetContract = DatasetContract(branch=BRANCH_NAME,\n",
    "                            state=db_transform.pipeline_state.pipeline_state_type.name,\n",
    "                            parent=db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name,\n",
    "                            child=db_transform.pipeline_state.pipeline.brand.name,\n",
    "                            dataset=db_transform.transformation_template.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE Cartridge Notebook::[transform name here]\n",
    "![CORE Logo](assets/coreLogo.png) \n",
    "\n",
    "---\n",
    "## Keep in Mind\n",
    "Good Transforms Are...\n",
    "- **singular in purpose:** good transforms do one and only one thing, and handle all known cases for that thing. \n",
    "- **repeatable:** transforms should be written in a way that they can be run against the same dataset an infinate number of times and get the same result every time. \n",
    "- **easy to read:** 99 times out of 100, readable, clear code that runs a little slower is more valuable than a mess that runs quickly. \n",
    "- **No 'magic numbers':** if a variable or function is not instantly obvious as to what it is or does, without context, maybe consider renaming it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow - how to use this notebook to make science\n",
    "#### Data Science\n",
    "1. **Document your transform.** Fill out the _description_ cell below describing what it is this transform does; this will appear in the configuration application where Ops will create, configure and update pipelines. \n",
    "1. **Define your config object.** Fill out the _configuration_ cell below the commented-out guide to define the variables you want ops to set in the configuration application (these will populate here for every pipeline). \n",
    "2. **Build your transformation logic.** Use the transformation cell to do that magic that you do. \n",
    "![caution](assets/cautionTape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "********* VARIABLES - PLEASE TOUCH ********* \n",
    "This section defines what you expect to get from the configuration application \n",
    "in a single \"transform\" object. Define the vars you need here, and comment inline to the right of them \n",
    "for all-in-one documentation. \n",
    "Engineering will build a production \"transform\" object for every pipeline that matches what you define here.\n",
    "\n",
    "@@@ FORMAT OF THE DATA CLASS IS: @@@ \n",
    "\n",
    "<variable_name>: <data_type> #<comment explaining what the value is to future us>\n",
    "\n",
    "e.g.\n",
    "\n",
    "class Transform(DbTransform):\n",
    "    some_ratio: float\n",
    "    site_name: str\n",
    "\n",
    "~~These ARE the droids you are looking for~~\n",
    "\"\"\"\n",
    "\n",
    "class Transform(DbTransform):\n",
    "    '''\n",
    "    YOUR properties go here!!\n",
    "    Variable properties should be assigned to the exact name of\n",
    "    the transformation as it appears in the Jupyter notebook filename.\n",
    "    '''\n",
    "    \n",
    "    pat_id: str # Patiend ID column\n",
    "    pharm_name: str # Pharmacy Name column\n",
    "    brand_id: str # Brand Name column\n",
    "    status_date: str # Status Date column\n",
    "    ref_date: str # Referral Date column\n",
    "    transaction_id: str # Transaction ID column\n",
    "    ref_source: str # Referral Source column\n",
    "    provider_fn: str # Provider First Name column\n",
    "    provider_ln: str # Provider Last Name column\n",
    "    provider_s: str # Provider State column\n",
    "    payer: str # Payer Name column\n",
    "    payer_type: str # Payer Type column\n",
    "    shipment_status: str # string of shipment status. Should be something like 'SHIPMENT'\n",
    "    transfered_status: str # string of transfered status. Should be something like 'TRANSFERRED'\n",
    "    cancelled_status: str # string of cancelled status. Should be something like 'CANCELLED'\n",
    "    ic_status: str # column name of integrichain status\n",
    "    ic_sub_status: str # column name of integrichain sub status\n",
    "    pjh: str # column name of patient journey heirarchy\n",
    "    shipdate: str # string of shipment date. Should be something like 'shipdate'\n",
    "    firstfilldate: str # string of first fill date. Should be something like 'firstfilldate'\n",
    "    canceldate: str # string of cancel date. Should be something like 'canceldate'\n",
    "    cancelsubstatus: str # string of cancel sub status. Should be something like 'cancelsubstatus'\n",
    "    cancelreason: str # string of cancel reason. Should be something like 'cancelreason'\n",
    "    referralstatus: str # string of referral status. Should be something like 'referralstatus'\n",
    "    filled: str # column name for created one hot column if PJ has been filled. Should be something like 'filled'\n",
    "    transferred: str # column name for created one hot column if PJ has been transferred. Should be something like 'transferred'\n",
    "    cancelled: str # column name for created one hot column if PJ has been cancelled. Should be something like 'cancelled'\n",
    "    stillopen: str # column name for created one hot column if PJ is still open. Should be something like 'stillopen'\n",
    "    \n",
    "    \n",
    "    def fill_rate_schema(self,df):\n",
    "        \n",
    "        col_ids = [self.pat_id, self.pharm_name, self.brand_id]\n",
    "        \n",
    "        df = df.sort_values([self.pat_id, self.pharm_name, self.brand_id, self.status_date],ascending=[True, True, True, True])\n",
    "        \n",
    "        df, max_status_date = self._return_groupby_max(df,col_ids,self.status_date)\n",
    "        \n",
    "        df['adjusted_transaction_id'] = df[transform.transaction_id].str.extract(r'(\\d+$)')\n",
    "        df['adjusted_transaction_id'] = df.adjusted_transaction_id.apply(lambda x: x[0:19])\n",
    "        df['adjusted_transaction_id'] = pd.to_numeric(df.adjusted_transaction_id)\n",
    "        \n",
    "        df['max_date'] = np.where(df[self.status_date] == df[max_status_date],1,0)\n",
    "        \n",
    "        df = self._return_groupby_min_status_date(df,[self.pat_id, self.pharm_name, self.brand_id, self.ic_sub_status],'min_status_date')\n",
    "        \n",
    "        df = df[df.max_date == 1][[self.pat_id, self.pharm_name, self.brand_id, self.transaction_id, 'adjusted_transaction_id', 'min_status_date',\n",
    "                                   self.status_date, self.ref_source, self.provider_fn, self.provider_ln, self.provider_s, self.payer, self.payer_type, \n",
    "                                   self.ref_date, self.ic_status, self.ic_sub_status, self.pjh]].drop_duplicates()\n",
    "        \n",
    "        df, max_transaction_id = self._return_groupby_max(df,col_ids,'adjusted_transaction_id')\n",
    "        \n",
    "        df['max_transaction'] = np.where(df.adjusted_transaction_id == df[max_transaction_id],1,0)\n",
    "        \n",
    "        df = df[df.max_transaction == 1].drop(labels=['max_transaction',max_transaction_id,'adjusted_transaction_id'],axis=1)\n",
    "        \n",
    "        df[self.filled] = np.where((df[self.ic_sub_status] == self.shipment_status),1,0)\n",
    "        df[self.transferred] = np.where((df[self.pjh] == self.transfered_status),1,0)\n",
    "        df[self.cancelled] = np.where((df[self.ic_status] == self.cancelled_status) & (df[self.pjh] != self.transfered_status),1,0)\n",
    "        df[self.stillopen] = np.where((df.filled == 0) & (df.transferred == 0) & (df.cancelled == 0),1,0)\n",
    "        \n",
    "        df = df.drop_duplicates(subset=[self.pat_id, self.pharm_name, self.brand_id, self.filled, self.transferred, self.cancelled, self.stillopen]).reset_index(drop=True)\n",
    "        \n",
    "        df.loc[df[self.filled] == 1,'firstfilldate'] = df['min_status_date']\n",
    "        df['firstfilldate'] = pd.to_datetime(df['firstfilldate'])\n",
    "        df.loc[:,'shipdate'] = df['firstfilldate']\n",
    "\n",
    "        df.loc[df[self.cancelled] == 1,'canceldate'] = df['min_status_date']\n",
    "        df['canceldate'] = pd.to_datetime(df['canceldate'])\n",
    "        \n",
    "        df.loc[df.filled == 1, self.referralstatus] = 'FILLED'\n",
    "        df.loc[df.cancelled == 1, self.referralstatus] = 'CANCELLED'\n",
    "        df.loc[(df.transferred == 1) | (df.stillopen == 1), self.referralstatus] = ' OPEN'\n",
    "        \n",
    "        df.loc[df.cancelled == 1,self.cancelsubstatus] = df[transform.ic_sub_status]\n",
    "        df.loc[df.cancelled == 1,self.cancelreason] = df[transform.pjh]\n",
    "        \n",
    "        df = df[[self.transaction_id,self.pat_id, self.pharm_name, self.brand_id, self.ref_source, self.provider_fn, self.provider_ln, self.provider_s, self.payer, \n",
    "                 self.payer_type, self.ic_status, self.ic_sub_status, self.status_date, self.ref_date, self.shipdate, self.firstfilldate, self.canceldate, self.cancelsubstatus, \n",
    "                 self.cancelreason, self.referralstatus, self.filled, self.transferred, self.cancelled, self.stillopen]]\n",
    "        \n",
    "        # This needs to be the last change made before returning.\n",
    "        df = df.rename(columns={self.transaction_id:'transactionid', self.pat_id:'patientid', self.pharm_name:'pharmname', self.brand_id:'brand', self.ref_source:'ref_source', \n",
    "                                self.provider_fn:'providerfirstname', self.provider_ln:'providerlastname', self.provider_s:'providerstate', self.payer:'payer', self.payer_type:'payertype',\n",
    "                                self.ic_status:'statuscode', self.ic_sub_status:'substatus', self.status_date:'statusdate', self.ref_date:'referraldate'})\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def _return_groupby_min_status_date(self,df,group_by_cols,col_rename):\n",
    "        min_date = (\n",
    "            df\n",
    "            .groupby(group_by_cols)\n",
    "            [self.status_date]\n",
    "            .min()\n",
    "            .reset_index(drop=False)\n",
    "            .rename(columns={self.status_date:col_rename})\n",
    "        )\n",
    "\n",
    "        df = pd.merge(df,min_date,on=group_by_cols,how='left')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _return_groupby_max(df,group_by_cols,series):\n",
    "        \n",
    "        max_col_name = 'max_{0}'.format(series)\n",
    "        \n",
    "        max_df = (\n",
    "            df\n",
    "            .groupby(group_by_cols)\n",
    "            [series]\n",
    "            .max()\n",
    "            .reset_index(drop=False)\n",
    "            .rename(columns={series:max_col_name})\n",
    "        )\n",
    "        \n",
    "        return pd.merge(df,max_df,on=group_by_cols), max_col_name\n",
    "\n",
    "\n",
    "transform = Transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform.pat_id = 'msa_patient_id'\n",
    "transform.pharm_name = 'pharm_code'\n",
    "transform.brand_id = 'medication'\n",
    "transform.status_date = 'status_date'\n",
    "transform.ref_date = 'ref_date'\n",
    "transform.transaction_id = 'pharm_transaction_id'\n",
    "transform.ref_source = 'ref_source'\n",
    "transform.provider_fn = 'pres_first_name'\n",
    "transform.provider_ln = 'pres_last_name'\n",
    "transform.provider_s = 'pres_state'\n",
    "transform.payer = 'primary_payer_name'\n",
    "transform.payer_type = 'primary_payer_type'\n",
    "transform.shipment_status = 'SHIPMENT'\n",
    "transform.transfered_status = 'TRANSFERRED'\n",
    "transform.cancelled_status = 'CANCELLED'\n",
    "transform.ic_status = 'integrichain_status'\n",
    "transform.ic_sub_status = 'integrichain_sub_status'\n",
    "transform.pjh = 'Patient_Journey_Hierarchy'\n",
    "transform.shipdate = 'shipdate'\n",
    "transform.firstfilldate = 'firstfilldate'\n",
    "transform.canceldate = 'canceldate'\n",
    "transform.cancelsubstatus = 'cancelsubstatus'\n",
    "transform.cancelreason = 'cancelreason'\n",
    "transform.referralstatus = 'referralstatus'\n",
    "transform.filled = 'filled'\n",
    "transform.transferred = 'transferred'\n",
    "transform.cancelled = 'cancelled'\n",
    "transform.stillopen = 'stillopen'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "What does this transformation do? be specific.\n",
    "\n",
    "![what does your transform do](assets/what.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fill Rate Schema](assets/FillRateSchema.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-08 19:46:06,747 - core.dataset_contract.DatasetContract - INFO - Fetching dataframe from s3 location s3://ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "************ FETCH DATA - TOUCH, BUT CAREFULLY **************\n",
    "This cell will be turned off in production, as the input_contract will be handled by the pipeline.\n",
    "\"\"\"\n",
    "\n",
    "if not input_branch:\n",
    "    input_branch = BRANCH_NAME\n",
    "input_contract = DatasetContract(branch=input_branch, state=input_state, parent=input_pharma, child=input_brand, dataset=input_name)\n",
    "run_filter = []\n",
    "# run_filter.append(dict(partition=\"run_id\", comparison=\"==\", values=[1]))\n",
    "# IF YOU HAVE PUBLISHED DATA MULTIPLE TIMES, uncomment the above line and change the int to the run_id to fetch.\n",
    "# Otherwise, you will have duplicate values in your fetched dataset!\n",
    "df = input_contract.fetch(filters=run_filter)\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df.status_date = df.status_date.str[:8].astype(str)\n",
    "df.ref_date = df.ref_date.str[:8].astype(str)\n",
    "\n",
    "df.status_date = pd.to_datetime(df.status_date, infer_datetime_format=True, errors='coerce')\n",
    "df.ref_date = pd.to_datetime(df.ref_date, infer_datetime_format=True, errors='coerce')\n",
    "\n",
    "os.chdir('{}'.format(os.path.expanduser('~')))\n",
    "status_config = pd.read_csv('status_mapping.csv')\n",
    "\n",
    "status_config.loc[:,'statusCode'] = status_config.statusCode.str.upper()\n",
    "status_config.loc[:,'subStatus'] = status_config.subStatus.str.upper()\n",
    "status_config.loc[:,'integrichain_sub_status'] = status_config.integrichain_sub_status.str.upper()\n",
    "status_config.loc[:,'integrichain_status'] = status_config.integrichain_status.str.upper()\n",
    "status_config.loc[:,'Patient_Journey_Hierarchy'] = status_config.Patient_Journey_Hierarchy.str.upper()\n",
    "\n",
    "status_config = status_config.rename(columns={'statusCode':'status_code','subStatus':'sub_status'})\n",
    "\n",
    "df.sub_status = df.sub_status.str.replace('PRESCRIBERHOLD','PRESCRIBER HOLD')\n",
    "\n",
    "df = pd.merge(df,status_config,on=['status_code','sub_status'])\n",
    "\n",
    "df = df[['rec_date', 'pharm_code', 'pharm_npi', 'transtype', 'pharm_transaction_id', 'trans_seq', 'ref_source', 'ref_date', 'program_id', 'pharmacy_id', 'pat_last_name', 'pat_first_name', 'pat_dob', 'pat_gender', \n",
    "         'pat_addr1', 'pat_addr2', 'pat_city', 'pat_state', 'pat_zip', 'dx1_code', 'dx2_code', 'status_date', 'status_code', 'sub_status', 'integrichain_status','integrichain_sub_status', 'Patient_Journey_Hierarchy', \n",
    "         'pres_last_name', 'pres_first_name', 'pres_addr1', 'pres_addr2', 'pres_city', 'pres_state', 'pres_zip', 'pres_phone', 'pres_npi', 'pres_dea', 'facility_name', 'rxdate', 'rxnumber', 'rxrefills', 'rxfill', \n",
    "         'refill_remaining', 'prev_disp', 'rx_ndc_number', 'medication', 'quantity', 'day_supply', 'ship_date', 'ship_carrier', 'shiptracking_num', 'ship_location', 'ship_address', 'ship_city', 'ship_state', 'ship_zip', \n",
    "         'has_medical', 'primary_coverage_type', 'primary_payer_name', 'primary_payer_type', 'secondary_coverage_type', 'secondary_payer_name', 'secondary_payer_type', 'plan_paid_amt', 'pat_copay', 'copay_assist_amount', \n",
    "         'oth_payer_amt', 'xfer_pharmname', 'msa_patient_id', 'msa_patient_bmap', '__metadata_run_timestamp', '__metadata_app_version', '__metadata_output_contract', '__metadata_transform_timestamp', '__metadata_run_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use the variables above to execute your transformation. the final output needs to be a variable named final_dataframe\n",
    "final_dataframe = transform.fill_rate_schema(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['transactionid', 'patientid', 'pharmname', 'brand', 'ref_source',\n",
       "       'providerfirstname', 'providerlastname', 'providerstate', 'payer',\n",
       "       'payertype', 'statuscode', 'substatus', 'statusdate', 'referraldate',\n",
       "       'shipdate', 'firstfilldate', 'canceldate', 'cancelsubstatus',\n",
       "       'cancelreason', 'referralstatus', 'filled', 'transferred', 'cancelled',\n",
       "       'stillopen'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transactionid</th>\n",
       "      <th>patientid</th>\n",
       "      <th>pharmname</th>\n",
       "      <th>brand</th>\n",
       "      <th>ref_source</th>\n",
       "      <th>providerfirstname</th>\n",
       "      <th>providerlastname</th>\n",
       "      <th>providerstate</th>\n",
       "      <th>payer</th>\n",
       "      <th>payertype</th>\n",
       "      <th>statuscode</th>\n",
       "      <th>substatus</th>\n",
       "      <th>statusdate</th>\n",
       "      <th>referraldate</th>\n",
       "      <th>shipdate</th>\n",
       "      <th>firstfilldate</th>\n",
       "      <th>canceldate</th>\n",
       "      <th>cancelsubstatus</th>\n",
       "      <th>cancelreason</th>\n",
       "      <th>referralstatus</th>\n",
       "      <th>filled</th>\n",
       "      <th>transferred</th>\n",
       "      <th>cancelled</th>\n",
       "      <th>stillopen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>183711690</td>\n",
       "      <td>2120001</td>\n",
       "      <td>CVS</td>\n",
       "      <td>ILUMYA SD PFS</td>\n",
       "      <td>HUB</td>\n",
       "      <td>TOMAS</td>\n",
       "      <td>CHAO</td>\n",
       "      <td>GA</td>\n",
       "      <td>None</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>SHIPMENT</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>2018-10-19</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FILLED</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>901177673820190405000000</td>\n",
       "      <td>2120006</td>\n",
       "      <td>CVS</td>\n",
       "      <td>ILUMYA SD PFS</td>\n",
       "      <td>DIRECT</td>\n",
       "      <td>RUSSELL</td>\n",
       "      <td>COHEN</td>\n",
       "      <td>NY</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CANCELLED</td>\n",
       "      <td>PATIENT RESPONSE</td>\n",
       "      <td>2019-04-05</td>\n",
       "      <td>2019-03-15</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-04-05</td>\n",
       "      <td>PATIENT RESPONSE</td>\n",
       "      <td>PATIENT</td>\n",
       "      <td>CANCELLED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BRIOVARX_20190306_118503541</td>\n",
       "      <td>2120009</td>\n",
       "      <td>BRV</td>\n",
       "      <td>ILUMYA 100MG/ML PFS INJ</td>\n",
       "      <td>HUB</td>\n",
       "      <td>ROBERT</td>\n",
       "      <td>GREENBERG</td>\n",
       "      <td>CA</td>\n",
       "      <td>UHC E AND I</td>\n",
       "      <td>COMMERCIAL</td>\n",
       "      <td>PENDING</td>\n",
       "      <td>BENEFITS</td>\n",
       "      <td>2019-03-05</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BRIOVARX_20190517_112180852</td>\n",
       "      <td>2120012</td>\n",
       "      <td>BRV</td>\n",
       "      <td>ILUMYA 100MG/ML PFS INJ</td>\n",
       "      <td>DIRECT</td>\n",
       "      <td>PAUL</td>\n",
       "      <td>MALLARI</td>\n",
       "      <td>MA</td>\n",
       "      <td>CAREMARK</td>\n",
       "      <td>MEDICARE D</td>\n",
       "      <td>CANCELLED</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>2019-05-16</td>\n",
       "      <td>2018-11-12</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-12-18</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>PROVIDER</td>\n",
       "      <td>CANCELLED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRIOVARX_20181128_113184881</td>\n",
       "      <td>2120024</td>\n",
       "      <td>BRV</td>\n",
       "      <td>ILUMYA 100MG/ML PFS INJ</td>\n",
       "      <td>DIRECT</td>\n",
       "      <td>GENA</td>\n",
       "      <td>GORE</td>\n",
       "      <td>IN</td>\n",
       "      <td>UHC E AND I</td>\n",
       "      <td>COMMERCIAL</td>\n",
       "      <td>PENDING</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>2018-11-27</td>\n",
       "      <td>2018-11-20</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 transactionid patientid pharmname                    brand  \\\n",
       "0                    183711690   2120001       CVS            ILUMYA SD PFS   \n",
       "1     901177673820190405000000   2120006       CVS            ILUMYA SD PFS   \n",
       "2  BRIOVARX_20190306_118503541   2120009       BRV  ILUMYA 100MG/ML PFS INJ   \n",
       "3  BRIOVARX_20190517_112180852   2120012       BRV  ILUMYA 100MG/ML PFS INJ   \n",
       "4  BRIOVARX_20181128_113184881   2120024       BRV  ILUMYA 100MG/ML PFS INJ   \n",
       "\n",
       "  ref_source providerfirstname providerlastname providerstate        payer  \\\n",
       "0        HUB             TOMAS             CHAO            GA         None   \n",
       "1     DIRECT           RUSSELL            COHEN            NY         None   \n",
       "2        HUB            ROBERT        GREENBERG            CA  UHC E AND I   \n",
       "3     DIRECT              PAUL          MALLARI            MA     CAREMARK   \n",
       "4     DIRECT              GENA             GORE            IN  UHC E AND I   \n",
       "\n",
       "    payertype statuscode         substatus statusdate referraldate   shipdate  \\\n",
       "0       OTHER     ACTIVE          SHIPMENT 2018-12-31   2018-10-19 2018-12-31   \n",
       "1        None  CANCELLED  PATIENT RESPONSE 2019-04-05   2019-03-15        NaT   \n",
       "2  COMMERCIAL    PENDING          BENEFITS 2019-03-05   2019-01-03        NaT   \n",
       "3  MEDICARE D  CANCELLED             OTHER 2019-05-16   2018-11-12        NaT   \n",
       "4  COMMERCIAL    PENDING             OTHER 2018-11-27   2018-11-20        NaT   \n",
       "\n",
       "  firstfilldate canceldate   cancelsubstatus cancelreason referralstatus  \\\n",
       "0    2018-12-31        NaT               NaN          NaN         FILLED   \n",
       "1           NaT 2019-04-05  PATIENT RESPONSE      PATIENT      CANCELLED   \n",
       "2           NaT        NaT               NaN          NaN           OPEN   \n",
       "3           NaT 2018-12-18             OTHER     PROVIDER      CANCELLED   \n",
       "4           NaT        NaT               NaN          NaN           OPEN   \n",
       "\n",
       "   filled  transferred  cancelled  stillopen  \n",
       "0       1            0          0          0  \n",
       "1       0            0          1          0  \n",
       "2       0            0          0          1  \n",
       "3       0            0          1          0  \n",
       "4       0            0          0          1  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataframe.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Missing columns\n",
    "\n",
    "statuscode: ic_status\n",
    "substatus: ic_sub_status\n",
    "statusdate: status date\n",
    "referraldate: referral date\n",
    "shipdate: Check with Lucas. Assumed latest shipment date\n",
    "firstfilldate: Check with Lucas. Assumed first shipment date\n",
    "canceldate: Check with Lucas. Date that PJH was cancelled\n",
    "cancelsubstatus: ic_sub_status\n",
    "cancelreason: PJH\n",
    "referralstatus: 'FILLED' 'CANCELLED' 'OPEN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_duplicates_exist (__main__.TestNotebook) ... ERROR\n",
      "test_no_multiple_types (__main__.TestNotebook) ... ok\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_duplicates_exist (__main__.TestNotebook)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-86-de976a93bf77>\", line 15, in test_duplicates_exist\n",
      "    self.assertEqual(duplicates_exist(final_dataframe),True)\n",
      "  File \"<ipython-input-86-de976a93bf77>\", line 7, in duplicates_exist\n",
      "    return df[col_ids].drop_duplicates().shape[0] == df.shape[0]\n",
      "NameError: name 'col_ids' is not defined\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.008s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f30859a4d68>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "def no_multiple_types(df):\n",
    "    return df[df.filled + df.transferred + df.cancelled + df.stillopen != 1].shape[0] == 0\n",
    "\n",
    "def duplicates_exist(df):\n",
    "    return df[col_ids].drop_duplicates().shape[0] == df.shape[0]\n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "    \n",
    "    def test_no_multiple_types(self):\n",
    "        self.assertEqual(no_multiple_types(final_dataframe),True)\n",
    "        \n",
    "    def test_duplicates_exist(self):\n",
    "        self.assertEqual(duplicates_exist(final_dataframe),True)\n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## that's it - just provide the final dataframe to the var final_dataframe and we take it from there\n",
    "transform.publish_contract.publish(final_dataframe, run_id, session)\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
