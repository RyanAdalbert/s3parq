{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.helpers.session_helper import SessionHelper\n",
    "session = SessionHelper().session"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "************ CONFIGURATION - PLEASE TOUCH **************\n",
    "Pipeline Builder configuration: creates configurations from variables specified here!!\n",
    "This cell will be off in production as configurations will come from the configuration postgres DB.\n",
    "\"\"\"\n",
    "# config vars: this dataset\n",
    "config_pharma = \"sun\" # the pharmaceutical company which owns {brand}\n",
    "config_brand = \"ilumya\" # the brand this pipeline operates on\n",
    "config_state = \"ingest\" # the state this transform runs in\n",
    "config_name = \"patient_status_date_standardization\" # the name of this transform, which is the name of this notebook without .ipynb\n",
    "\n",
    "# input vars: dataset to fetch. Recall that a contract published to S3 has a key format branch/pharma/brand/state/name\n",
    "input_pharma = \"bi\"\n",
    "input_brand = \"ofev\"\n",
    "input_state = \"ingest\"\n",
    "input_name = \"patient_status_ingest_column_mapping\"\n",
    "input_branch = \"seed-data\" # if None, input_branch is automagically set to your working branch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "Populating config mocker based on config parameters...\n",
    "\"\"\"\n",
    "import core.helpers.pipeline_builder as builder\n",
    "\n",
    "ids = builder.build(config_pharma, config_brand, config_state, config_name, session)\n",
    "transform_id = ids[0]\n",
    "run_id = ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "This section imports data from the configuration database\n",
    "and should not need to be altered or otherwise messed with. \n",
    "~~These are not the droids you are looking for~~\n",
    "\"\"\"\n",
    "from core.constants import BRANCH_NAME, ENV_BUCKET\n",
    "from core.helpers.session_helper import SessionHelper\n",
    "from core.models.configuration import Transformation\n",
    "from dataclasses import dataclass\n",
    "from core.dataset_contract import DatasetContract\n",
    "\n",
    "db_transform = session.query(Transformation).filter(Transformation.id == transform_id).one()\n",
    "\n",
    "@dataclass\n",
    "class DbTransform:\n",
    "    id: int = db_transform.id ## the instance id of the transform in the config app\n",
    "    name: str = db_transform.transformation_template.name ## the transform name in the config app\n",
    "    state: str = db_transform.pipeline_state.pipeline_state_type.name ## the pipeline state, one of raw, ingest, master, enhance, enrich, metrics, dimensional\n",
    "    branch:str = BRANCH_NAME ## the git branch for this execution \n",
    "    brand: str = db_transform.pipeline_state.pipeline.brand.name ## the pharma brand name\n",
    "    pharmaceutical_company: str = db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name # the pharma company name\n",
    "    publish_contract: DatasetContract = DatasetContract(branch=BRANCH_NAME,\n",
    "                            state=db_transform.pipeline_state.pipeline_state_type.name,\n",
    "                            parent=db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name,\n",
    "                            child=db_transform.pipeline_state.pipeline.brand.name,\n",
    "                            dataset=db_transform.transformation_template.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE Cartridge Notebook::Patient Status Date Standardization\n",
    "![CORE Logo](assets/coreLogo.png) \n",
    "\n",
    "---\n",
    "## Keep in Mind\n",
    "Good Transforms Are...\n",
    "- **singular in purpose:** good transforms do one and only one thing, and handle all known cases for that thing. \n",
    "- **repeatable:** transforms should be written in a way that they can be run against the same dataset an infinate number of times and get the same result every time. \n",
    "- **easy to read:** 99 times out of 100, readable, clear code that runs a little slower is more valuable than a mess that runs quickly. \n",
    "- **No 'magic numbers':** if a variable or function is not instantly obvious as to what it is or does, without context, maybe consider renaming it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow - how to use this notebook to make science\n",
    "#### Data Science\n",
    "1. **Document your transform.** Fill out the _description_ cell below describing what it is this transform does; this will appear in the configuration application where Ops will create, configure and update pipelines. \n",
    "1. **Define your config object.** Fill out the _configuration_ cell below the commented-out guide to define the variables you want ops to set in the configuration application (these will populate here for every pipeline). \n",
    "2. **Build your transformation logic.** Use the transformation cell to do that magic that you do. \n",
    "![caution](assets/cautionTape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "********* VARIABLES - PLEASE TOUCH ********* \n",
    "This section defines what you expect to get from the configuration application \n",
    "in a single \"transform\" object. Define the vars you need here, and comment inline to the right of them \n",
    "for all-in-one documentation. \n",
    "Engineering will build a production \"transform\" object for every pipeline that matches what you define here.\n",
    "\n",
    "@@@ FORMAT OF THE DATA CLASS IS: @@@ \n",
    "\n",
    "<variable_name>: <data_type> #<comment explaining what the value is to future us>\n",
    "\n",
    "e.g.\n",
    "\n",
    "class Transform(DbTransform):\n",
    "    some_ratio: float\n",
    "    site_name: str\n",
    "\n",
    "~~These ARE the droids you are looking for~~\n",
    "\"\"\"\n",
    "\n",
    "class Transform(DbTransform):\n",
    "    '''\n",
    "    YOUR properties go here!!\n",
    "    Variable properties should be assigned to the exact name of\n",
    "    the transformation as it appears in the Jupyter notebook filename.\n",
    "    '''\n",
    "    input_transform: str = db_transform.variables.input_transform # The source data to pull from\n",
    "    status_date: str = db_transform.variables.status_date # String format of the status date. Blank default to attempt to auto-read.\n",
    "    transaction_date: str = db_transform.variables.transaction_date # String format of the transaction date. Blank default to attempt to auto-read.\n",
    "    referral_date: str = db_transform.variables.referral_date # String format of the referral date. Blank default to attempt to auto-read.\n",
    "    patient_dob: str = db_transform.variables.patient_dob # String format of the patient date of birth. Blank default to attempt to auto-read.\n",
    "    rx_date: str = db_transform.variables.rx_date # String format of the rx date. Blank default to attempt to auto-read.\n",
    "    ship_date: str = db_transform.variables.ship_date # String format of the ship date. Blank default to attempt to auto-read.\n",
    "    primary_prior_auth_expiration_date: str = db_transform.variables.primary_prior_auth_expiration_date # String format of the primary prior auth expiration date. Blank default to attempt to auto-read.\n",
    "    patient_consent_date: str = db_transform.variables.patient_consent_date # String format of the patient consent date. Blank default to attempt to auto-read.\n",
    "    enroll_received_date: str = db_transform.variables.enroll_received_date # String format of the enroll received date. Blank default to attempt to auto-read.\n",
    "    fitness_for_duty_ship_date: str = db_transform.variables.fitness_for_duty_ship_date # String format of the fitness for duty ship date. Blank default to attempt to auto-read.\n",
    "    triage_date: str = db_transform.variables.triage_date # String format of the triage date. Blank default to attempt to auto-read.\n",
    "\n",
    "transform = Transform()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Please place your value assignments for development here!!\n",
    "## This cell will be turned off in production and Engineering will set to pull from the configuration application instead\n",
    "## For the last example, this could look like...\n",
    "## transform.some_ratio = 0.6\n",
    "## transform.site_name = \"WALGREENS\"\n",
    "transform.status_date = \"\"\n",
    "transform.transaction_date = \"\"\n",
    "transform.referral_date = \"\"\n",
    "transform.patient_dob = \"\"\n",
    "transform.rx_date = \"\"\n",
    "transform.ship_date = \"\"\n",
    "transform.primary_prior_auth_expiration_date = \"\"\n",
    "transform.patient_consent_date = \"\"\n",
    "transform.enroll_received_date = \"\"\n",
    "transform.fitness_for_duty_ship_date = \"\"\n",
    "transform.triage_date = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = [\n",
    "    \"status_date\",\n",
    "    \"transaction_date\",\n",
    "    \"referral_date\",\n",
    "    \"patient_dob\",\n",
    "    \"rx_date\",\n",
    "    \"ship_date\",\n",
    "    \"primary_prior_auth_expiration_date\",\n",
    "    \"patient_consent_date\",\n",
    "    \"enroll_received_date\",\n",
    "    \"fitness_for_duty_ship_date\",\n",
    "    \"triage_date\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "What does this transformation do? be specific.\n",
    "\n",
    "![what does your transform do](assets/what.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transform takes the predetermined date columns and applies the given date format to the strings to turn them into a standard date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "************ FETCH DATA - TOUCH, BUT CAREFULLY **************\n",
    "This cell will be turned off in production, as the input_contract will be handled by the pipeline.\n",
    "\"\"\"\n",
    "\n",
    "if not input_branch:\n",
    "    input_branch = BRANCH_NAME\n",
    "    \n",
    "input_contract = DatasetContract(branch=input_branch, state=input_state, parent=input_pharma, child=input_brand, dataset=input_name)\n",
    "run_filter = []\n",
    "run_filter.append(dict(partition=\"__metadata_run_id\", comparison=\"==\", values=[1]))\n",
    "# IF YOU HAVE PUBLISHED DATA MULTIPLE TIMES, uncomment the above line and change the int to the run_id to fetch.\n",
    "# Otherwise, you will have duplicate values in your fetched dataset!\n",
    "final_dataframe = input_contract.fetch(filters=run_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.logging import get_logger\n",
    "\n",
    "import dateutil.parser as parser\n",
    "from operator import attrgetter\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(f\"core.transforms.{transform.state}.{transform.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_parse_and_format(date_string: Optional[str] = \"\"):\n",
    "    try:\n",
    "        if type(date_string) is pd.Timestamp:\n",
    "            return date_string\n",
    "        elif date_string is None or date_string == \"\" or not isinstance(date_string, str):\n",
    "            return pd.NaT\n",
    "        elif date_string == \"Under18\":\n",
    "            # Given by some manufacturers, may get adjust handling later\n",
    "            return pd.NaT\n",
    "\n",
    "        date_string = re.sub('\\W+', '', date_string)\n",
    "        date_string = date_string.replace(\"_\",\"\")\n",
    "\n",
    "        date_object = parser.parse(date_string)\n",
    "        formatted_date_string = date_object.strftime(\"%Y%m%d\")\n",
    "        return date_object\n",
    "    except:\n",
    "        # Error catch like this due to the hazards of attempting real logs with 500,000+ rows\n",
    "        logger.error(f\"Failed to auto-parse date: {date_string}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def date_predifined_format(date_format: str, date_string: Optional[str] = \"\"):\n",
    "    try:\n",
    "        if type(date_string) is pd.Timestamp:\n",
    "            return date_string\n",
    "        elif date_string is None or date_string == \"\" or not isinstance(date_string, str):\n",
    "            return pd.NaT\n",
    "        elif date_string == \"Under18\":\n",
    "            # Given by some manufacturers, may get adjust handling later\n",
    "            return pd.NaT\n",
    "\n",
    "        date_object = datetime.strptime(date_string, date_format)\n",
    "        formatted_date_string = date_object.strftime(\"%Y%m%d\")\n",
    "        return date_object\n",
    "    except:\n",
    "        # Error catch like this due to the hazards of attempting real logs with 500,000+ rows\n",
    "        logger.error(f\"Failed to parse date: {date_string}   with format {date_format}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_contract = DatasetContract(branch=BRANCH_NAME,\n",
    "                                state=db_transform.pipeline_state.pipeline_state_type.name,\n",
    "                                parent=db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name,\n",
    "                                child=db_transform.pipeline_state.pipeline.brand.name,\n",
    "                                dataset=transform.input_transform)\n",
    "\n",
    "run_filter = dict(partition=\"run_id\", comparison=\"==\", values=[run_id])\n",
    "\n",
    "final_dataframe = input_contract.fetch(filters=run_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use the variables above to execute your transformation. the final output needs to be a variable named final_dataframe\n",
    "\n",
    "for date_column in date_columns:\n",
    "    # List matches transform variables. Much more straightforward than the inverse\n",
    "    date_column_attr = attrgetter(date_column)\n",
    "    transform_date = date_column_attr(transform)\n",
    "    \n",
    "    # If theres not a format, auto-parse, otherwise parse by the given\n",
    "    if transform_date == \"\":\n",
    "        final_dataframe[date_column] = final_dataframe[date_column].map(date_parse_and_format)\n",
    "    else:\n",
    "        final_dataframe[date_column] = final_dataframe[date_column].map(date_predifined_format, transform_date)\n",
    "        \n",
    "    # Floor the date to avoid weird errors later\n",
    "    # Date doesn't transfer but datetime does, but not all data has times so equal out the lack of precision\n",
    "    final_dataframe[date_column] = final_dataframe[date_column].dt.floor('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## that's it - just provide the final dataframe to the var final_dataframe and we take it from there\n",
    "transform.publish_contract.publish(final_dataframe, run_id, session)\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
