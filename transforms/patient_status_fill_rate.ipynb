{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.helpers.session_helper import SessionHelper\n",
    "session = SessionHelper().session"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "************ CONFIGURATION - PLEASE TOUCH **************\n",
    "Pipeline Builder configuration: creates configurations from variables specified here!!\n",
    "This cell will be off in production as configurations will come from the configuration postgres DB.\n",
    "\"\"\"\n",
    "# config vars: this dataset\n",
    "config_pharma = \"sun\" # the pharmaceutical company which owns {brand}\n",
    "config_brand = \"ilumya\" # the brand this pipeline operates on\n",
    "config_state = \"enrich\" # the state this transform runs in\n",
    "config_name = \"fill_rate_schema\" # the name of this transform, which is the name of this notebook without .ipynb\n",
    "\n",
    "# input vars: dataset to fetch. Recall that a contract published to S3 has a key format branch/pharma/brand/state/name\n",
    "input_pharma = \"sun\"\n",
    "input_brand = \"ilumya\"\n",
    "input_state = \"ingest\"\n",
    "input_name = \"symphony_health_association_ingest_column_mapping\"\n",
    "input_branch = \"sun-extract-validation\" # if None, input_branch is automagically set to your working branch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "Populating config mocker based on config parameters...\n",
    "\"\"\"\n",
    "import core.helpers.pipeline_builder as builder\n",
    "\n",
    "ids = builder.build(config_pharma, config_brand, config_state, config_name, session)\n",
    "transform_id = ids[0]\n",
    "run_id = ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "This section imports data from the configuration database\n",
    "and should not need to be altered or otherwise messed with. \n",
    "~~These are not the droids you are looking for~~\n",
    "\"\"\"\n",
    "from core.constants import BRANCH_NAME, ENV_BUCKET\n",
    "from core.models.configuration import Transformation\n",
    "from dataclasses import dataclass\n",
    "from core.dataset_contract import DatasetContract\n",
    "\n",
    "db_transform = session.query(Transformation).filter(Transformation.id == transform_id).one()\n",
    "\n",
    "@dataclass\n",
    "class DbTransform:\n",
    "    id: int = db_transform.id ## the instance id of the transform in the config app\n",
    "    name: str = db_transform.transformation_template.name ## the transform name in the config app\n",
    "    state: str = db_transform.pipeline_state.pipeline_state_type.name ## the pipeline state, one of raw, ingest, master, enhance, enrich, metrics, dimensional\n",
    "    branch:str = BRANCH_NAME ## the git branch for this execution \n",
    "    brand: str = db_transform.pipeline_state.pipeline.brand.name ## the pharma brand name\n",
    "    pharmaceutical_company: str = db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name # the pharma company name\n",
    "    publish_contract: DatasetContract = DatasetContract(branch=BRANCH_NAME,\n",
    "                            state=db_transform.pipeline_state.pipeline_state_type.name,\n",
    "                            parent=db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name,\n",
    "                            child=db_transform.pipeline_state.pipeline.brand.name,\n",
    "                            dataset=db_transform.transformation_template.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE Cartridge Notebook::Patient Status Fill Rate\n",
    "![CORE Logo](assets/coreLogo.png) \n",
    "\n",
    "---\n",
    "## Keep in Mind\n",
    "Good Transforms Are...\n",
    "- **singular in purpose:** good transforms do one and only one thing, and handle all known cases for that thing. \n",
    "- **repeatable:** transforms should be written in a way that they can be run against the same dataset an infinate number of times and get the same result every time. \n",
    "- **easy to read:** 99 times out of 100, readable, clear code that runs a little slower is more valuable than a mess that runs quickly. \n",
    "- **No 'magic numbers':** if a variable or function is not instantly obvious as to what it is or does, without context, maybe consider renaming it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow - how to use this notebook to make science\n",
    "#### Data Science\n",
    "1. **Document your transform.** Fill out the _description_ cell below describing what it is this transform does; this will appear in the configuration application where Ops will create, configure and update pipelines. \n",
    "1. **Define your config object.** Fill out the _configuration_ cell below the commented-out guide to define the variables you want ops to set in the configuration application (these will populate here for every pipeline). \n",
    "2. **Build your transformation logic.** Use the transformation cell to do that magic that you do. \n",
    "![caution](assets/cautionTape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "********* VARIABLES - PLEASE TOUCH ********* \n",
    "This section defines what you expect to get from the configuration application \n",
    "in a single \"transform\" object. Define the vars you need here, and comment inline to the right of them \n",
    "for all-in-one documentation. \n",
    "Engineering will build a production \"transform\" object for every pipeline that matches what you define here.\n",
    "\n",
    "@@@ FORMAT OF THE DATA CLASS IS: @@@ \n",
    "\n",
    "<variable_name>: <data_type> #<comment explaining what the value is to future us>\n",
    "\n",
    "e.g.\n",
    "\n",
    "class Transform(DbTransform):\n",
    "    some_ratio: float\n",
    "    site_name: str\n",
    "\n",
    "~~These ARE the droids you are looking for~~\n",
    "\"\"\"\n",
    "\n",
    "class Transform(DbTransform):\n",
    "    '''\n",
    "    YOUR properties go here!!\n",
    "    Variable properties should be assigned to the exact name of\n",
    "    the transformation as it appears in the Jupyter notebook filename.\n",
    "    '''\n",
    "    \n",
    "#     pat_id: str = db_transform.variables.pat_id # Patiend ID column\n",
    "#     pharm_name: str = db_transform.variables.pharm_name # Pharmacy Name column\n",
    "#     cs_outlet_id: str = db_transform.variables.cs_outlet_id # Outlet ID column\n",
    "#     brand_id: str = db_transform.variables.brand_id # Brand Name column\n",
    "#     diagnosis_code: str = db_transform.variables.diagnosis_code # Diagnosis column\n",
    "#     status_date: str = db_transform.variables.status_date # Status Date column\n",
    "#     ref_date: str = db_transform.variables.ref_date # Referral Date column\n",
    "#     transaction_id: str = db_transform.variables.transaction_id # Transaction ID column\n",
    "#     ref_source: str = db_transform.variables.ref_source # Referral Source column\n",
    "#     provider_fn: str = db_transform.variables.provider_fn # Provider First Name column\n",
    "#     provider_ln: str = db_transform.variables.provider_ln # Provider Last Name column\n",
    "#     provider_s: str = db_transform.variables.provider_s # Provider State column\n",
    "#     provider_z: str = db_transform.variables.provider_z # Provider Zip Code column\n",
    "#     payer: str = db_transform.variables.payer # Payer Name column\n",
    "#     payer_type: str = db_transform.variables.payer_type # Payer Type column\n",
    "        \n",
    "    input_transform: str = db_transform.variables.input_transform # The name of the transform to input source data from\n",
    "    ic_status: str = db_transform.variables.ic_status # column name of integrichain status\n",
    "    ic_sub_status: str = db_transform.variables.ic_sub_status # column name of integrichain sub status\n",
    "    pjh: str = db_transform.variables.pjh # column name of patient journey heirarchy\n",
    "    shipment_status: str = db_transform.variables.shipment_status # string of shipment status. Should be something like 'SHIPMENT'\n",
    "    transfered_status: str = db_transform.variables.transfered_status # string of transfered status. Should be something like 'TRANSFERRED'\n",
    "    cancelled_status: str = db_transform.variables.cancelled_status # string of cancelled status. Should be something like 'CANCELLED'\n",
    "    open_status: str = db_transform.variables.open_status # string of cancelled status. Should be something like 'OPEN'\n",
    "    filled_status: str = db_transform.variables.filled_status # string of cancelled status. Should be something like 'FILLED'\n",
    "    referral_status: str = db_transform.variables.referral_status # column name of referral status. Should be something like 'referral_status'\n",
    "    \n",
    "    \n",
    "    def fill_rate_schema(self,df):\n",
    "        \n",
    "        col_ids = [self.pat_id, self.pharm_name, self.brand_id]\n",
    "        df = df.sort_values([self.pat_id, self.pharm_name, self.brand_id, self.status_date],ascending=[True, True, True, True])\n",
    "        \n",
    "        min_df = self._return_groupby_column(df, col_ids, self.status_date, 'min', 'status_date_to_use', filter_column=self.ic_sub_status, df_filter=self.shipment_status, comparison='==')\n",
    "        max_df = self._return_groupby_column(df, col_ids, self.status_date, 'max', 'non_active_date', filter_column=self.ic_sub_status, df_filter=self.shipment_status, comparison='!=')\n",
    "        min_max_df = pd.merge(min_df,max_df,on=col_ids,how='outer')\n",
    "        min_max_df.loc[min_max_df.status_date_to_use.isna(),'status_date_to_use'] = min_max_df.non_active_date\n",
    "        min_max_df = min_max_df.drop(labels=['non_active_date'],axis=1)\n",
    "        df = pd.merge(df,min_max_df,on=col_ids)\n",
    "                \n",
    "        df = self._format_transaction_id(df)\n",
    "        \n",
    "        df['to_use_date'] = np.where(df[self.status_date] == df['status_date_to_use'],1,0)\n",
    "        df = df[df.to_use_date == 1][[self.pat_id, self.pharm_name, self.brand_id, self.transaction_id, 'adjusted_transaction_id', self.status_date, \n",
    "                                      self.ref_source, self.provider_fn, self.provider_ln, self.provider_s, self.payer, self.payer_type, self.ref_date, \n",
    "                                      self.ic_status, self.ic_sub_status, self.pjh, self.cs_outlet_id, self.diagnosis_code, self.provider_z]].drop_duplicates()\n",
    "        \n",
    "        max_transaction_id_df = self._return_groupby_column(df,col_ids,'adjusted_transaction_id','max','max_transaction_id')\n",
    "        df = pd.merge(df,max_transaction_id_df,on=col_ids)\n",
    "        df['max_transaction'] = np.where(df.adjusted_transaction_id == df['max_transaction_id'],1,0)\n",
    "\n",
    "        df = (\n",
    "            df\n",
    "            [df.max_transaction == 1]\n",
    "            .drop(labels=['max_transaction','max_transaction_id','adjusted_transaction_id'],axis=1)\n",
    "        )\n",
    "        \n",
    "        df = self._create_one_hot_encoding(df)\n",
    "        \n",
    "        df = (\n",
    "            df\n",
    "            .drop_duplicates(subset=[self.pat_id, self.pharm_name, self.brand_id, self.filled, self.transferred, self.cancelled, self.still_open])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        \n",
    "        df.loc[df[self.filled] == 1,self.firstfilldate] = df[self.status_date]\n",
    "        df[self.firstfilldate] = pd.to_datetime(df[self.firstfilldate])\n",
    "\n",
    "        df.loc[df[self.cancelled] == 1,self.canceldate] = df[self.status_date]\n",
    "        df[self.canceldate] = pd.to_datetime(df[self.canceldate])\n",
    "        \n",
    "        df.loc[df.filled == 1, self.referral_status] = self.filled_status\n",
    "        df.loc[df.cancelled == 1, self.referral_status] = self.cancelled_status\n",
    "        df.loc[(df.transferred == 1) | (df.still_open == 1), self.referral_status] = self.open_status\n",
    "        \n",
    "        df.loc[df.cancelled == 1,self.cancelsubstatus] = df[self.ic_sub_status]\n",
    "        df.loc[df.cancelled == 1,self.cancelreason] = df[self.pjh]\n",
    "        \n",
    "        # This needs to be the last change made before returning.\n",
    "        df = self._sort_for_table(df)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def _return_groupby_column(self, df ,group_by_cols, series, min_max ,new_column_name, filter_column=None, df_filter=None, comparison=None):\n",
    "        \n",
    "        if df_filter is not None:\n",
    "            df = eval(\"df[df['\" + filter_column + \"'] \" + comparison + \" '\" +  df_filter + \"']\")\n",
    "        \n",
    "        if min_max == 'min':\n",
    "            status_date_to_use_df = (\n",
    "                df\n",
    "                .groupby(group_by_cols)\n",
    "                [series]\n",
    "                .min()\n",
    "            )\n",
    "            \n",
    "        if min_max == 'max':\n",
    "            status_date_to_use_df = (\n",
    "                df\n",
    "                .groupby(group_by_cols)\n",
    "                [series]\n",
    "                .min()\n",
    "            )\n",
    "            \n",
    "        status_date_to_use_df = (\n",
    "            status_date_to_use_df\n",
    "            .reset_index(drop=False)\n",
    "            .rename(columns={series:new_column_name})\n",
    "        )\n",
    "        \n",
    "        return status_date_to_use_df\n",
    "    \n",
    "    \n",
    "    def _format_transaction_id(self,df):\n",
    "    \n",
    "        df['adjusted_transaction_id'] = df[self.transaction_id].str.extract(r'(\\d+$)')\n",
    "        \n",
    "        df['adjusted_transaction_id'] = (\n",
    "            df\n",
    "            .adjusted_transaction_id\n",
    "            .apply(lambda x: x[0:19])\n",
    "        )\n",
    "        \n",
    "        df['adjusted_transaction_id'] = pd.to_numeric(df.adjusted_transaction_id)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def _create_one_hot_encoding(self,df):\n",
    "        \n",
    "        df[self.filled] = np.where((df[self.ic_sub_status] == self.shipment_status),1,0)\n",
    "        df[self.transferred] = np.where((df[self.pjh] == self.transfered_status),1,0)\n",
    "        df[self.cancelled] = np.where((df[self.ic_status] == self.cancelled_status) & (df[self.pjh] != self.transfered_status),1,0)\n",
    "        df[self.still_open] = np.where((df.filled == 0) & (df.transferred == 0) & (df.cancelled == 0),1,0)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def _sort_for_table(self, df):\n",
    "        \n",
    "        df = df[[self.transaction_id, self.pat_id, self.pharm_name, self.cs_outlet_id, self.brand_id, self.diagnosis_code, self.provider_fn, self.provider_ln, \n",
    "                 self.provider_s, self.provider_z, self.payer, self.payer_type, self.ref_source, self.referral_status, self.ref_date, self.ic_status, self.ic_sub_status,\n",
    "                 self.status_date, self.firstfilldate, self.canceldate, self.cancelsubstatus, self.cancelreason, self.filled, self.cancelled, self.transferred, self.still_open]]\n",
    "        \n",
    "        df = df.rename(columns={self.transaction_id:'transaction_id', self.pat_id:'longitudinal_patient_id', self.pharm_name:'pharmacy_name', self.brand_id:'brand', self.ref_source:'referral_source', \n",
    "                                self.provider_fn:'hcp_first_name', self.provider_ln:'hcp_last_name', self.provider_s:'hcp_state', self.payer:'primary_payer', self.payer_type:'primary_payer_type',\n",
    "                                self.ic_status:'customer_status', self.ic_sub_status:'customer_status_description', self.status_date:'status_date', self.ref_date:'referral_date', self.provider_z:'hcp_zip',\n",
    "                                self.cs_outlet_id:'cs_outlet_id', self.diagnosis_code:'dx_1'})\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "transform = Transform()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "transform.pat_id = 'msa_patient_id'\n",
    "transform.pharm_name = 'pharm_code'\n",
    "transform.cs_outlet_id = 'pharmacy_id'\n",
    "transform.brand_id = 'medication'\n",
    "transform.diagnosis_code = 'dx1_code'\n",
    "transform.status_date = 'status_date'\n",
    "transform.ref_date = 'ref_date'\n",
    "transform.transaction_id = 'pharm_transaction_id'\n",
    "transform.ref_source = 'ref_source'\n",
    "transform.provider_fn = 'pres_first_name'\n",
    "transform.provider_ln = 'pres_last_name'\n",
    "transform.provider_s = 'pres_state'\n",
    "transform.provider_z = 'pres_zip'\n",
    "transform.payer = 'primary_payer_name'\n",
    "transform.payer_type = 'primary_payer_type'\n",
    "transform.shipment_status = 'SHIPMENT'\n",
    "transform.transfered_status = 'TRANSFERRED'\n",
    "transform.cancelled_status = 'CANCELLED'\n",
    "transform.open_status = 'OPEN'\n",
    "transform.filled_status = 'FILLED'\n",
    "transform.ic_status = 'integrichain_status'\n",
    "transform.ic_sub_status = 'integrichain_sub_status'\n",
    "transform.pjh = 'Patient_Journey_Hierarchy'\n",
    "transform.firstfilldate = 'firstfill_date'\n",
    "transform.canceldate = 'cancel_date'\n",
    "transform.cancelsubstatus = 'cancel_substatus'\n",
    "transform.cancelreason = 'cancel_reason'\n",
    "transform.referral_status = 'referral_status'\n",
    "transform.filled = 'filled'\n",
    "transform.cancelled = 'cancelled'\n",
    "transform.transferred = 'transferred'\n",
    "transform.still_open = 'still_open'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "What does this transformation do? be specific.\n",
    "\n",
    "![what does your transform do](assets/what.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fill Rate Schema](assets/FillRateSchema.png)\n",
    "\n",
    "\n",
    "\n",
    "This script checks for any active status and grabs the first instance of Active Shipped and maps it as filled. For any other instance (Transferred, Cancelled, or Still Open) it grabs the last instance and maps it under the appropriate column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ FETCH DATA - TOUCH, BUT CAREFULLY **************\n",
    "This cell will be turned off in production, as the input_contract will be handled by the pipeline.\n",
    "\"\"\"\n",
    "\n",
    "from core.dataset_diff import DatasetDiff\n",
    "\n",
    "diff = DatasetDiff(db_transform.id)\n",
    "final_dataframe = diff.get_diff(transform_name=transform.input_transform, values=[run_id])\n",
    "\n",
    "# import os\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# pd.options.display.max_columns=100\n",
    "\n",
    "# df.status_date = df.status_date.str[:8].astype(str)\n",
    "# df.ref_date = df.ref_date.str[:8].astype(str)\n",
    "\n",
    "# df.status_date = pd.to_datetime(df.status_date, infer_datetime_format=True, errors='coerce')\n",
    "# df.ref_date = pd.to_datetime(df.ref_date, infer_datetime_format=True, errors='coerce')\n",
    "\n",
    "# os.chdir('{}'.format(os.path.expanduser('~')))\n",
    "# status_config = pd.read_csv('status_mapping.csv')\n",
    "\n",
    "# status_config.loc[:,'statusCode'] = status_config.statusCode.str.upper()\n",
    "# status_config.loc[:,'subStatus'] = status_config.subStatus.str.upper()\n",
    "# status_config.loc[:,'integrichain_sub_status'] = status_config.integrichain_sub_status.str.upper()\n",
    "# status_config.loc[:,'integrichain_status'] = status_config.integrichain_status.str.upper()\n",
    "# status_config.loc[:,'Patient_Journey_Hierarchy'] = status_config.Patient_Journey_Hierarchy.str.upper()\n",
    "\n",
    "# status_config = status_config.rename(columns={'statusCode':'status_code','subStatus':'sub_status'})\n",
    "\n",
    "# df.sub_status = df.sub_status.str.replace('PRESCRIBERHOLD','PRESCRIBER HOLD')\n",
    "\n",
    "# df = pd.merge(df,status_config,on=['status_code','sub_status'])\n",
    "\n",
    "# df = df[['rec_date', 'pharm_code', 'pharm_npi', 'transtype', 'pharm_transaction_id', 'trans_seq', 'ref_source', 'ref_date', 'program_id', 'pharmacy_id', 'pat_last_name', 'pat_first_name', 'pat_dob', 'pat_gender', \n",
    "#          'pat_addr1', 'pat_addr2', 'pat_city', 'pat_state', 'pat_zip', 'dx1_code', 'dx2_code', 'status_date', 'status_code', 'sub_status', 'integrichain_status','integrichain_sub_status', 'Patient_Journey_Hierarchy', \n",
    "#          'pres_last_name', 'pres_first_name', 'pres_addr1', 'pres_addr2', 'pres_city', 'pres_state', 'pres_zip', 'pres_phone', 'pres_npi', 'pres_dea', 'facility_name', 'rxdate', 'rxnumber', 'rxrefills', 'rxfill', \n",
    "#          'refill_remaining', 'prev_disp', 'rx_ndc_number', 'medication', 'quantity', 'day_supply', 'ship_date', 'ship_carrier', 'shiptracking_num', 'ship_location', 'ship_address', 'ship_city', 'ship_state', 'ship_zip', \n",
    "#          'has_medical', 'primary_coverage_type', 'primary_payer_name', 'primary_payer_type', 'secondary_coverage_type', 'secondary_payer_name', 'secondary_payer_type', 'plan_paid_amt', 'pat_copay', 'copay_assist_amount', \n",
    "#          'oth_payer_amt', 'xfer_pharmname', 'msa_patient_id', 'msa_patient_bmap', '__metadata_run_timestamp', '__metadata_app_version', '__metadata_output_contract', '__metadata_transform_timestamp', '__metadata_run_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use the variables above to execute your transformation. the final output needs to be a variable named final_dataframe\n",
    "#final_dataframe\n",
    "final_dataframe = transform.fill_rate_schema(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import unittest\n",
    "\n",
    "def no_multiple_types(df):\n",
    "    return df[df.filled + df.transferred + df.cancelled + df.still_open != 1].shape[0] == 0\n",
    "\n",
    "def duplicates_exist(df):\n",
    "    return df[['longitudinal_patient_id', 'pharmacy_name', 'brand', transform.filled, transform.transferred, transform.cancelled, transform.still_open]].drop_duplicates().shape[0] == df.shape[0]\n",
    "\n",
    "def duplicate_pj(df):\n",
    "    return df[['longitudinal_patient_id', 'pharmacy_name', 'brand']].drop_duplicates().shape[0] == df.shape[0]\n",
    "\n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "    \n",
    "    def test_no_multiple_types(self):\n",
    "        self.assertEqual(no_multiple_types(final_dataframe),True)\n",
    "        \n",
    "    def test_duplicates_exist(self):\n",
    "        self.assertEqual(duplicates_exist(final_dataframe),True)\n",
    "       \n",
    "    def test_duplicate_pj(self):\n",
    "        self.assertEqual(duplicate_pj(final_dataframe),True)\n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## that's it - just provide the final dataframe to the var final_dataframe and we take it from there\n",
    "transform.publish_contract.publish(final_dataframe, run_id, session)\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
