{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# CORE Cartridge Notebook::[master_patient_substatus]\n",
    "![CORE Logo](assets/coreLogo.png) \n",
    "\n",
    "---\n",
    "## Keep in Mind\n",
    "Good Transforms Are...\n",
    "- **singular in purpose:** good transforms do one and only one thing, and handle all known cases for that thing. \n",
    "- **repeatable:** transforms should be written in a way that they can be run against the same dataset an infinate number of times and get the same result every time. \n",
    "- **easy to read:** 99 times out of 100, readable, clear code that runs a little slower is more valuable than a mess that runs quickly. \n",
    "- **No 'magic numbers':** if a variable or function is not instantly obvious as to what it is or does, without context, maybe consider renaming it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow - how to use this notebook to make science\n",
    "#### Data Science\n",
    "1. **Document your transform.** Fill out the _description_ cell below describing what it is this transform does; this will appear in the configuration application where Ops will create, configure and update pipelines. \n",
    "1. **Define your config object.** Fill out the _configuration_ cell below the commented-out guide to define the variables you want ops to set in the configuration application (these will populate here for every pipeline). \n",
    "2. **Build your transformation logic.** Use the transformation cell to do that magic that you do. \n",
    "![caution](assets/cautionTape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "What does this transformation do? be specific.\n",
    "\n",
    "![what does your transform do](assets/what.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planned\n",
    "1. Collect all unique raw patient substatus instances\n",
    "2. Auto-map as many raw patient substatus instances to a defined cleansed data model per **Customer**\n",
    "\n",
    "3. Process for identifying and manually mapping where auto-map fails.\n",
    "4. Do not publish un-mapped instances. Drop them, give us the ability to triage and map to IC-gold in a later event.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"CELL1\"></a>\n",
    "## CELL 1 \n",
    "<font color=orange>\n",
    "Friday, August 16, 2019 7:57:30 AM GMT-04:00 DST</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "> <ipython-input-2-dda7d8feb95d>(12)<module>()->None\n",
      "-> import pdb; pdb.set_trace()\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  \n",
      "(Pdb)  \n",
      "(Pdb)  go\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'go' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  next\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py(3329)run_code()\n",
      "-> sys.excepthook = old_excepthook\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  next\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py(3345)run_code()\n",
      "-> outflag = False\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  continue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-27 16:43:47,204 - core.helpers.session_helper.SessionHelper - INFO - Creating session for dev environment...\n",
      "2019-08-27 16:43:47,223 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating administrator mocks.\n",
      "2019-08-27 16:43:47,228 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating administrator mocks.\n",
      "2019-08-27 16:43:47,230 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pharmaceutical company mocks.\n",
      "2019-08-27 16:43:47,237 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pharmaceutical company mocks.\n",
      "2019-08-27 16:43:47,239 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating brand mocks.\n",
      "2019-08-27 16:43:47,243 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating brand mocks.\n",
      "2019-08-27 16:43:47,245 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating segment mocks.\n",
      "2019-08-27 16:43:47,252 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating segment mocks.\n",
      "2019-08-27 16:43:47,254 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline type mocks.\n",
      "2019-08-27 16:43:47,259 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline type mocks.\n",
      "2019-08-27 16:43:47,261 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline mocks.\n",
      "2019-08-27 16:43:47,267 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline mocks.\n",
      "2019-08-27 16:43:47,270 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline state type mocks.\n",
      "2019-08-27 16:43:47,283 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline state type mocks.\n",
      "2019-08-27 16:43:47,285 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline state mocks.\n",
      "2019-08-27 16:43:47,291 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline state mocks.\n",
      "2019-08-27 16:43:47,294 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating tag mocks.\n",
      "2019-08-27 16:43:47,299 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating tag mocks.\n",
      "2019-08-27 16:43:47,301 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation_template mocks.\n",
      "2019-08-27 16:43:47,305 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_template mocks.\n",
      "2019-08-27 16:43:47,308 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating bridge table mocks for transformation_templates <=> tags.\n",
      "2019-08-27 16:43:47,318 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_templates_tags mocks.\n",
      "2019-08-27 16:43:47,320 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation mocks.\n",
      "2019-08-27 16:43:47,325 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation mocks.\n",
      "2019-08-27 16:43:47,327 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation_variables mocks\n",
      "2019-08-27 16:43:47,332 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_variables mocks.\n",
      "2019-08-27 16:43:47,334 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating run_events mocks.\n",
      "2019-08-27 16:43:47,339 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating run_events mocks.\n",
      "2019-08-27 16:43:47,340 - core.helpers.session_helper.SessionHelper - INFO - Done. Created dev session with mock data.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"CELL 1\n",
    "builds and returns a database session\n",
    "local assumes a psql instance in a local docker container\n",
    "only postgres database is supported for configuration_application at this time\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "gets env-based configuration secret\n",
    "returns a session to the configuration db\n",
    "for dev env it pre-populates the database with helper and seed data\n",
    "\"\"\"\n",
    "from core.helpers.session_helper import SessionHelper\n",
    "\n",
    "session = SessionHelper().session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIGURATION - PLEASE TOUCH\n",
    "### <font color=pink>This cell will be off in production as configurations will come from the configuration postgres DB</color>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ CONFIGURATION - PLEASE TOUCH **************\n",
    "Pipeline Builder configuration: creates configurations from variables specified here!!\n",
    "This cell will be off in production as configurations will come from the configuration postgres DB.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "PIPELINE STATE:\n",
    "raw-->ingest-->master-->enhance-->enrich-->metrics-->dimensional\n",
    "\"\"\"\n",
    "# config vars: this dataset\n",
    "config_pharma = \"sun\" # the pharmaceutical company which owns {brand}\n",
    "config_brand = \"ilumya\" # the brand this pipeline operates on\n",
    "config_state = \"master\" # the state this transform runs in\n",
    "config_name = \"master_patient_substatus\" # the name of this transform!!!, which is the name of this notebook without .ipynb\n",
    "\n",
    "# input vars: dataset to fetch. \n",
    "# Recall that a contract published to S3 has a key format branch/pharma/brand/state/name\n",
    "#input_branch = \"sun-extract-validation\"\n",
    "input_branch =\"dc-627_alkermes_ingest_column_mapping\"\n",
    "# None\n",
    "# if None, input_branch is automagically set to your working branch\n",
    "input_pharma = \"alkermes\"\n",
    "input_brand = \"vivitrol\"\n",
    "input_state = \"ingest\"\n",
    "#input_name = \"symphony_health_association_ingest_column_mapping\"\n",
    "input_name = \"patient_status_ingest_column_mapping\"\n",
    "#df = pandas_from_parquet_s3('ichain-dev/dc-627_alkermes_ingest_column_mapping/alkermes/vivitrol/ingest/patient_status_ingest_column_mapping/__metadata_run_id=1/037547bb129341b9aad0ec52424b55e3.parquet')\n",
    "#This contract defines the base of the output structure of data into S3.\n",
    "#\n",
    "#contract structure in s3: \n",
    "#s3:// {ENV} / {BRANCH} / {PARENT} / {CHILD} / {STATE} / {name of input}\n",
    "#\n",
    "#ENV - environment Must be one of development, uat, production.\n",
    "#Prefixed with integrichain- due to global unique reqirement\n",
    "#BRANCH - the software branch for development this will be the working pull request (eg pr-225)\n",
    "#in uat this will be edge, in production this will be master\n",
    "#PARENT - The top level source identifier\n",
    "#this is generally the customer (and it is aliased as such) but can be IntegriChain for internal sources,\n",
    "#or another aggregator for future-proofing\n",
    "#CHILD - The sub level source identifier, generally the brand (and is aliased as such)\n",
    "#STATE - One of: raw, ingest, master, enhance, enrich, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=orange>SETUP - DON'T TOUCH </font>\n",
    "Populating config mocker based on config parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-28 11:59:20,390 - core.logging - DEBUG - Adding/getting mocks for specified configurations...\n",
      "2019-08-28 11:59:20,426 - core.logging - DEBUG - Done. Creating mock run event and committing results to configuration mocker.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "Populating config mocker based on config parameters...\n",
    "\"\"\"\n",
    "import core.helpers.pipeline_builder as builder\n",
    "\n",
    "ids = builder.build(config_pharma, config_brand, config_state, config_name, session)\n",
    "\"\"\"\n",
    "RETURNS: A list of 2 items: [transformation_id, run_id] where transformation_id corresponds\n",
    "to the configuration created/found for {transformation} and run_id is a randomly generated 6 digit\n",
    "number (to avoid publishing to the same place with the same dataset)\n",
    "\"\"\"\n",
    "\n",
    "transform_id = ids[0]\n",
    "run_id = ids[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=orange>SETUP - DON'T TOUCH </font>\n",
    "This section imports data from the configuration database\n",
    "and should not need to be altered or otherwise messed with. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"************ SETUP - DON'T TOUCH **************\n",
    "This section imports data from the configuration database\n",
    "and should not need to be altered or otherwise messed with. \n",
    "~~These are not the droids you are looking for~~\n",
    "\"\"\"\n",
    "from core.constants import BRANCH_NAME, ENV_BUCKET, BATCH_JOB_QUEUE\n",
    "from core.helpers.session_helper import SessionHelper\n",
    "from core.models.configuration import Transformation\n",
    "from dataclasses import dataclass\n",
    "from core.dataset_contract import DatasetContract\n",
    "\n",
    "\n",
    "db_transform = session.query(Transformation).filter(Transformation.id == transform_id).one()\n",
    "\n",
    "@dataclass\n",
    "class DbTransform:\n",
    "    id: int = db_transform.id ## the instance id of the transform in the config app\n",
    "    name: str = db_transform.transformation_template.name ## the transform name in the config app\n",
    "    state: str = db_transform.pipeline_state.pipeline_state_type.name ## the pipeline state, one of raw, ingest, master, enhance, enrich, metrics, dimensional\n",
    "    branch:str = BRANCH_NAME ## the git branch for this execution \n",
    "    brand: str = db_transform.pipeline_state.pipeline.brand.name ## the pharma brand name\n",
    "    pharmaceutical_company: str = db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name # the pharma company name\n",
    "    publish_contract: DatasetContract = DatasetContract(branch=BRANCH_NAME,\n",
    "                            state=db_transform.pipeline_state.pipeline_state_type.name,\n",
    "                            parent=db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name,\n",
    "                            child=db_transform.pipeline_state.pipeline.brand.name,\n",
    "                            dataset=db_transform.transformation_template.name)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-28 17:28:05,903 - root - DEBUG - Branch name:DC-673_Standardization_PatientSubstatus_for_Alkermes Env Bucket:ichain-dev Batch Job Queue:dev-core\n"
     ]
    }
   ],
   "source": [
    "\n",
    "log.debug('Branch name:{} Env Bucket:{} Batch Job Queue:{}'.format(BRANCH_NAME,ENV_BUCKET,BATCH_JOB_QUEUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIGURATION - VARIABLES - PLEASE TOUCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "CONFIGURATION ********* VARIABLES - PLEASE TOUCH ********* \n",
    "This section defines what you expect to get from the configuration application \n",
    "in a single \"transform\" object. Define the vars you need here, and comment inline to the right of them \n",
    "for all-in-one documentation. \n",
    "Engineering will build a production \"transform\" object for every pipeline that matches what you define here.\n",
    "\n",
    "@@@ FORMAT OF THE DATA CLASS IS: @@@ \n",
    "\n",
    "<variable_name>: <data_type> #<comment explaining what the value is to future us>\n",
    "e.g.\n",
    "class Transform(DbTransform):\n",
    "    some_ratio: float\n",
    "    site_name: str\n",
    "\n",
    "~~These ARE the droids you are looking for~~\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "imports\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import re \n",
    "from core.logging import get_logger\n",
    " \n",
    "class Transform(DbTransform):\n",
    "    '''\n",
    "    YOUR properties go here!!\n",
    "    Variable properties should be assigned to the exact name of\n",
    "    the transformation as it appears in the Jupyter notebook filename.\n",
    "    ''' \n",
    "    # PROD\n",
    "    '''\n",
    "    col_substatus: str = db_transform.variables.col_substatus # The column of interest for the transform \n",
    "    customer_name: str = db_transform.variables.col_customer_name # The customer name\n",
    "    input_transform: str = db_transform.variables.input_transform  # the name of the dataset to pull from \n",
    "    '''\n",
    "    # DEV\n",
    "    col_substatus: str  \n",
    "    customer_name: str  \n",
    "    input_transform: str   \n",
    "\n",
    "\n",
    "        \n",
    "    def master_substatus():\n",
    "        customer_name = transform.customer_name\n",
    "        try:\n",
    "            \n",
    "            if customer_name=='sun':\n",
    "                substatus_dict = Transform.master_substatus_sun()\n",
    "                substatus_conversion_dict = Transform.master_substatus_conversion_sun()\n",
    "            elif customer_name=='bi':\n",
    "                substatus_dict = Transform.master_substatus_bi()\n",
    "            elif customer_name=='alkermes':\n",
    "                substatus_dict = Transform.master_substatus_alkermes()\n",
    "                substatus_conversion_dict = Transform.master_substatus_conversion_alkermes()\n",
    "            else:\n",
    "                #go = False # something did not work\n",
    "                logger.exception('expecting customer name as sun bi or alkermes')\n",
    "                raise Exception('expecting customer name as sun bi or alkermes') \n",
    "        except Exception as e:\n",
    "            go = False # something did not work\n",
    "            logger.exception(f'exception:{e}')\n",
    "            raise Exception(f'raise exception:{e}')  \n",
    "        return substatus_dict, substatus_conversion_dict  \n",
    "    \n",
    "    def master_substatus_sun():\n",
    "        # need to input/ define for ic-gold mapping\n",
    "        # for substatus for sun\n",
    "        # temporary until furture User defines\n",
    "        # IC - GOLD persistence solution\n",
    "        substatus_dict = {}\n",
    "        substatus_dict[1]='ALT THERAPY'\n",
    "        substatus_dict[2]='APPEAL'\n",
    "        substatus_dict[3]='BENEFITS'\n",
    "        substatus_dict[4]='COPAY ASSISTANCE'\n",
    "        substatus_dict[5]='DELAY'\n",
    "        substatus_dict[6]='DOSAGE'\n",
    "        substatus_dict[7]='FORMULARY'\n",
    "        substatus_dict[8]='FOUNDATION'\n",
    "        substatus_dict[9]='HOLD OTHER'\n",
    "        substatus_dict[10]='HOLD RTS'\n",
    "        substatus_dict[11]='INFORMATION'\n",
    "        substatus_dict[12]='INS OTHER'\n",
    "        substatus_dict[13]='INSURANCE COPAY'\n",
    "        substatus_dict[14]='INSURANCE DENIED'\n",
    "        substatus_dict[15]='INSURANCE HOLD'\n",
    "        substatus_dict[16]='INSURANCE OON'\n",
    "        substatus_dict[17]='INSURANCE OTHER'\n",
    "        substatus_dict[18]='INVENTORY HOLD'\n",
    "        substatus_dict[19]='MATERIAL'\n",
    "        substatus_dict[20]='NEW'\n",
    "        substatus_dict[21]='OTHER'\n",
    "        substatus_dict[22]='PA'\n",
    "        substatus_dict[23]='PATIENT CONTACT'\n",
    "        substatus_dict[24]='PATIENT DECEASED'\n",
    "        substatus_dict[25]='PATIENT END'\n",
    "        substatus_dict[26]='PATIENT FINANCIAL'\n",
    "        substatus_dict[27]='PATIENT HOLD'\n",
    "        substatus_dict[28]='PATIENT RESPONSE'\n",
    "        substatus_dict[29]='PRESCRIBER'\n",
    "        substatus_dict[30]='PRESCRIBER END'\n",
    "        substatus_dict[31]='PRESCRIBER HOLD'\n",
    "        substatus_dict[32]='PT HOLD'\n",
    "        substatus_dict[33]='QUANTITY'\n",
    "        substatus_dict[34]='READY'\n",
    "        substatus_dict[35]='SERVICES END'\n",
    "        substatus_dict[36]='SHIPMENT'\n",
    "        substatus_dict[37]='STEP EDIT'\n",
    "        substatus_dict[38]='THERAPY COMPLETE'\n",
    "        substatus_dict[39]='THERAPY END'\n",
    "        substatus_dict[40]='THERAPY HOLD'\n",
    "        substatus_dict[41]='TRANSFER HUB'\n",
    "        substatus_dict[42]='TRANSFER SP'\n",
    "        substatus_dict[43]='TREATMENT DELAY'\n",
    "        return substatus_dict\n",
    "   \n",
    "\n",
    "    def master_substatus_conversion_sun():\n",
    "        substatus_conversion_dict = {}\n",
    "        substatus_conversion_dict = {'BENEFITS INVESTIGATION':'BENEFITS','INS OON ':'INSURANCE OON','OTHER ':'OTHER','P05':'PA','PATENT RESPONSE':'PATIENT RESPONSE','PATIENT  RESPONSE':'PATIENT RESPONSE','PATIENT RESPOSNE':'PATIENT RESPONSE','PRESCRIBERHOLD':'PRESCRIBER HOLD','TRANSER SP':'TRANSFER SP'}\n",
    "        return substatus_conversion_dict\n",
    "    \n",
    "    def master_substatus_conversion_alkermes():\n",
    "        substatus_conversion_dict = {}\n",
    "        return substatus_conversion_dict\n",
    "    \n",
    "    def master_substatus_bi():\n",
    "        substatus_dict = {}\n",
    "        return substatus_dict\n",
    "\n",
    "    \n",
    "    def master_substatus_alkermes():\n",
    "        substatus_dict = {}\n",
    "        substatus_dict[1]='AWAITING FINANCIAL DECISION'\n",
    "        substatus_dict[2]='AWAITING INJECTION PROVIDER'\n",
    "        substatus_dict[3]='BI -- BENEFITS VERIFICATION STARTED'\n",
    "        substatus_dict[4]='CDF APPROVAL'\n",
    "        substatus_dict[5]='COPAY ASSISTANCE'\n",
    "        substatus_dict[6]='COVERAGE DENIED'\n",
    "        substatus_dict[7]='DELAY IN TREATMENT INITIATION'\n",
    "        substatus_dict[8]='EBI - BV COMPLETE/COVERED'\n",
    "        substatus_dict[9]='ENROLLMENT MISSING INFO - WAITING ON HCP'\n",
    "        substatus_dict[10]='ENROLLMENT MISSING INFO - WAITING ON PATIENT'\n",
    "        substatus_dict[11]='ENROLLMENT STARTED - DATA ENTRY STARTED'\n",
    "        substatus_dict[12]='HCP UNRESPONSIVE -- ATTEMPTS EXHAUSTED'\n",
    "        substatus_dict[13]='INDICATION CRITERIA NOT MET'\n",
    "        substatus_dict[14]='INSURANCE CHANGE - REVERIFYING BENEFITS'\n",
    "        substatus_dict[15]='NO COVERAGE - DRUG NOT ON FORMULARY'\n",
    "        substatus_dict[16]='NO INSURANCE'\n",
    "        substatus_dict[17]='OTHER'\n",
    "        substatus_dict[18]='PA -- PA APPROVED'\n",
    "        substatus_dict[19]='PA -- PA DENIED'\n",
    "        substatus_dict[20]='PA -- PA STARTED'\n",
    "        substatus_dict[21]='PA -- WAITING ON HCP'\n",
    "        substatus_dict[22]='PA -- WAITING ON PAYER'\n",
    "        substatus_dict[23]='PA APPEAL -- STARTED'\n",
    "        substatus_dict[24]='PATIENT CHOICE - INJECTION RESISTANCE'\n",
    "        substatus_dict[25]='PATIENT REFUSED TREATMENT'\n",
    "        substatus_dict[26]='PATIENT TRANSFERRED BACK TO HUB'\n",
    "        substatus_dict[27]='PATIENT UNRESPONSIVE -- ATTEMPTS EXHAUSTED'\n",
    "        substatus_dict[28]='PATIENTS CHOICE - FINANCIAL'\n",
    "        substatus_dict[29]='PHYSICIAN CANCELLED'\n",
    "        substatus_dict[30]='PHYSICIAN DISCONTINUED'\n",
    "        substatus_dict[31]='REFILL TOO SOON'\n",
    "        substatus_dict[32]='SHIPMENT DELAYED -PATIENT HAS SUPPLY ON HAND'\n",
    "        substatus_dict[33]='SHIPMENT SCHEDULED'\n",
    "        substatus_dict[34]='SHIPPED'\n",
    "        substatus_dict[35]='SWITCHED TO COMPETITOR PRODUCT'\n",
    "        substatus_dict[36]='THERAPY COMPLETE'\n",
    "        substatus_dict[37]='TRANSFER TO PBM FOR PROCESSING'\n",
    "        substatus_dict[38]='TRIAGED TO MAIL ORDER'\n",
    "        substatus_dict[39]='TRIAGED TO OTHER SP'\n",
    "        substatus_dict[40]='TRIAGED TO PBM FOR PROCESSING'\n",
    "        substatus_dict[41]='WAITING ON HCP RESPONSE'\n",
    "        substatus_dict[42]='WAITING ON NEW RX FROM PHYSICIAN'\n",
    "        substatus_dict[43]='WAITING ON PATIENT SHIP DATE DECISION'\n",
    "        return substatus_dict\n",
    "        \n",
    "    \n",
    "    def master_patient_substatus(self,df):\n",
    "        try:        \n",
    "            logger.info('try:')\n",
    "            go = False # assume things are not working YET.\n",
    "           \n",
    "            dffail = pd.DataFrame() # initialize df for fails\n",
    "            \n",
    "            # df in\n",
    "\n",
    "            #dfSize = df.size\n",
    "            dfShape = df.shape\n",
    "            logger.info('df in  shape: {} {}'.format(dfShape[0],dfShape[1])) \n",
    "            logger.info('df in {}'.format(df.head()))  \n",
    "            \n",
    "            # am I expecting certain column names? YES \n",
    "            substatusColNameExpected = transform.col_substatus\n",
    "            \n",
    "            logger.info('expecting column name patient sub status as:{}'.format(substatusColNameExpected))\n",
    "            columnNamesArr = df.columns.values.tolist()\n",
    "            logger.info('df column names:{}'.format(columnNamesArr))\n",
    "            \n",
    "            if substatusColNameExpected in columnNamesArr:\n",
    "                logger.info('Clean: space Strip and Upper and other cleanup...')  \n",
    "\n",
    "                df[substatusColNameExpected]= df[substatusColNameExpected].apply(lambda x: x.upper() if x is not None else x)   \n",
    "                df[substatusColNameExpected]= df[substatusColNameExpected].apply(lambda x: x.strip() if x is not None else x)\n",
    "                df[substatusColNameExpected]= df[substatusColNameExpected].apply(lambda x: x.replace('_',' ').replace('\\r', '').replace('\\t', '').replace('\\w', '').replace(\"'\",'').replace('.','').replace('  ',' ') if x is not None else x)\n",
    "              \n",
    "                                                                  \n",
    "                # master data IC-GOLD substatus\n",
    "                substatus_dict = {}\n",
    "                substatus_conversion_dict = {}\n",
    "                substatus_dict, substatus_conversion_dict = Transform.master_substatus()\n",
    "                # print(substatus_conversion_dict)\n",
    "                # store the golden values in a list\n",
    "                substatus_list = list(substatus_dict.values())           \n",
    "                logger.info('Gold Domain List:{}'.format(substatus_list))  \n",
    "                \n",
    "                #apply master conversions\n",
    "                if transform.customer_name=='sun':\n",
    "                    df[substatusColNameExpected].replace(substatus_conversion_dict, inplace=True)\n",
    "                \n",
    "                \n",
    "                # what fails\n",
    "                dffail = df.loc[~df[substatusColNameExpected].isin(substatus_list)]\n",
    "                # apply master selection for the column of interest\n",
    "                # what passes\n",
    "                df = df.loc[df[substatusColNameExpected].isin(substatus_list)]\n",
    "                \n",
    "                # meta data log for what comes out of the function pass and fail df\n",
    "                dfOutSize = df.size\n",
    "                dfOutShape = df.shape\n",
    "                dffailSize = dffail.size\n",
    "                dffailShape = dffail.shape\n",
    "                logger.info('df in   shape: {} {}'.format(dfShape[0],dfShape[1]))                 \n",
    "                logger.info('df pass shape: {} {}'.format(dfOutShape[0],dfOutShape[1]))\n",
    "                logger.info('df fail shape: {} {}'.format(dffailShape[0],dffailShape[1]))  \n",
    "                # meta data log for what comes out of the function pass df\n",
    "                logger.info('df pass {}'.format(df.head()))\n",
    "                # meta data log for what comes out of the function fail df\n",
    "                logger.info('df fail {}'.format(dffail.head()))  \n",
    "                go = True\n",
    "            else:\n",
    "                go = False # something did not work\n",
    "                logger.exception('expecting column name for patient substatus if/else exception raise')\n",
    "                raise Exception(\"sub_status ColNameExpected NOT in columnNamesArr\")              \n",
    "        except Exception as e:\n",
    "            go = False # something did not work\n",
    "            logger.exception(f'exception:{e}')\n",
    "            raise Exception(f'raise exception:{e}')  \n",
    "        else:\n",
    "            pass\n",
    "        finally:\n",
    "            pass\n",
    "        return df.copy(),dffail.copy(),go\n",
    "                \n",
    "\n",
    "transform = Transform()\n",
    "logger = get_logger(f\"core.transforms.{transform.state}.{transform.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Please place your value assignments for development below !!!*\n",
    "### <font color=pink>This cell will be turned off in production, Engineering will set to pull from the configuration</color>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please place your value assignments for development here!!\n",
    "## This cell will be turned off in production and Engineering will set to pull from the configuration application instead\n",
    "## For the last example, this could look like...\n",
    "## transform.some_ratio = 0.6\n",
    "## transform.site_name = \"WALGREENS\"\n",
    "\n",
    "#transform.customer_name = 'sun'\n",
    "#transform.col_substatus = 'sub_status' # based on dev data works for sun \n",
    "transform.customer_name = 'alkermes'\n",
    "transform.col_substatus = 'customer_status_description' # based on dev data works for alkermes\n",
    "transform.input_transform = '' # for DEV NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FETCH DATA - TOUCH, BUT CAREFULLY\n",
    "### <font color=pink>This cell will be turned off in production, as the input_contract will be handled by the pipeline</color>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"FETCH DATA CELL - TOUCH - This cell will be turned off in production, as the input_contract will be handled by the pipeline. \")\n",
    "\n",
    "# for testing / development only\n",
    "run_id = 1\n",
    "\n",
    "if not input_branch:\n",
    "    input_branch = BRANCH_NAME\n",
    "input_contract = DatasetContract(branch=input_branch,\n",
    "                                 state=input_state, \n",
    "                                 parent=input_pharma, \n",
    "                                 child=input_brand, \n",
    "                                 dataset=input_name)\n",
    "run_filter = []\n",
    "run_filter.append(dict(partition=\"__metadata_run_id\", comparison=\"==\", values=[run_id]))\n",
    "# IF YOU HAVE PUBLISHED DATA MULTIPLE TIMES, uncomment the above line and change the int to the run_id to fetch.\n",
    "# Otherwise, you will have duplicate values in your fetched dataset!\n",
    "# bypass/comment out when unit testing individual parquet files\n",
    "df = input_contract.fetch(filters=run_filter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-27 17:12:26,586 - core.helpers.session_helper.SessionHelper - INFO - Creating session for dev environment...\n",
      "2019-08-27 17:12:26,614 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating administrator mocks.\n",
      "2019-08-27 17:12:26,622 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating administrator mocks.\n",
      "2019-08-27 17:12:26,624 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pharmaceutical company mocks.\n",
      "2019-08-27 17:12:26,629 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pharmaceutical company mocks.\n",
      "2019-08-27 17:12:26,633 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating brand mocks.\n",
      "2019-08-27 17:12:26,638 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating brand mocks.\n",
      "2019-08-27 17:12:26,640 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating segment mocks.\n",
      "2019-08-27 17:12:26,645 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating segment mocks.\n",
      "2019-08-27 17:12:26,646 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline type mocks.\n",
      "2019-08-27 17:12:26,650 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline type mocks.\n",
      "2019-08-27 17:12:26,654 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline mocks.\n",
      "2019-08-27 17:12:26,660 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline mocks.\n",
      "2019-08-27 17:12:26,661 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline state type mocks.\n",
      "2019-08-27 17:12:26,666 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline state type mocks.\n",
      "2019-08-27 17:12:26,670 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline state mocks.\n",
      "2019-08-27 17:12:26,674 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline state mocks.\n",
      "2019-08-27 17:12:26,677 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating tag mocks.\n",
      "2019-08-27 17:12:26,682 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating tag mocks.\n",
      "2019-08-27 17:12:26,687 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation_template mocks.\n",
      "2019-08-27 17:12:26,693 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_template mocks.\n",
      "2019-08-27 17:12:26,695 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating bridge table mocks for transformation_templates <=> tags.\n",
      "2019-08-27 17:12:26,700 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_templates_tags mocks.\n",
      "2019-08-27 17:12:26,702 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation mocks.\n",
      "2019-08-27 17:12:26,714 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation mocks.\n",
      "2019-08-27 17:12:26,715 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation_variables mocks\n",
      "2019-08-27 17:12:26,727 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_variables mocks.\n",
      "2019-08-27 17:12:26,729 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating run_events mocks.\n",
      "2019-08-27 17:12:26,749 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating run_events mocks.\n",
      "2019-08-27 17:12:26,759 - core.helpers.session_helper.SessionHelper - INFO - Done. Created dev session with mock data.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Error: No transform found with id 6'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b07ef670ad79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_diff\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetDiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetDiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_diff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/src/app/core/dataset_diff.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, transform_id)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDatasetDiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontract_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontract_from_transformation_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/src/app/core/helpers/contract_creator.py\u001b[0m in \u001b[0;36mcontract_from_transformation_id\u001b[0;34m(t_id)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTransformation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error: No transform found with id \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformation_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpharmaceutical_company\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Error: No transform found with id 6'"
     ]
    }
   ],
   "source": [
    "### Retrieve current dataset from contract\n",
    "from core.dataset_diff import DatasetDiff\n",
    "\n",
    "diff = DatasetDiff(db_transform.id)\n",
    "df = diff.get_diff(transform_name=transform.input_transform, values=[run_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *<font color=grey>unit test development only*</font>\n",
    "*<font color=grey>The next **5** cells will be deleted in production.* </font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "\n",
    "def pandas_from_parquet_s3(file_path):  \n",
    "    s3 = s3fs.S3FileSystem()\n",
    "    df = (\n",
    "        pq\n",
    "        .ParquetDataset(file_path, filesystem=s3)\n",
    "        .read_pandas()\n",
    "        .to_pandas()\n",
    "    )    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# unit test/development only\n",
    "# isolate on individual parquet files\n",
    "######## sun ###\n",
    "#TEST 1\n",
    "#df = pandas_from_parquet_s3('ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping/__metadata_run_id=3/d7ad974cef284e19aa7b5ac410220b96.parquet')\n",
    "# TEST 2\n",
    "# df = pandas_from_parquet_s3('ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping/__metadata_run_id=3/1a6ffd3598d442e38fbba66ea85a55a2.parquet')\n",
    "# TEST 3\n",
    "# df = pandas_from_parquet_s3('ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping/__metadata_run_id=3/6eceb7ce59bd4dec8720316b4209b0e3.parquet')\n",
    "# TEST 4\n",
    "#df = pandas_from_parquet_s3('ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping/__metadata_run_id=3/5c00059d9fc04b0e8bc4ce764c50f3fb.parquet')\n",
    "# TEST 5\n",
    "#df = pandas_from_parquet_s3('ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping/__metadata_run_id=3/90ca3aa7b0bb4246a281591b013ff54e.parquet')\n",
    "#################\n",
    "#################\n",
    "# TEST 6 alkermes\n",
    "#################\n",
    "df = pandas_from_parquet_s3('ichain-dev/dc-627_alkermes_ingest_column_mapping/alkermes/vivitrol/ingest/patient_status_ingest_column_mapping/__metadata_run_id=1/037547bb129341b9aad0ec52424b55e3.parquet')\n",
    "# 33927\n",
    "\n",
    "#####################################################\n",
    "# THEN ALL TEST use \n",
    "# then use the FETCH DATA - TOUCH, BUT CAREFULLY CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test/development only\n",
    "# before shot unit testing only\n",
    "dfSize = df.size\n",
    "dfShape = df.shape\n",
    "print('shape: {} {}'.format(dfShape[0],dfShape[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test/development only \n",
    "# needed to see the col(s) of interest\n",
    "pd.set_option('display.max_columns', 135)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['customer_status_description'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>**CALL**</font> THE TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use the variables above to execute your transformation.\n",
    "### the final output needs to be a variable named final_dataframe\n",
    "logger.info(\"CALL THE TRANSFORM - execute your transformation\")\n",
    "\n",
    "final_dataframe, final_fail, go = transform.master_patient_substatus(df)\n",
    "\n",
    "if go==True:\n",
    "    logger.info(\"CALL THE TRANSFORM -  go no go = GO\")\n",
    "elif go==False:\n",
    "    logger.info(\"CALL THE TRANSFORM -  go no go = NO go\")\n",
    "else:\n",
    "    go=False\n",
    "    logger.info(\"CALL THE TRANSFORM -  go no go = unknown make it NO go\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *<font color=grey>unittest python*</font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import unittest\n",
    "\n",
    "def ut_shape(final_dataframe,df):\n",
    "    \"\"\"\n",
    "    assertion will change based on coding state\n",
    "    \"\"\"\n",
    "    return final_dataframe.shape == df.shape\n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "    \n",
    "    def test_ut_shape(self):\n",
    "        \n",
    "        self.assertEqual(ut_shape(final_dataframe,df),True)\n",
    "                \n",
    "\"\"\"\n",
    "expect ... ok for now until I add some transform code since transform function is doing nothing yet\n",
    "expect ... FAIL for sun run id = 3 do to nulls ( None )\n",
    "\"\"\"\n",
    "# for development only\n",
    "unittest.main(argv=[''], verbosity= 2, exit=False)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360      DUPLICATE REFERRAL FROM HUB\n",
       "493      DUPLICATE REFERRAL FROM HUB\n",
       "760      DUPLICATE REFERRAL FROM HUB\n",
       "761      DUPLICATE REFERRAL FROM HUB\n",
       "1468     DUPLICATE REFERRAL FROM HUB\n",
       "1474     DUPLICATE REFERRAL FROM HUB\n",
       "1672     DUPLICATE REFERRAL FROM HUB\n",
       "2111     DUPLICATE REFERRAL FROM HUB\n",
       "4934     DUPLICATE REFERRAL FROM HUB\n",
       "4999     DUPLICATE REFERRAL FROM HUB\n",
       "5004     DUPLICATE REFERRAL FROM HUB\n",
       "5530     DUPLICATE REFERRAL FROM HUB\n",
       "6409     DUPLICATE REFERRAL FROM HUB\n",
       "6763     DUPLICATE REFERRAL FROM HUB\n",
       "6950     DUPLICATE REFERRAL FROM HUB\n",
       "7931     DUPLICATE REFERRAL FROM HUB\n",
       "8651     DUPLICATE REFERRAL FROM HUB\n",
       "11350    DUPLICATE REFERRAL FROM HUB\n",
       "13031    DUPLICATE REFERRAL FROM HUB\n",
       "13194    DUPLICATE REFERRAL FROM HUB\n",
       "15195    DUPLICATE REFERRAL FROM HUB\n",
       "15231    DUPLICATE REFERRAL FROM HUB\n",
       "15232    DUPLICATE REFERRAL FROM HUB\n",
       "15296    DUPLICATE REFERRAL FROM HUB\n",
       "15309    DUPLICATE REFERRAL FROM HUB\n",
       "15401    DUPLICATE REFERRAL FROM HUB\n",
       "15582    DUPLICATE REFERRAL FROM HUB\n",
       "15585    DUPLICATE REFERRAL FROM HUB\n",
       "15676    DUPLICATE REFERRAL FROM HUB\n",
       "15713    DUPLICATE REFERRAL FROM HUB\n",
       "                    ...             \n",
       "26906    DUPLICATE REFERRAL FROM HUB\n",
       "27154    DUPLICATE REFERRAL FROM HUB\n",
       "27600    DUPLICATE REFERRAL FROM HUB\n",
       "27753    DUPLICATE REFERRAL FROM HUB\n",
       "27917    DUPLICATE REFERRAL FROM HUB\n",
       "27924    DUPLICATE REFERRAL FROM HUB\n",
       "28388    DUPLICATE REFERRAL FROM HUB\n",
       "28452    DUPLICATE REFERRAL FROM HUB\n",
       "28506    DUPLICATE REFERRAL FROM HUB\n",
       "28518    DUPLICATE REFERRAL FROM HUB\n",
       "29046    DUPLICATE REFERRAL FROM HUB\n",
       "29135    DUPLICATE REFERRAL FROM HUB\n",
       "29147    DUPLICATE REFERRAL FROM HUB\n",
       "29529    DUPLICATE REFERRAL FROM HUB\n",
       "29612    DUPLICATE REFERRAL FROM HUB\n",
       "29842    DUPLICATE REFERRAL FROM HUB\n",
       "29956    DUPLICATE REFERRAL FROM HUB\n",
       "30324    DUPLICATE REFERRAL FROM HUB\n",
       "30327    DUPLICATE REFERRAL FROM HUB\n",
       "30414    DUPLICATE REFERRAL FROM HUB\n",
       "30455    DUPLICATE REFERRAL FROM HUB\n",
       "30877    DUPLICATE REFERRAL FROM HUB\n",
       "32013    DUPLICATE REFERRAL FROM HUB\n",
       "32201    DUPLICATE REFERRAL FROM HUB\n",
       "32367    DUPLICATE REFERRAL FROM HUB\n",
       "32890    DUPLICATE REFERRAL FROM HUB\n",
       "33023    DUPLICATE REFERRAL FROM HUB\n",
       "33444    DUPLICATE REFERRAL FROM HUB\n",
       "33503    DUPLICATE REFERRAL FROM HUB\n",
       "33915    DUPLICATE REFERRAL FROM HUB\n",
       "Name: customer_status_description, Length: 86, dtype: object"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# untit test/development only look at the fails\n",
    "final_fail.head()\n",
    "final_fail[transform.col_substatus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DUPLICATE REFERRAL FROM HUB    86\n",
       "Name: customer_status_description, dtype: int64"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# untit test/development only look at the fails\n",
    "final_fail['customer_status_description'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                           SHIPPED\n",
       "1                                           SHIPPED\n",
       "2                       AWAITING FINANCIAL DECISION\n",
       "3                                           SHIPPED\n",
       "4               BI -- BENEFITS VERIFICATION STARTED\n",
       "5               BI -- BENEFITS VERIFICATION STARTED\n",
       "6               BI -- BENEFITS VERIFICATION STARTED\n",
       "7                           WAITING ON HCP RESPONSE\n",
       "8               BI -- BENEFITS VERIFICATION STARTED\n",
       "9             WAITING ON PATIENT SHIP DATE DECISION\n",
       "10              BI -- BENEFITS VERIFICATION STARTED\n",
       "11              BI -- BENEFITS VERIFICATION STARTED\n",
       "12              BI -- BENEFITS VERIFICATION STARTED\n",
       "13                          WAITING ON HCP RESPONSE\n",
       "14              BI -- BENEFITS VERIFICATION STARTED\n",
       "15              BI -- BENEFITS VERIFICATION STARTED\n",
       "16              BI -- BENEFITS VERIFICATION STARTED\n",
       "17              BI -- BENEFITS VERIFICATION STARTED\n",
       "18              BI -- BENEFITS VERIFICATION STARTED\n",
       "19              BI -- BENEFITS VERIFICATION STARTED\n",
       "20       PATIENT UNRESPONSIVE -- ATTEMPTS EXHAUSTED\n",
       "21                                 COPAY ASSISTANCE\n",
       "22                                          SHIPPED\n",
       "23            WAITING ON PATIENT SHIP DATE DECISION\n",
       "24            WAITING ON PATIENT SHIP DATE DECISION\n",
       "25            WAITING ON PATIENT SHIP DATE DECISION\n",
       "26                          WAITING ON HCP RESPONSE\n",
       "27                                 COPAY ASSISTANCE\n",
       "28                  PATIENT TRANSFERRED BACK TO HUB\n",
       "29            WAITING ON PATIENT SHIP DATE DECISION\n",
       "                            ...                    \n",
       "33896       ENROLLMENT STARTED - DATA ENTRY STARTED\n",
       "33897         WAITING ON PATIENT SHIP DATE DECISION\n",
       "33898                       WAITING ON HCP RESPONSE\n",
       "33899                                       SHIPPED\n",
       "33900                     EBI - BV COMPLETE/COVERED\n",
       "33901                              PA -- PA STARTED\n",
       "33902                              PA -- PA STARTED\n",
       "33903                                         OTHER\n",
       "33904                                       SHIPPED\n",
       "33905                                       SHIPPED\n",
       "33906                                       SHIPPED\n",
       "33907                                       SHIPPED\n",
       "33908       ENROLLMENT STARTED - DATA ENTRY STARTED\n",
       "33909                       WAITING ON HCP RESPONSE\n",
       "33910                                       SHIPPED\n",
       "33911                                       SHIPPED\n",
       "33912         WAITING ON PATIENT SHIP DATE DECISION\n",
       "33913       ENROLLMENT STARTED - DATA ENTRY STARTED\n",
       "33914                                       SHIPPED\n",
       "33916                                       SHIPPED\n",
       "33917                                       SHIPPED\n",
       "33918         WAITING ON PATIENT SHIP DATE DECISION\n",
       "33919                                       SHIPPED\n",
       "33920                                       SHIPPED\n",
       "33921                   AWAITING FINANCIAL DECISION\n",
       "33922           BI -- BENEFITS VERIFICATION STARTED\n",
       "33923                       WAITING ON HCP RESPONSE\n",
       "33924           BI -- BENEFITS VERIFICATION STARTED\n",
       "33925                        PA -- WAITING ON PAYER\n",
       "33926                       WAITING ON HCP RESPONSE\n",
       "Name: customer_status_description, Length: 33841, dtype: object"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# untit test/development only look at the pass(es)\n",
    "final_dataframe.head()\n",
    "final_dataframe[transform.col_substatus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **publish**\n",
    "### Writing to S3\n",
    "Invoke the `publish()` command to write to a given contract. Some things to know:\n",
    "- To invoke publish a contract must be at the grain of dataset. This is because file names will be set by the dataframe=\\>parquet conversion. \n",
    "- publish only accepts a pandas dataframe.\n",
    "- publish does not allow for timedelta data types at this time (this is missing functionality in pyarrow).\n",
    "- publish handles partitioning the data as per contract, creating file paths, and creating the binary parquet files in S3, as well as the needed metadata. <br>\n",
    "**- by default, all datasets include a single partition, \\_\\_metadata\\_run\\_id, the RunEvent ID of an executed pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-27 17:15:53,558 - core.transforms.master.master_patient_substatus - INFO - PUBLISH - that's it - its a GO - just provide the final dataframe to the var final_dataframe and we take it from there\n",
      "2019-08-27 17:15:53,563 - core.dataset_contract.DatasetContract - INFO - Publishing dataframe to s3 location s3://ichain-dev/dc-673_standardization_patientsubstatus_for_alkermes/sun/ilumya/master/master_patient_substatus with run ID 3.\n",
      "2019-08-27 17:15:53,566 - core.dataset_contract.DatasetContract - INFO - Setting environment variables to Core sandbox service account...\n",
      "2019-08-27 17:15:53,692 - urllib3.util.retry - DEBUG - Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n",
      "2019-08-27 17:15:53,697 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): secretsmanager.us-east-1.amazonaws.com:443\n",
      "2019-08-27 17:15:53,922 - urllib3.connectionpool - DEBUG - https://secretsmanager.us-east-1.amazonaws.com:443 \"POST / HTTP/1.1\" 200 354\n",
      "2019-08-27 17:15:53,935 - core.dataset_contract.DatasetContract - DEBUG - Publishing dataframe to Redshift Spectrum database ichain_core to schema.table                 data_core.sun_ilumya_master_patient_substatus...\n",
      "2019-08-27 17:15:53,940 - s3parq.publish_parq - DEBUG - Found redshift parameters. Checking validity of params...\n",
      "2019-08-27 17:15:53,943 - s3parq.publish_parq - DEBUG - Checking redshift params are correctly formatted\n",
      "2019-08-27 17:15:53,946 - s3parq.publish_parq - DEBUG - Done checking redshift params\n",
      "2019-08-27 17:15:53,950 - s3parq.publish_parq - DEBUG - Redshift parameters valid. Opening Session helper.\n",
      "2019-08-27 17:15:54,155 - urllib3.util.retry - DEBUG - Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n",
      "2019-08-27 17:15:54,156 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): iam.amazonaws.com:443\n",
      "2019-08-27 17:15:54,268 - urllib3.connectionpool - DEBUG - https://iam.amazonaws.com:443 \"POST / HTTP/1.1\" 200 473\n",
      "2019-08-27 17:15:54,331 - urllib3.util.retry - DEBUG - Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n",
      "2019-08-27 17:15:54,333 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): redshift.us-east-1.amazonaws.com:443\n",
      "2019-08-27 17:15:54,485 - urllib3.connectionpool - DEBUG - https://redshift.us-east-1.amazonaws.com:443 \"POST / HTTP/1.1\" 200 487\n",
      "2019-08-27 17:15:54,638 - s3parq.publish_redshift - INFO - Running query to create schema: CREATE EXTERNAL SCHEMA IF NOT EXISTS data_core                 FROM DATA CATALOG                 database 'ichain_core'                 iam_role 'arn:aws:iam::265991248033:role/mySpectrumRole';\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-27 17:15:55,117 - s3parq.publish_parq - DEBUG - Schema data_core created. Creating table sun_ilumya_master_patient_substatus...\n",
      "2019-08-27 17:15:55,173 - s3parq.publish_redshift - DEBUG - Determining write metadata for publish...\n",
      "2019-08-27 17:15:55,175 - s3parq.publish_redshift - DEBUG - Determining write metadata for publish...\n",
      "2019-08-27 17:15:55,389 - s3parq.publish_parq - DEBUG - Table sun_ilumya_master_patient_substatus created.\n",
      "2019-08-27 17:15:55,391 - s3parq.publish_parq - INFO - Checking params...\n",
      "2019-08-27 17:15:55,393 - s3parq.publish_parq - DEBUG - Checking partition args...\n",
      "2019-08-27 17:15:55,398 - s3parq.publish_parq - DEBUG - Done checking partitions.\n",
      "2019-08-27 17:15:55,400 - s3parq.publish_parq - INFO - Params valid.\n",
      "2019-08-27 17:15:55,403 - s3parq.publish_parq - DEBUG - Begin writing to S3..\n",
      "2019-08-27 17:15:55,430 - s3parq.publish_parq - DEBUG - row size estimate: 2904 bytes.\n",
      "number of rows: 24447 rows\n",
      "frame size estimate: 70994088 bytes\n",
      "compression ratio: 4:1\n",
      "ideal size: 251658240.0 bytes\n",
      "\n",
      "2019-08-27 17:15:55,433 - s3parq.publish_parq - INFO - Writing to S3...\n",
      "2019-08-27 17:15:55,562 - s3parq.publish_parq - DEBUG - Writing to s3 location: s3://ichain-dev/dc-673_standardization_patientsubstatus_for_alkermes/sun/ilumya/master/master_patient_substatus...\n",
      "2019-08-27 17:15:55,565 - s3fs.core - DEBUG - Open S3 connection.  Anonymous: False\n",
      "2019-08-27 17:15:55,961 - urllib3.util.retry - DEBUG - Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n",
      "2019-08-27 17:15:55,962 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): ichain-dev.s3.amazonaws.com:443\n",
      "2019-08-27 17:15:56,584 - urllib3.connectionpool - DEBUG - https://ichain-dev.s3.amazonaws.com:443 \"PUT /dc-673_standardization_patientsubstatus_for_alkermes/sun/ilumya/master/master_patient_substatus/__metadata_run_id%3D3/465e42b43ccf496096581a21c6f5e956.parquet HTTP/1.1\" 200 0\n",
      "2019-08-27 17:15:56,603 - s3parq.publish_parq - DEBUG - Done writing to location.\n",
      "2019-08-27 17:15:56,620 - urllib3.util.retry - DEBUG - Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n",
      "2019-08-27 17:15:56,622 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): ichain-dev.s3.amazonaws.com:443\n",
      "2019-08-27 17:15:56,713 - urllib3.connectionpool - DEBUG - https://ichain-dev.s3.amazonaws.com:443 \"GET /?prefix=dc-673_standardization_patientsubstatus_for_alkermes%2Fsun%2Filumya%2Fmaster%2Fmaster_patient_substatus&encoding-type=url HTTP/1.1\" 200 None\n",
      "2019-08-27 17:15:56,720 - urllib3.util.retry - DEBUG - Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n",
      "2019-08-27 17:15:56,740 - urllib3.connectionpool - DEBUG - https://ichain-dev.s3.amazonaws.com:443 \"HEAD /dc-673_standardization_patientsubstatus_for_alkermes/sun/ilumya/master/master_patient_substatus/__metadata_run_id%3D3/465e42b43ccf496096581a21c6f5e956.parquet HTTP/1.1\" 200 0\n",
      "2019-08-27 17:15:56,743 - s3parq.publish_redshift - INFO - Running query to create: ALTER TABLE data_core.sun_ilumya_master_patient_substatus             ADD IF NOT EXISTS PARTITION (__metadata_run_id='3')             LOCATION 's3://ichain-dev/dc-673_standardization_patientsubstatus_for_alkermes/sun/ilumya/master/master_patient_substatus/__metadata_run_id=3';\n",
      "2019-08-27 17:15:57,371 - s3parq.publish_parq - DEBUG - Appending metadata to file dc-673_standardization_patientsubstatus_for_alkermes/sun/ilumya/master/master_patient_substatus/__metadata_run_id=3/465e42b43ccf496096581a21c6f5e956.parquet..\n",
      "2019-08-27 17:15:57,372 - s3parq.publish_parq - DEBUG - Determining write metadata for publish...\n",
      "2019-08-27 17:15:57,379 - s3parq.publish_parq - DEBUG - Done.Metadata set as {'__metadata_run_id': 'integer'}\n",
      "2019-08-27 17:15:57,384 - urllib3.util.retry - DEBUG - Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n",
      "2019-08-27 17:15:57,475 - urllib3.connectionpool - DEBUG - https://ichain-dev.s3.amazonaws.com:443 \"PUT /dc-673_standardization_patientsubstatus_for_alkermes/sun/ilumya/master/master_patient_substatus/__metadata_run_id%3D3/465e42b43ccf496096581a21c6f5e956.parquet HTTP/1.1\" 200 234\n",
      "2019-08-27 17:15:57,478 - s3parq.publish_parq - DEBUG - Done appending metadata.\n",
      "2019-08-27 17:15:57,481 - s3parq.publish_parq - DEBUG - Done writing to S3.\n"
     ]
    }
   ],
   "source": [
    "## that's it - just provide the final dataframe to the var final_dataframe and we take it from there\n",
    "if go==True:\n",
    "    logger.info(\"PUBLISH - that's it - its a GO - just provide the final dataframe to the var final_dataframe and we take it from there\")\n",
    "    transform.publish_contract.publish(final_dataframe, run_id, session)\n",
    "elif go==False:\n",
    "    logger.info(\"PUBLISH -  go no go = NO go -  so DONT publish\")\n",
    "else:\n",
    "    go=False\n",
    "    logger.info(\"PUBLISH -  go no go = unknown make it NO go - so DONT publish\")    \n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
