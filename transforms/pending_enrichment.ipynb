{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-02 18:09:00,109 - core.helpers.session_helper.SessionHelper - INFO - Creating session for dev environment...\n",
      "2019-08-02 18:09:00,133 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating administrator mocks.\n",
      "2019-08-02 18:09:00,172 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating administrator mocks.\n",
      "2019-08-02 18:09:00,173 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pharmaceutical company mocks.\n",
      "2019-08-02 18:09:00,179 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pharmaceutical company mocks.\n",
      "2019-08-02 18:09:00,181 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating brand mocks.\n",
      "2019-08-02 18:09:00,185 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating brand mocks.\n",
      "2019-08-02 18:09:00,186 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating segment mocks.\n",
      "2019-08-02 18:09:00,190 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating segment mocks.\n",
      "2019-08-02 18:09:00,192 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline type mocks.\n",
      "2019-08-02 18:09:00,198 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline type mocks.\n",
      "2019-08-02 18:09:00,199 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline mocks.\n",
      "2019-08-02 18:09:00,206 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline mocks.\n",
      "2019-08-02 18:09:00,207 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline state type mocks.\n",
      "2019-08-02 18:09:00,214 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline state type mocks.\n",
      "2019-08-02 18:09:00,215 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline state mocks.\n",
      "2019-08-02 18:09:00,220 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline state mocks.\n",
      "2019-08-02 18:09:00,221 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating tag mocks.\n",
      "2019-08-02 18:09:00,224 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating tag mocks.\n",
      "2019-08-02 18:09:00,225 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation_template mocks.\n",
      "2019-08-02 18:09:00,232 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_template mocks.\n",
      "2019-08-02 18:09:00,233 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating bridge table mocks for transformation_templates <=> tags.\n",
      "2019-08-02 18:09:00,239 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_templates_tags mocks.\n",
      "2019-08-02 18:09:00,241 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation mocks.\n",
      "2019-08-02 18:09:00,248 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation mocks.\n",
      "2019-08-02 18:09:00,249 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation_variables mocks\n",
      "2019-08-02 18:09:00,254 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_variables mocks.\n",
      "2019-08-02 18:09:00,256 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating run_events mocks.\n",
      "2019-08-02 18:09:00,260 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating run_events mocks.\n",
      "2019-08-02 18:09:00,263 - core.helpers.session_helper.SessionHelper - INFO - Done. Created dev session with mock data.\n"
     ]
    }
   ],
   "source": [
    "from core.helpers.session_helper import SessionHelper\n",
    "session = SessionHelper().session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ CONFIGURATION - PLEASE TOUCH **************\n",
    "Pipeline Builder configuration: creates configurations from variables specified here!!\n",
    "This cell will be off in production as configurations will come from the configuration postgres DB.\n",
    "\"\"\"\n",
    "# config vars: this dataset\n",
    "config_pharma = \"sun\" # the pharmaceutical company which owns {brand}\n",
    "config_brand = \"ilumya\" # the brand this pipeline operates on\n",
    "config_state = \"enrich\" # the state this transform runs in\n",
    "config_name = \"pending_enrichment\" # the name of this transform, which is the name of this notebook without .ipynb\n",
    "\n",
    "# input vars: dataset to fetch. Recall that a contract published to S3 has a key format branch/pharma/brand/state/name\n",
    "input_pharma = \"sun\"\n",
    "input_brand = \"ilumya\"\n",
    "input_state = \"ingest\"\n",
    "input_name = \"symphony_health_association_ingest_column_mapping\"\n",
    "input_branch = \"sun-extract-validation\" # if None, input_branch is automagically set to your working branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-02 18:09:00,332 - core.logging - DEBUG - Adding/getting mocks for specified configurations...\n",
      "2019-08-02 18:09:00,378 - core.logging - DEBUG - Done. Creating mock run event and committing results to configuration mocker.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "Populating config mocker based on config parameters...\n",
    "\"\"\"\n",
    "import core.helpers.pipeline_builder as builder\n",
    "\n",
    "ids = builder.build(config_pharma, config_brand, config_state, config_name, session)\n",
    "transform_id = ids[0]\n",
    "run_id = ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "This section imports data from the configuration database\n",
    "and should not need to be altered or otherwise messed with. \n",
    "~~These are not the droids you are looking for~~\n",
    "\"\"\"\n",
    "from core.constants import BRANCH_NAME, ENV_BUCKET\n",
    "from core.models.configuration import Transformation\n",
    "from dataclasses import dataclass\n",
    "from core.dataset_contract import DatasetContract\n",
    "\n",
    "db_transform = session.query(Transformation).filter(Transformation.id == transform_id).one()\n",
    "\n",
    "@dataclass\n",
    "class DbTransform:\n",
    "    id: int = db_transform.id ## the instance id of the transform in the config app\n",
    "    name: str = db_transform.transformation_template.name ## the transform name in the config app\n",
    "    state: str = db_transform.pipeline_state.pipeline_state_type.name ## the pipeline state, one of raw, ingest, master, enhance, enrich, metrics, dimensional\n",
    "    branch:str = BRANCH_NAME ## the git branch for this execution \n",
    "    brand: str = db_transform.pipeline_state.pipeline.brand.name ## the pharma brand name\n",
    "    pharmaceutical_company: str = db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name # the pharma company name\n",
    "    publish_contract: DatasetContract = DatasetContract(branch=BRANCH_NAME,\n",
    "                            state=db_transform.pipeline_state.pipeline_state_type.name,\n",
    "                            parent=db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name,\n",
    "                            child=db_transform.pipeline_state.pipeline.brand.name,\n",
    "                            dataset=db_transform.transformation_template.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE Cartridge Notebook::[transform name here]\n",
    "![CORE Logo](assets/coreLogo.png) \n",
    "\n",
    "---\n",
    "## Keep in Mind\n",
    "Good Transforms Are...\n",
    "- **singular in purpose:** good transforms do one and only one thing, and handle all known cases for that thing. \n",
    "- **repeatable:** transforms should be written in a way that they can be run against the same dataset an infinate number of times and get the same result every time. \n",
    "- **easy to read:** 99 times out of 100, readable, clear code that runs a little slower is more valuable than a mess that runs quickly. \n",
    "- **No 'magic numbers':** if a variable or function is not instantly obvious as to what it is or does, without context, maybe consider renaming it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow - how to use this notebook to make science\n",
    "#### Data Science\n",
    "1. **Document your transform.** Fill out the _description_ cell below describing what it is this transform does; this will appear in the configuration application where Ops will create, configure and update pipelines. \n",
    "1. **Define your config object.** Fill out the _configuration_ cell below the commented-out guide to define the variables you want ops to set in the configuration application (these will populate here for every pipeline). \n",
    "2. **Build your transformation logic.** Use the transformation cell to do that magic that you do. \n",
    "![caution](assets/cautionTape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "********* VARIABLES - PLEASE TOUCH ********* \n",
    "This section defines what you expect to get from the configuration application \n",
    "in a single \"transform\" object. Define the vars you need here, and comment inline to the right of them \n",
    "for all-in-one documentation. \n",
    "Engineering will build a production \"transform\" object for every pipeline that matches what you define here.\n",
    "\n",
    "@@@ FORMAT OF THE DATA CLASS IS: @@@ \n",
    "\n",
    "<variable_name>: <data_type> #<comment explaining what the value is to future us>\n",
    "\n",
    "e.g.\n",
    "\n",
    "class Transform(DbTransform):\n",
    "    some_ratio: float\n",
    "    site_name: str\n",
    "\n",
    "~~These ARE the droids you are looking for~~\n",
    "\"\"\"\n",
    "\n",
    "class Transform(DbTransform):\n",
    "    '''\n",
    "    YOUR properties go here!!\n",
    "    Variable properties should be assigned to the exact name of\n",
    "    the transformation as it appears in the Jupyter notebook filename.\n",
    "    '''\n",
    "\n",
    "    sort_columns: list # List of Long-ID, Pharmacy Code, Brand/Medication and Status Date used to sort the dataframe. Order of variables matters\n",
    "    substatus_list: list # List of integrichain substatuses for current customer. Order of variables does not matter.\n",
    "    pjh: str # Patient Journey Hiearchy column, should have been added in an early transform\n",
    "    ic_status: str # Integrichain Status column, should have been added in an early transform\n",
    "    ic_substatus: str # Integrichain Sub Status column, should have been added in an early transform\n",
    "    bvpa: str # BV/PA string in Patient Journey Hierarchy column. String should be something like 'BV/PA'\n",
    "    intake: str # Intake string in Patient Journey Hierarchy column. String should be something like 'INTAKE'\n",
    "    fulfillment: str # Fulfillment string in Patient Journey Hierarchy column. String should be something like 'FULFILLMENT'\n",
    "    pending: str # Pending string in Integrichain Status column. String should be something like 'PENDING'\n",
    "    \n",
    "    \n",
    "    def pending_enrichment(self,df):\n",
    "\n",
    "        df = df.sort_values(self.sort_columns,ascending=[True, True, True, True])\n",
    "\n",
    "        groupby_cols = self.sort_columns[:-1]\n",
    "        status_date = self.sort_columns[-1]\n",
    "        \n",
    "        min_bvpa, max_bvpa, max_bvpa_intake, min_bvpa_fulfillment = self._create_bvpa(df=df, groupby_cols=groupby_cols, status_date=status_date)\n",
    "        \n",
    "        df = pd.merge(df, min_bvpa_fulfillment, how='left', on=groupby_cols)\n",
    "        df = pd.merge(df, min_bvpa, how='left', on=groupby_cols)\n",
    "        df = pd.merge(df, max_bvpa, how='left', on=groupby_cols)\n",
    "        df = pd.merge(df, max_bvpa_intake, how='left', on=groupby_cols)\n",
    "\n",
    "        enrich_df = df[(df[self.ic_status] == self.pending) & (df[self.ic_substatus].isin(self.substatus_list))]\n",
    "        df = df[~((df[self.ic_status] == self.pending) & (df[self.ic_substatus].isin(self.substatus_list)))]\n",
    "\n",
    "        enrich_df.loc[:,'Before_min_fulfillment'] = np.where(enrich_df.loc[:,status_date] < enrich_df.loc[:,'min_bvpa_fulfillment'],1,0)\n",
    "        enrich_df.loc[:,'After_min_BVPA'] = np.where(enrich_df.loc[:,status_date] >= enrich_df.loc[:,'min_bvpa_date'],1,0)\n",
    "        enrich_df.loc[:,'Before_max_BVPA'] = np.where(enrich_df.loc[:,status_date] <= enrich_df.loc[:,'max_bvpa_date'],1,0)\n",
    "        enrich_df.loc[:,'After_max_intake'] = np.where(enrich_df.loc[:,status_date] > enrich_df.loc[:,'max_bvpa_intake_date'],1,0)\n",
    "        \n",
    "        enrich_df.loc[(enrich_df.Before_min_fulfillment == 0) & (enrich_df.After_min_BVPA == 0) & (enrich_df.Before_max_BVPA == 1), 'Before_min_fulfillment'] = 1\n",
    "        enrich_df.loc[(enrich_df.Before_min_fulfillment == 1) & (enrich_df.After_min_BVPA == 0) & (enrich_df.Before_max_BVPA == 1), 'Before_max_BVPA'] = 0\n",
    "\n",
    "        enrich_df.loc[(enrich_df.After_max_intake == 0) & (enrich_df.Before_max_BVPA == 0) & (enrich_df.After_min_BVPA == 1), 'After_max_intake'] = 1\n",
    "        enrich_df.loc[(enrich_df.After_max_intake == 1) & (enrich_df.Before_max_BVPA == 0) & (enrich_df.After_min_BVPA == 1), 'After_min_BVPA'] = 0\n",
    "        \n",
    "        enrich_df.loc[:,self.pjh] = (\n",
    "            np.where(enrich_df.loc[:,'Before_min_fulfillment'] == 1, self.intake,\n",
    "            np.where((enrich_df.loc[:,'After_min_BVPA'] == 1) & (enrich_df.loc[:,'Before_max_BVPA'] == 1),self.bvpa,\n",
    "            np.where(enrich_df.loc[:,'After_max_intake'] == 1, self.fulfillment,\n",
    "            enrich_df.loc[:,self.pjh])))\n",
    "        )\n",
    "        \n",
    "        df = pd.concat([df,enrich_df],sort=False)\n",
    "        \n",
    "        final_df = df.drop(labels=['After_min_BVPA','After_max_intake','Before_max_BVPA','Before_min_fulfillment','min_bvpa_date','max_bvpa_intake_date','max_bvpa_date','min_bvpa_fulfillment'],axis=1)\n",
    "        \n",
    "        return final_df,df\n",
    "    \n",
    "        \n",
    "    def _create_bvpa(self, df, groupby_cols, status_date):\n",
    "        \n",
    "        bvpa_df = df[(df[self.pjh] == self.bvpa)]\n",
    "\n",
    "        intake_df = df[(df.Patient_Journey_Hierarchy.isin([self.bvpa,self.intake]))]\n",
    "\n",
    "        fulfillment_df = df[(df.Patient_Journey_Hierarchy.isin([self.bvpa,self.fulfillment]))]\n",
    "\n",
    "        min_bvpa = (\n",
    "            bvpa_df\n",
    "            .groupby(groupby_cols)[status_date]\n",
    "            .min()\n",
    "            .reset_index(drop=False)\n",
    "            .rename(columns={status_date:'min_bvpa_date'})\n",
    "        )\n",
    "        \n",
    "        max_bvpa = (\n",
    "            bvpa_df\n",
    "            .groupby(groupby_cols)[status_date]\n",
    "            .max()\n",
    "            .reset_index(drop=False)\n",
    "            .rename(columns={status_date:'max_bvpa_date'})\n",
    "        )\n",
    "        \n",
    "        max_bvpa_intake = (\n",
    "            intake_df\n",
    "            .groupby(groupby_cols)[status_date]\n",
    "            .max()\n",
    "            .reset_index(drop=False)\n",
    "            .rename(columns={status_date:'max_bvpa_intake_date'})\n",
    "        )\n",
    "        \n",
    "        min_bvpa_fulfillment = (\n",
    "            fulfillment_df\n",
    "            .groupby(groupby_cols)[status_date]\n",
    "            .min()\n",
    "            .reset_index(drop=False)\n",
    "            .rename(columns={status_date:'min_bvpa_fulfillment'})\n",
    "        )\n",
    "\n",
    "        return min_bvpa, max_bvpa, max_bvpa_intake, min_bvpa_fulfillment\n",
    "    \n",
    "transform = Transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please place your value assignments for development here!!\n",
    "## This cell will be turned off in production and Engineering will set to pull from the configuration application instead\n",
    "## For the last example, this could look like...\n",
    "## transform.some_ratio = 0.6\n",
    "## transform.site_name = \"WALGREENS\"\n",
    "\n",
    "transform.sort_columns = ['msa_patient_id', 'pharm_code', 'medication', 'status_date']\n",
    "transform.pjh = 'Patient_Journey_Hierarchy'\n",
    "transform.ic_status = 'integrichain_status'\n",
    "transform.ic_substatus = 'integrichain_sub_status'\n",
    "transform.bvpa = 'BV/PA'\n",
    "transform.intake = 'INTAKE'\n",
    "transform.fulfillment = 'FULFILLMENT'\n",
    "transform.pending = 'PENDING'\n",
    "transform.substatus_list = ['OTHER','PATIENT CONTACT','PATIENT RESPONSE','PATIENT HOLD','PATIENT FINANCIAL','PRESCRIBER','READY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "What does this transformation do? be specific.\n",
    "\n",
    "![what does your transform do](assets/what.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pending Enrichment Flowchart](assets/PendingEnrichment.png)\n",
    "\n",
    "Replaces ambiguous substatus with info based on sub-statuses surrounding the ambiguous field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-02 18:36:00,578 - core.dataset_contract.DatasetContract - INFO - Fetching dataframe from s3 location s3://ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "************ FETCH DATA - TOUCH, BUT CAREFULLY **************\n",
    "This cell will be turned off in production, as the input_contract will be handled by the pipeline.\n",
    "\"\"\"\n",
    "\n",
    "if not input_branch:\n",
    "    input_branch = BRANCH_NAME\n",
    "input_contract = DatasetContract(branch=input_branch, state=input_state, parent=input_pharma, child=input_brand, dataset=input_name)\n",
    "run_filter = []\n",
    "# run_filter.append(dict(partition=\"__metadata_run_id\", comparison=\"==\", values=[1]))\n",
    "# IF YOU HAVE PUBLISHED DATA MULTIPLE TIMES, uncomment the above line and change the int to the run_id to fetch.\n",
    "# Otherwise, you will have duplicate values in your fetched dataset!\n",
    "df = input_contract.fetch(filters=run_filter)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns=999\n",
    "# pd.options.display.max_rows=999\n",
    "\n",
    "df.status_date = df.status_date.str[:8].astype(str)\n",
    "df.ref_date = df.ref_date.str[:8].astype(str)\n",
    "\n",
    "df.status_date = pd.to_datetime(df.status_date, infer_datetime_format=True, errors='coerce')\n",
    "df.ref_date = pd.to_datetime(df.ref_date, infer_datetime_format=True, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('{}'.format(os.path.expanduser('~')))\n",
    "status_config = pd.read_csv('status_mapping.csv')\n",
    "\n",
    "status_config.loc[:,'statusCode'] = status_config.statusCode.str.upper()\n",
    "status_config.loc[:,'subStatus'] = status_config.subStatus.str.upper()\n",
    "status_config.loc[:,'integrichain_sub_status'] = status_config.integrichain_sub_status.str.upper()\n",
    "status_config.loc[:,'integrichain_status'] = status_config.integrichain_status.str.upper()\n",
    "status_config.loc[:,'Patient_Journey_Hierarchy'] = status_config.Patient_Journey_Hierarchy.str.upper()\n",
    "\n",
    "status_config = status_config.rename(columns={'statusCode':'status_code','subStatus':'sub_status'})\n",
    "\n",
    "df.sub_status = df.sub_status.str.replace('PRESCRIBERHOLD','PRESCRIBER HOLD')\n",
    "\n",
    "df = pd.merge(df,status_config,on=['status_code','sub_status'])\n",
    "\n",
    "df = df[['rec_date', 'pharm_code', 'pharm_npi', 'transtype', 'pharm_transaction_id', 'trans_seq', 'ref_source', 'ref_date', 'program_id', 'pharmacy_id', 'pat_last_name', 'pat_first_name', 'pat_dob', 'pat_gender', \n",
    "         'pat_addr1', 'pat_addr2', 'pat_city', 'pat_state', 'pat_zip', 'dx1_code', 'dx2_code', 'status_date', 'status_code', 'sub_status', 'integrichain_status','integrichain_sub_status', 'Patient_Journey_Hierarchy', \n",
    "         'pres_last_name', 'pres_first_name', 'pres_addr1', 'pres_addr2', 'pres_city', 'pres_state', 'pres_zip', 'pres_phone', 'pres_npi', 'pres_dea', 'facility_name', 'rxdate', 'rxnumber', 'rxrefills', 'rxfill', \n",
    "         'refill_remaining', 'prev_disp', 'rx_ndc_number', 'medication', 'quantity', 'day_supply', 'ship_date', 'ship_carrier', 'shiptracking_num', 'ship_location', 'ship_address', 'ship_city', 'ship_state', 'ship_zip', \n",
    "         'has_medical', 'primary_coverage_type', 'primary_payer_name', 'primary_payer_type', 'secondary_coverage_type', 'secondary_payer_name', 'secondary_payer_type', 'plan_paid_amt', 'pat_copay', 'copay_assist_amount', \n",
    "         'oth_payer_amt', 'xfer_pharmname', 'msa_patient_id', 'msa_patient_bmap', '__metadata_run_timestamp', '__metadata_app_version', '__metadata_output_contract', '__metadata_transform_timestamp', '__metadata_run_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use the variables above to execute your transformation. the final output needs to be a variable named final_dataframe\n",
    "final_dataframe, unittest_df = transform.pending_enrichment(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.loc[(df.Before_min_fulfillment == 0) & (df.After_min_BVPA == 0) & (df.Before_max_BVPA == 1), 'Before_min_fulfillment'] == 1\n",
    "df.loc[df.Before_min_fulfillment == 1 & (df.After_min_BVPA == 0) & (df.Before_max_BVPA == 1), 'Before_max_BVPA'] == 0\n",
    "\n",
    "df.loc[(df.After_max_intake == 0) & (df.Before_max_BVPA == 0) & (df.After_min_BVPA == 1), 'After_max_intake'] == 1\n",
    "df.loc[df.After_max_intake == 1 & (df.Before_max_BVPA == 0) & (df.After_min_BVPA == 1), 'After_min_BVPA'] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "def shape_status(final_dataframe,df):\n",
    "    return df.shape[0] == final_dataframe.shape[0]\n",
    "\n",
    "def dummy_fail(df):\n",
    "    df['binary_series_1'] = df.Before_min_fulfillment + df.Before_max_BVPA + df.After_max_intake\n",
    "    df['binary_series_2'] = df.Before_min_fulfillment + df.After_min_BVPA + df.After_max_intake\n",
    "    return df[(df.binary_series_1 > 1) & (df.binary_series_2 > 1)].shape[0] == 0\n",
    "\n",
    "def between_status(df):\n",
    "    df['binary_series'] = df.Before_max_BVPA + df.After_min_BVPA\n",
    "    return df[(unittest_df.binary_series == 1)].shape[0] == 0\n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "    \n",
    "    def test_shape_status(self):\n",
    "        self.assertEqual(shape_status(final_dataframe,df),True)\n",
    "        \n",
    "    def test_dummy_fail(self):\n",
    "        self.assertEqual(dummy_fail(unittest_df),True)\n",
    "        \n",
    "    def test_between_status(self):\n",
    "        self.assertEqual(between_status(unittest_df),True)\n",
    "    \n",
    "unittest.main(argv=[''],verbosity=2,exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FULFILLMENT    14072\n",
       "BV/PA          12247\n",
       "INTAKE          3812\n",
       "PAYER           2748\n",
       "PROVIDER        2541\n",
       "PATIENT         1460\n",
       "TRANSFERRED      381\n",
       "FINANCIAL        144\n",
       "Name: Patient_Journey_Hierarchy, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fulfill <= Intake >=\n",
    "final_dataframe.Patient_Journey_Hierarchy.value_counts()\n",
    "# Fails test_dummy_fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FULFILLMENT    14069\n",
       "BV/PA          12250\n",
       "INTAKE          3812\n",
       "PAYER           2748\n",
       "PROVIDER        2541\n",
       "PATIENT         1460\n",
       "TRANSFERRED      381\n",
       "FINANCIAL        144\n",
       "Name: Patient_Journey_Hierarchy, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BVPA, Fulfillment <=\n",
    "final_dataframe.Patient_Journey_Hierarchy.value_counts()\n",
    "# Fails test_dummy_fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FULFILLMENT    15334\n",
       "BV/PA          12256\n",
       "PAYER           2748\n",
       "PROVIDER        2541\n",
       "INTAKE          2541\n",
       "PATIENT         1460\n",
       "TRANSFERRED      381\n",
       "FINANCIAL        144\n",
       "Name: Patient_Journey_Hierarchy, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BVPA <=, >=\n",
    "final_dataframe.Patient_Journey_Hierarchy.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FULFILLMENT    15340\n",
       "BV/PA          12250\n",
       "PAYER           2748\n",
       "PROVIDER        2541\n",
       "INTAKE          2541\n",
       "PATIENT         1460\n",
       "TRANSFERRED      381\n",
       "FINANCIAL        144\n",
       "Name: Patient_Journey_Hierarchy, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BVPA, Intake >=\n",
    "final_dataframe.Patient_Journey_Hierarchy.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## that's it - just provide the final dataframe to the var final_dataframe and we take it from there\n",
    "transform.publish_contract.publish(final_dataframe, run_id, session)\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
