{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_id = 1\n",
    "\n",
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "This section imports data from the configuration database\n",
    "and should not need to be altered, molested or otherwise messed with. \n",
    "~~These are not the droids you are looking for~~\n",
    "\"\"\"\n",
    "from core.constants import BRANCH_NAME, ENV_BUCKET\n",
    "from core.helpers.session_helper import SessionHelper\n",
    "from core.models.configuration import Transformation\n",
    "from dataclasses import dataclass\n",
    "from core.dataset_contract import DatasetContract\n",
    "\n",
    "db_transform = SessionHelper().session.query(Transformation).filter(Transformation.id == transform_id).one()\n",
    "\n",
    "@dataclass\n",
    "class DbTransform:\n",
    "    id: int = db_transform.id ## the instance id of the transform in the config app\n",
    "    name: str = db_transform.transformation_template.name ## the transform name in the config app\n",
    "    state: str = db_transform.pipeline_state.pipeline_state_type.name ## the pipeline state, one of raw, ingest, master, enhance, enrich, metrics, dimensional\n",
    "    branch:str = BRANCH_NAME ## the git branch for this execution \n",
    "    brand: str = db_transform.pipeline_state.pipeline.brand.name ## the pharma brand name\n",
    "    pharmaceutical_company: str = db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name # the pharma company name\n",
    "    publish_contract: DatasetContract = DatasetContract(branch=BRANCH_NAME,\n",
    "                            state=db_transform.pipeline_state.pipeline_state_type.name,\n",
    "                            parent=db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name,\n",
    "                            child=db_transform.pipeline_state.pipeline.brand.name,\n",
    "                            dataset=db_transform.transformation_template.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE Cartridge Notebook::[Referral Date Enrichment]\n",
    "![CORE Logo](assets/coreLogo.png) \n",
    "\n",
    "---\n",
    "## Keep in Mind\n",
    "Good Transforms Are...\n",
    "- **singular in purpose:** good transforms do one and only one thing, and handle all known cases for that thing. \n",
    "- **repeatable:** transforms should be written in a way that they can be run against the same dataset an infinate number of times and get the same result every time. \n",
    "- **easy to read:** 99 times out of 100, readable, clear code that runs a little slower is more valuable than a mess that runs quickly. \n",
    "- **No 'magic numbers':** if a variable or function is not instantly obvious as to what it is or does, without context, maybe consider renaming it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow - how to use this notebook to make science\n",
    "#### Data Science\n",
    "1. **Document your transform.** Fill out the _description_ cell below describing what it is this transform does; this will appear in the configuration application where Ops will create, configure and update pipelines. \n",
    "1. **Define your config object.** Fill out the _configuration_ cell below the commented-out guide to define the variables you want ops to set in the configuration application (these will populate here for every pipeline). \n",
    "2. **Build your transformation logic.** Use the transformation cell to do that magic that you do. \n",
    "![caution](assets/cautionTape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "********* CONFIGURATION - PLEASE TOUCH ********* \n",
    "This section defines what you expect to get from the configuration application \n",
    "in a single \"transform\" object. Define the vars you need here, and comment inline to the right of them \n",
    "for all-in-one documentation. \n",
    "Engineering will build a production \"transform\" object for every pipeline that matches what you define here.\n",
    "\n",
    "@@@ FORMAT OF THE DATA CLASS IS: @@@ \n",
    "\n",
    "<value_name>: <data_type> #<comment explaining what the value is to future us>\n",
    "\n",
    "~~These ARE the droids you are looking for~~\n",
    "\"\"\"\n",
    "\n",
    "class Transform(DbTransform):\n",
    "    '''\n",
    "    YOUR properties go here!!\n",
    "    Include your input dataset(s) here. Variable properties should be assigned to the exact name of\n",
    "    the transformation as it appears in the Jupyter notebook filename.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please place your value assignments for development here!!\n",
    "## This cell will be turned off in production and Engineering will set to pull form the configuration application instead\n",
    "trans_id = 'pmcTransactionId'\n",
    "product = 'medication'\n",
    "patient_id = 'pmcPatientId'\n",
    "pharm = 'pharmName'\n",
    "status_date = 'statusDate'\n",
    "ref_date = 'refDate'\n",
    "status =  'statusCode'\n",
    "substatus =  'custStatusCode'\n",
    "ic_status = 'integrichain_status'\n",
    "ic_substatus = 'integrichain_sub_status'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "What does this transformation do? be specific.\n",
    "\n",
    "![what does your transform do](assets/what.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Transform provides the preliminary enrichment format. Takes patient data in from S3 and adjusts all possibly inaccurate reference dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Transform()\n",
    "\n",
    "transform.name = 'DS_290'\n",
    "transform.brand = 'ofev'\n",
    "transform.state = 'enrich'\n",
    "transform.pharmaceutical_company = 'bi'\n",
    "transform.filesystem_path = 's3://ichain-dev/movahlx/bi/transactions'\n",
    "\n",
    "transform.secret_name = 'bi'\n",
    "transform.secret_type_of = 'database'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from s3fs import S3FileSystem\n",
    "#import mysql.connector as mysql\n",
    "\n",
    "from core.secret import Secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_from_parquet_s3(file_path):\n",
    "    \n",
    "    s3 = S3FileSystem()\n",
    "    df = (\n",
    "        pq\n",
    "        .ParquetDataset(file_path, filesystem=s3)\n",
    "        .read_pandas()\n",
    "        .to_pandas()\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def pandas_from_database(query, *args):\n",
    "    \n",
    "    connection = mysql.connect(\n",
    "        host=secret.host, \n",
    "        user=secret.user, \n",
    "        passwd=secret.password, \n",
    "        port=secret.port, \n",
    "        charset='utf8'\n",
    "    )\n",
    "    \n",
    "    query = query.format(*args)\n",
    "    df = pd.read_sql(sql=query, con=connection)\n",
    "    connection.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def pandas_to_parquet_s3(df, partition, file_path):\n",
    "    \n",
    "    s3 = S3FileSystem()\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    \n",
    "    pq.write_to_dataset(table, file_path, partition_cols=partition, filesystem=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas_from_parquet_s3(transform.filesystem_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_file(cust_df, trans_id, product, patient_id, pharm, status_date, ref_date, status, substatus):\n",
    "    \n",
    "    # Extract and map relevant columns\n",
    "    #cust_df = cust_input_df.loc[:,[trans_id,\n",
    "    #                               patient,\n",
    "    #                               patient_id,\n",
    "    #                               pharm,\n",
    "    #                               product,\n",
    "    #                               status_date,\n",
    "    #                               ref_date,\n",
    "    #                               status,\n",
    "    #                               substatus]]\n",
    "\n",
    "    cust_df = cust_df.rename(columns={trans_id:'trans_id',\n",
    "                                      patient_id:'patient_id',\n",
    "                                      pharm:'pharm',\n",
    "                                      product:'product',\n",
    "                                      status_date:'status_date',\n",
    "                                      ref_date:'ref_date',\n",
    "                                      status:'status_code',\n",
    "                                      substatus:'substatus_code'})\n",
    "    \n",
    "    # Convert dates to datetime format\n",
    "    cust_df.status_date = pd.to_datetime(cust_df.status_date)\n",
    "    cust_df.ref_date = pd.to_datetime(cust_df.ref_date)\n",
    "    \n",
    "    ## Extract brand from medication\n",
    "    cust_df['product'] = cust_df['product'].apply(lambda x: x.split()[0].strip())\n",
    "    \n",
    "\t## Convert status codes to uppercase\n",
    "    cust_df.status_code = cust_df.status_code.str.upper()\n",
    "    cust_df.substatus_code = cust_df.substatus_code.str.upper()\n",
    "    \n",
    "\t## Fill missing referral_date with min(status_date)\n",
    "\t\n",
    "    min_status_dates=cust_df.groupby(['patient_id','pharm','product'])['status_date'].min().reset_index().rename(columns={'status_date':'min_status_date'})\n",
    "    \n",
    "    cust_df = pd.merge(cust_df, min_status_dates, how='inner', on=['patient_id','pharm','product'])\n",
    "    \n",
    "    cust_df.loc[cust_df['ref_date'].isnull(), ['ref_date']] = cust_df['min_status_date']\n",
    "    \n",
    "    cust_df = cust_df.drop(['min_status_date'],axis=1).drop_duplicates()\n",
    "    \n",
    "    cust_df.sort_values(by=['patient_id', 'pharm', 'product', 'status_date'], ascending=[True, True, True, True], inplace=True)\n",
    "    cust_df = cust_df.reset_index(drop=True)\n",
    "\n",
    "    # Output clean customer file\n",
    "    return cust_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def referral_date_enrichment(df: pd.DataFrame, cols, statuses,\n",
    "#                         table_columns,\n",
    "#                         ref_date_enrichment_threshold):\n",
    "def referral_date_enrichment(df: pd.DataFrame, table_columns, ref_date_enrichment_threshold):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "#    status_date = cols.status_date\n",
    "#    ref_date = cols.ref_date\n",
    "#    patient = cols.patient\n",
    "#    pharm = cols.pharm\n",
    "#    product = cols.product\n",
    "    min_date_df = df.groupby(['patient_id', 'pharm', 'product'])['status_date'].min().reset_index().rename(columns={'status_date': 'First_Status_Date'})\n",
    "    df = pd.merge(df, min_date_df, how='inner', on=['patient_id', 'pharm', 'product'])\n",
    "    min_ref_date_df = df.groupby(['patient_id', 'pharm', 'product'])['ref_date'].min().reset_index().rename(columns={'ref_date': 'First_Ref_Date'})\n",
    "    df = pd.merge(df, min_ref_date_df, how='inner', on=['patient_id', 'pharm', 'product'])\n",
    "\n",
    "    df['min_ref_day_diff'] = (df['First_Status_Date'] - df['First_Ref_Date']) / np.timedelta64(1, 'D')\n",
    "    df['ref_day_diff'] = (df['First_Status_Date'] - df['ref_date']) / np.timedelta64(1, 'D')\n",
    "\n",
    "    to_enrich_df = df[(df['min_ref_day_diff'] > ref_date_enrichment_threshold) & (df['ref_day_diff'] > ref_date_enrichment_threshold)]\n",
    "    to_enrich_df['ref_date'] = to_enrich_df['First_Status_Date']\n",
    "    #to_enrich_df = to_enrich_df[table_columns]\n",
    "    enriched_ids = to_enrich_df['trans_id'].values.tolist()\n",
    "    df = df[~(df['trans_id'].isin(enriched_ids))]\n",
    "    df = df.append(to_enrich_df)\n",
    "    #df = df[table_columns]\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_standard(cust_df, trans_id, product, patient_id, pharm, status_date, ref_date, status, substatus):\n",
    "\n",
    "    cust_df = cust_df.rename(columns={'trans_id': trans_id,\n",
    "                                      'patient_id': patient_id,\n",
    "                                      'pharm': pharm,\n",
    "                                      'product': product,\n",
    "                                      'status_date': status_date,\n",
    "                                      'ref_date': ref_date,\n",
    "                                      'status_code': status,\n",
    "                                      'substatus_code': substatus})\n",
    "    return cust_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use the variables above to execute your transformation. the final output needs to be a variable named final_dataframe\n",
    "df = load_clean_file(df, trans_id, product, patient_id, pharm, status_date, ref_date, status, substatus)\n",
    "columns = list(df.columns)\n",
    "final_dataframe = referral_date_enrichment(df, columns, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## that's it - just provide the final dataframe to the var final_dataframe and we take it from there\n",
    "transform.publish_contract.publish(final_dataframe)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
