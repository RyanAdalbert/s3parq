{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE Cartridge Notebook:: Active Shipment Enrichment\n",
    "![CORE Logo](assets/coreLogo.png) \n",
    "\n",
    "---\n",
    "## Keep in Mind\n",
    "Good Transforms Are...\n",
    "- **singular in purpose:** good transforms do one and only one thing, and handle all known cases for that thing. \n",
    "- **repeatable:** transforms should be written in a way that they can be run against the same dataset an infinate number of times and get the same result every time. \n",
    "- **easy to read:** 99 times out of 100, readable, clear code that runs a little slower is more valuable than a mess that runs quickly. \n",
    "- **No 'magic numbers':** if a variable or function is not instantly obvious as to what it is or does, without context, maybe consider renaming it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow - how to use this notebook to make science\n",
    "#### Data Science\n",
    "1. **Document your transform.** Fill out the _description_ cell below describing what it is this transform does; this will appear in the configuration application where Ops will create, configure and update pipelines. \n",
    "1. **Define your config object.** Fill out the _configuration_ cell below the commented-out guide to define the variables you want ops to set in the configuration application (these will populate here for every pipeline). \n",
    "2. **Build your transformation logic.** Use the transformation cell to do that magic that you do. \n",
    "![caution](assets/cautionTape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "What does this transformation do? be specific.\n",
    "\n",
    "![what does your transform do](assets/what.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(clear out and replace with your description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.helpers.session_helper import SessionHelper\n",
    "session = SessionHelper().session"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "************ CONFIGURATION - PLEASE TOUCH **************\n",
    "Pipeline Builder configuration: creates configurations from variables specified here!!\n",
    "This cell will be off in production as configurations will come from the configuration postgres DB.\n",
    "\"\"\"\n",
    "# config vars: this dataset\n",
    "config_pharma = \"alkermes\" # the pharmaceutical company which owns {brand}\n",
    "config_brand = \"vivitrol\" # the brand this pipeline operates on\n",
    "config_state = \"enrich\" # the state this transform runs in\n",
    "config_name = \"active_shipment\" # the name of this transform, which is the name of this notebook without .ipynb\n",
    "\n",
    "# input vars: dataset to fetch. Recall that a contract published to S3 has a key format branch/pharma/brand/state/name\n",
    "input_pharma = \"alkermes\"\n",
    "input_brand = \"vivitrol\"\n",
    "input_state = \"ingest\"\n",
    "input_name = \"patient_status_standardize_numbers\"\n",
    "input_branch = \"ds-321\" # if None, input_branch is automagically set to your working branch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "Populating config mocker based on config parameters...\n",
    "\"\"\"\n",
    "import core.helpers.pipeline_builder as builder\n",
    "\n",
    "ids = builder.build(config_pharma, config_brand, config_state, config_name, session)\n",
    "transform_id = ids[0]\n",
    "run_id = ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "This section imports data from the configuration database\n",
    "and should not need to be altered or otherwise messed with. \n",
    "~~These are not the droids you are looking for~~\n",
    "\"\"\"\n",
    "from core.constants import BRANCH_NAME, ENV_BUCKET\n",
    "from core.helpers.session_helper import SessionHelper\n",
    "from core.models.configuration import Transformation\n",
    "from dataclasses import dataclass\n",
    "from core.dataset_contract import DatasetContract\n",
    "\n",
    "db_transform = session.query(Transformation).filter(Transformation.id == transform_id).one()\n",
    "\n",
    "@dataclass\n",
    "class DbTransform:\n",
    "    id: int = db_transform.id ## the instance id of the transform in the config app\n",
    "    name: str = db_transform.transformation_template.name ## the transform name in the config app\n",
    "    state: str = db_transform.pipeline_state.pipeline_state_type.name ## the pipeline state, one of raw, ingest, master, enhance, enrich, metrics, dimensional\n",
    "    branch:str = BRANCH_NAME ## the git branch for this execution \n",
    "    brand: str = db_transform.pipeline_state.pipeline.brand.name ## the pharma brand name\n",
    "    pharmaceutical_company: str = db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name # the pharma company name\n",
    "    publish_contract: DatasetContract = DatasetContract(branch=BRANCH_NAME,\n",
    "                            state=db_transform.pipeline_state.pipeline_state_type.name,\n",
    "                            parent=db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name,\n",
    "                            child=db_transform.pipeline_state.pipeline.brand.name,\n",
    "                            dataset=db_transform.transformation_template.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "********* VARIABLES - PLEASE TOUCH ********* \n",
    "This section defines what you expect to get from the configuration application \n",
    "in a single \"transform\" object. Define the vars you need here, and comment inline to the right of them \n",
    "for all-in-one documentation. \n",
    "Engineering will build a production \"transform\" object for every pipeline that matches what you define here.\n",
    "\n",
    "@@@ FORMAT OF THE DATA CLASS IS: @@@ \n",
    "\n",
    "<variable_name>: <data_type> #<comment explaining what the value is to future us>\n",
    "\n",
    "e.g.\n",
    "\n",
    "class Transform(DbTransform):\n",
    "    some_ratio: float\n",
    "    site_name: str\n",
    "\n",
    "~~These ARE the droids you are looking for~~\n",
    "\"\"\"\n",
    "\n",
    "class Transform(DbTransform):\n",
    "    '''\n",
    "    YOUR properties go here!!\n",
    "    Variable properties should be assigned to the exact name of\n",
    "    the transformation as it appears in the Jupyter notebook filename.\n",
    "    '''\n",
    "    input_transform: str = db_transform.variables.input_transform # The name of the transform to input source data from\n",
    "    dispense_input_transform: str = db_transform.variables.dispense_input_transform # The name of the transform to input dispense data from\n",
    "    brand_name: str = DbTransform.brand.upper() # Name of brand to be inserted into brand column of Dispense data\n",
    "    active_shipment_status: str = db_transform.variables.active_shipment_status # Status indicating active shipment (customer-specific)\n",
    "    active_shipment_substatus: str = db_transform.variables.active_shipment_substatus # Substatus indicating active shipment (customer-specific)\n",
    "    active_hierarchy: str = db_transform.variables.active_hierarchy # Hierarchy indicating active shipment (customer-specific)\n",
    "    pharmacy_name_map: str = db_transform.variables.pharmacy_name_map # Dictionary (stored as string) with pharmacy names found in Dispense data to be mapped to standardized names, e.g. \"{'CVS':'CVS Specialty'}\" (customer-specific)\n",
    "\n",
    "transform = Transform()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Please place your value assignments for development here!!\n",
    "## This cell will be turned off in production and Engineering will set to pull from the configuration application instead\n",
    "## For the last example, this could look like...\n",
    "## transform.some_ratio = 0.6\n",
    "## transform.site_name = \"WALGREENS\"\n",
    "\n",
    "\n",
    "transform.active_shipment_status = 'ACTIVE'\n",
    "transform.active_shipment_substatus = 'AS01'\n",
    "transform.active_hierarchy = 'FULFILLMENT'\n",
    "transform.pharmacy_name_map = \"{'CVS':'CVS Specialty'}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "************ FETCH DATA - TOUCH, BUT CAREFULLY **************\n",
    "This cell will be turned off in production, as the input_contract will be handled by the pipeline.\n",
    "\"\"\"\n",
    "\n",
    "if not input_branch:\n",
    "    input_branch = BRANCH_NAME\n",
    "input_contract = DatasetContract(branch=input_branch, state=input_state, parent=input_pharma, child=input_brand, dataset=input_name)\n",
    "run_filter = []\n",
    "run_filter.append(dict(partition=\"__metadata_run_id\", comparison=\"==\", values=[2]))\n",
    "# IF YOU HAVE PUBLISHED DATA MULTIPLE TIMES, uncomment the above line and change the int to the run_id to fetch.\n",
    "# Otherwise, you will have duplicate values in your fetched dataset!\n",
    "df = input_contract.fetch(filters=run_filter)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# input vars: dataset to fetch. Recall that a contract published to S3 has a key format branch/pharma/brand/state/name\n",
    "dispense_input_pharma = \"alkermes\"\n",
    "dispense_input_brand = \"vivitrol\"\n",
    "dispense_input_state = \"ingest\"\n",
    "dispense_input_name = \"dispense_ingest_column_mapping\"\n",
    "dispense_input_branch = \"ds-321\" # if None, input_branch is automagically set to your working branch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "************ FETCH DATA - TOUCH, BUT CAREFULLY **************\n",
    "This cell will be turned off in production, as the input_contract will be handled by the pipeline.\n",
    "\"\"\"\n",
    "\n",
    "if not dispense_input_branch:\n",
    "    dispense_input_branch = BRANCH_NAME\n",
    "dispense_input_contract = DatasetContract(branch=dispense_input_branch, state=dispense_input_state, parent=dispense_input_pharma, child=dispense_input_brand, dataset=dispense_input_name)\n",
    "dispense_run_filter = []\n",
    "dispense_run_filter.append(dict(partition=\"__metadata_run_id\", comparison=\"==\", values=[2]))\n",
    "# IF YOU HAVE PUBLISHED DATA MULTIPLE TIMES, uncomment the above line and change the int to the run_id to fetch.\n",
    "# Otherwise, you will have duplicate values in your fetched dataset!\n",
    "dispense_input_df = dispense_input_contract.fetch(filters=dispense_run_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve current dataset and dispense data from contract\n",
    "from core.dataset_diff import DatasetDiff\n",
    "\n",
    "diff = DatasetDiff(db_transform.id)\n",
    "df = diff.get_diff(transform_name=transform.input_transform, values=[run_id])\n",
    "dispense_input_df = diff.get_diff(transform_name=transform.dispense_input_transform, values=[run_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispense_input_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use the variables above to execute your transformation. the final output needs to be a variable named final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign column headers to variables\n",
    "\n",
    "brand_col = 'brand'\n",
    "patient_id = 'longitudinal_patient_id'\n",
    "pharmacy = 'pharmacy_name'\n",
    "status_date = 'status_date'\n",
    "referral_date = 'referral_date'\n",
    "status =  'status'\n",
    "substatus =  'substatus'\n",
    "hierarchy = 'patient_journey_hierarchy'\n",
    "dispense_status_date = 'ship_date'\n",
    "\n",
    "\n",
    "if DbTransform.pharmaceutical_company.upper() == 'SUN':\n",
    "    unique_id = 'pharmacy_transaction_id'\n",
    "    trans_id = 'pharmacy_transaction_id'\n",
    "\n",
    "else:\n",
    "    trans_id = 'aggregator_transaction_id'\n",
    "    unique_id = 'aggregator_transaction_id'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA CLEANING: ADDRESS THIS SECTION BEFORE PIPELINE INTEGRATION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "datetime = '%Y%m%d'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# CLEAN DATA - This step should not be necessary once transform is integrated into pipeline.\n",
    "#    Extract and map relevant columns\n",
    "#    Convert dates to datetime format\n",
    "#    Extract brand from medication\n",
    "#    Convert substatuses to uppercase\n",
    "#    Populate null referral dates with the min(status_date) for that patient/pharmacy/brand.\n",
    "    \n",
    "def clean_data(input_df, datetime, transform):\n",
    "\n",
    "    clean_df = (\n",
    "        input_df\n",
    "        .loc[:,\n",
    "             [transform.trans_id,\n",
    "              transform.patient_id,\n",
    "              transform.pharmacy,\n",
    "              transform.brand_col,\n",
    "              transform.status_date,\n",
    "              transform.referral_date,\n",
    "              transform.status,\n",
    "              transform.substatus]\n",
    "            ]\n",
    "        .assign(**{\n",
    "            'min_status_date' : lambda x: (\n",
    "                x.groupby([transform.patient_id,transform.pharmacy,transform.brand_col])\n",
    "                [transform.status_date]\n",
    "                .transform(min)\n",
    "            )\n",
    "        })\n",
    "        .assign(**{\n",
    "            transform.status_date : lambda x: (\n",
    "                pd.to_datetime(\n",
    "                    x[transform.status_date].str[:8].astype(str),\n",
    "                    format=datetime,\n",
    "                    errors='coerce'\n",
    "                ))\n",
    "        })\n",
    "        .assign(**{\n",
    "            transform.referral_date : lambda x: (\n",
    "                pd.to_datetime(\n",
    "                    x[transform.referral_date].fillna(x['min_status_date']).str[:8].astype(str),\n",
    "                    format=datetime,\n",
    "                    errors='coerce'\n",
    "                ))\n",
    "                \n",
    "        })\n",
    "        .assign(**{\n",
    "            transform.brand_col : lambda x: (x[transform.brand_col].apply(lambda x: x.split()[0].strip())),\n",
    "            transform.status : lambda x: (x[transform.status].str.upper()),\n",
    "            transform.substatus : lambda x: (x[transform.substatus].str.upper()),\n",
    "            transform.trans_id : lambda x: (x[transform.trans_id].astype(str)),\n",
    "            transform.unique_id : lambda x: (x[transform.unique_id].astype(str)),\n",
    "            transform.patient_id : lambda x: (x[transform.patient_id].astype(int)),\n",
    "            transform.hierarchy : 'Null'\n",
    "        })\n",
    "#        .drop(['min_status_date'],axis=1)\n",
    "#        .drop_duplicates()\n",
    "        .sort_values(\n",
    "            by=[transform.patient_id, transform.pharmacy, transform.brand_col, transform.status_date, transform.status, transform.trans_id],\n",
    "            ascending=[True, True, True, True, False, True])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = clean_data(\n",
    "    df,\n",
    "    datetime,\n",
    "    transform\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN DATA - This step should not be necessary once transform is integrated into pipeline.\n",
    "#    Extract and map relevant columns\n",
    "#    Rename dispense ship date to status_date and convert to datetime format\n",
    "#    Assign brand name, active hierarchy, active status, and active substatus using transform variables\n",
    "#    Filter data to only include patient journeys found in patient status data\n",
    "#    Keep only the first dispense reported for each patient journey\n",
    "    \n",
    "def clean_dispense_data(dispense_input_df, transform):\n",
    "\n",
    "    clean_dispense_df = (\n",
    "        dispense_input_df\n",
    "        .loc[:,\n",
    "             [patient_id,\n",
    "              pharmacy,\n",
    "              dispense_status_date]\n",
    "            ]\n",
    "        .rename(columns = \n",
    "                {dispense_status_date : status_date}\n",
    "        )\n",
    "        .dropna()\n",
    "        .assign(**{\n",
    "            brand_col : transform.brand_name,\n",
    "            hierarchy : transform.active_hierarchy,\n",
    "            status : transform.active_shipment_status,\n",
    "            substatus : transform.active_shipment_substatus,\n",
    "            patient_id : lambda x: (x[patient_id].astype(int)),\n",
    "            pharmacy : lambda x: (\n",
    "                np.where(\n",
    "                    x[pharmacy].isin(eval(transform.pharmacy_name_map).keys()),\n",
    "                    x[pharmacy].replace(eval(transform.pharmacy_name_map), inplace=True),\n",
    "                    x[pharmacy]\n",
    "                )\n",
    "            ),\n",
    "            'pj_concat' : lambda x: (x[patient_id].astype(str) + x[pharmacy].astype(str) + x[brand_col].astype(str))\n",
    "        })\n",
    "        .loc[lambda x: (\n",
    "            x['pj_concat'].isin(\n",
    "                (df[patient_id].astype(str) + df[pharmacy].astype(str) + df[brand_col].astype(str))\n",
    "                .drop_duplicates()\n",
    "                .tolist()\n",
    "            )\n",
    "        )]\n",
    "        .sort_values(\n",
    "            by=[patient_id, pharmacy, brand_col, status_date],\n",
    "            ascending=[True, True, True, True])\n",
    "        .drop_duplicates(subset=[patient_id, pharmacy, brand_col], keep='first')\n",
    "        .drop(['pj_concat'], axis=1)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return clean_dispense_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispense_df = clean_dispense_data(\n",
    "    dispense_input_df,\n",
    "    transform\n",
    ")\n",
    "\n",
    "dispense_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inherit \"missing\" information into dispense data using patient status data\n",
    "#    Inherit from the status immediately prior, if available\n",
    "#    Otherwise, use the status immediately following.\n",
    "\n",
    "inherit_df = (\n",
    "    df\n",
    "    .assign(\n",
    "        pj_step =  lambda x: x.index,\n",
    "        dispense_flag = False\n",
    "    )\n",
    "    .append(\n",
    "        dispense_df.assign(dispense_flag =  True),\n",
    "        sort = False\n",
    "    )\n",
    "    .assign(\n",
    "        pj_id = lambda x: (\n",
    "            x.groupby([patient_id, pharmacy, brand_col]).grouper.group_info[0]\n",
    "        )\n",
    "    )\n",
    "    .sort_values(\n",
    "        by=[patient_id, pharmacy, brand_col, status_date, status, substatus, 'dispense_flag'],\n",
    "        ascending=[True, True, True, True, False, True, True])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "for column in inherit_df.columns.tolist():\n",
    "    inherit_df = (\n",
    "        inherit_df\n",
    "        .assign(**{\n",
    "            column : lambda x: (\n",
    "                x.groupby(['pj_id'], sort=False)[column]\n",
    "                .ffill(limit = 1)\n",
    "                .bfill(limit = 1)\n",
    "            )\n",
    "        })\n",
    "    )\n",
    "    \n",
    "inherit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanse data to prepare for final output\n",
    "#    Append \"ic_{}_as\" to id column for dispense records that were added\n",
    "#    If referral date > status date, override referral date with status date.\n",
    "#    If dispense data is redundant (i.e. active shipment is already reported for that date in the status data), drop that dispense record from the final dataframe.\n",
    "\n",
    "output_df = (\n",
    "    inherit_df\n",
    "    .assign(**{\n",
    "        unique_id : lambda x: (\n",
    "            np.where(\n",
    "                x['dispense_flag'],\n",
    "                'ic_' + x[unique_id].astype(str) + '_as',\n",
    "                x[unique_id]\n",
    "            )\n",
    "        ),\n",
    "        referral_date : lambda x: (\n",
    "            np.where(\n",
    "                x[referral_date] > x[status_date],\n",
    "                x[status_date],\n",
    "                x[referral_date]\n",
    "            )\n",
    "        )\n",
    "    })\n",
    "    .sort_values(\n",
    "        by=['pj_step', 'dispense_flag'],\n",
    "        ascending=[True, True])\n",
    "    .drop_duplicates(\n",
    "        subset = [\n",
    "            patient_id,\n",
    "            pharmacy,\n",
    "            brand_col,\n",
    "            status_date,\n",
    "            status,\n",
    "            substatus,\n",
    "            'pj_step'\n",
    "            \n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe = (\n",
    "    output_df.loc[:,df.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST TRANSFORM OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Check that the output dataframe (minus the inserted dispense records) has the same number of records as the input dataframe.\n",
    "\n",
    "test1 = (len(df) == len(output_df.loc[output_df['dispense_flag']==False]))\n",
    "\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Check that inserted records' IDs all have \"ic\" and \"_as\" appended.\n",
    "\n",
    "test2 = (\n",
    "    (output_df.loc[output_df['dispense_flag']][unique_id].str[:2] == 'ic')\n",
    "    &\n",
    "    (output_df.loc[output_df['dispense_flag']][unique_id].str[-3:] == '_as')\n",
    ").all()\n",
    "\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Check that inserted records are all listed as active shipments with the correct hierarchy.\n",
    "\n",
    "test3 = (\n",
    "    (output_df.loc[output_df['dispense_flag'], [status]] == transform.active_shipment_status).all().all()\n",
    "    &\n",
    "    (output_df.loc[output_df['dispense_flag'], [substatus]] == transform.active_shipment_substatus).all().all()\n",
    "    &\n",
    "    (output_df.loc[output_df['dispense_flag'], [hierarchy]] == transform.active_hierarchy).all().all()\n",
    ")\n",
    "\n",
    "test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST: Did all 3 tests pass?\n",
    "\n",
    "test1 & test2 & test3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## that's it - just provide the final dataframe to the var final_dataframe and we take it from there\n",
    "transform.publish_contract.publish(final_dataframe, run_id, session)\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
