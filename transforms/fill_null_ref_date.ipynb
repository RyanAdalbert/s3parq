<<<<<<< HEAD
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.helpers.session_helper import SessionHelper\n",
    "session = SessionHelper().session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ CONFIGURATION - PLEASE TOUCH **************\n",
    "Pipeline Builder configuration: creates configurations from variables specified here!!\n",
    "This cell will be off in production as configurations will come from the configuration postgres DB.\n",
    "\"\"\"\n",
    "# config vars: this dataset\n",
    "config_pharma = \"sun\" # the pharmaceutical company which owns {brand}\n",
    "config_brand = \"ilumya\" # the brand this pipeline operates on\n",
    "config_state = \"raw\" # the state this transform runs in\n",
    "config_name = \"fill_null_ref_date\" # the name of this transform, which is the name of this notebook without .ipynb\n",
    "\n",
    "# input vars: dataset to fetch. Recall that a contract published to S3 has a key format branch/pharma/brand/state/name\n",
    "input_pharma = \"sun\"\n",
    "input_brand = \"ilumya\"\n",
    "input_state = \"raw\"\n",
    "input_name = \"upstream\"\n",
    "input_branch = None # if None, input_branch is automagically set to your working branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "Populating config mocker based on config parameters...\n",
    "\"\"\"\n",
    "import core.helpers.pipeline_builder as builder\n",
    "\n",
    "ids = builder.build(config_pharma, config_brand, config_state, config_name, session)\n",
    "transform_id = ids[0]\n",
    "run_id = ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "This section imports data from the configuration database\n",
    "and should not need to be altered or otherwise messed with. \n",
    "~~These are not the droids you are looking for~~\n",
    "\"\"\"\n",
    "from core.constants import BRANCH_NAME, ENV_BUCKET\n",
    "from core.helpers.session_helper import SessionHelper\n",
    "from core.models.configuration import Transformation\n",
    "from dataclasses import dataclass\n",
    "from core.dataset_contract import DatasetContract\n",
    "\n",
    "db_transform = session.query(Transformation).filter(Transformation.id == transform_id).one()\n",
    "\n",
    "@dataclass\n",
    "class DbTransform:\n",
    "    id: int = db_transform.id ## the instance id of the transform in the config app\n",
    "    name: str = db_transform.transformation_template.name ## the transform name in the config app\n",
    "    state: str = db_transform.pipeline_state.pipeline_state_type.name ## the pipeline state, one of raw, ingest, master, enhance, enrich, metrics, dimensional\n",
    "    branch:str = BRANCH_NAME ## the git branch for this execution \n",
    "    brand: str = db_transform.pipeline_state.pipeline.brand.name ## the pharma brand name\n",
    "    pharmaceutical_company: str = db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name # the pharma company name\n",
    "    publish_contract: DatasetContract = DatasetContract(branch=BRANCH_NAME,\n",
    "                            state=db_transform.pipeline_state.pipeline_state_type.name,\n",
    "                            parent=db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name,\n",
    "                            child=db_transform.pipeline_state.pipeline.brand.name,\n",
    "                            dataset=db_transform.transformation_template.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE Cartridge Notebook::[transform name here]\n",
    "![CORE Logo](assets/coreLogo.png) \n",
    "\n",
    "---\n",
    "## Keep in Mind\n",
    "Good Transforms Are...\n",
    "- **singular in purpose:** good transforms do one and only one thing, and handle all known cases for that thing. \n",
    "- **repeatable:** transforms should be written in a way that they can be run against the same dataset an infinate number of times and get the same result every time. \n",
    "- **easy to read:** 99 times out of 100, readable, clear code that runs a little slower is more valuable than a mess that runs quickly. \n",
    "- **No 'magic numbers':** if a variable or function is not instantly obvious as to what it is or does, without context, maybe consider renaming it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow - how to use this notebook to make science\n",
    "#### Data Science\n",
    "1. **Document your transform.** Fill out the _description_ cell below describing what it is this transform does; this will appear in the configuration application where Ops will create, configure and update pipelines. \n",
    "1. **Define your config object.** Fill out the _configuration_ cell below the commented-out guide to define the variables you want ops to set in the configuration application (these will populate here for every pipeline). \n",
    "2. **Build your transformation logic.** Use the transformation cell to do that magic that you do. \n",
    "![caution](assets/cautionTape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "********* VARIABLES - PLEASE TOUCH ********* \n",
    "This section defines what you expect to get from the configuration application \n",
    "in a single \"transform\" object. Define the vars you need here, and comment inline to the right of them \n",
    "for all-in-one documentation. \n",
    "Engineering will build a production \"transform\" object for every pipeline that matches what you define here.\n",
    "\n",
    "@@@ FORMAT OF THE DATA CLASS IS: @@@ \n",
    "\n",
    "<variable_name>: <data_type> #<comment explaining what the value is to future us>\n",
    "\n",
    "e.g.\n",
    "\n",
    "class Transform(DbTransform):\n",
    "    some_ratio: float\n",
    "    site_name: str\n",
    "\n",
    "~~These ARE the droids you are looking for~~\n",
    "\"\"\"\n",
    "\n",
    "class Transform(DbTransform):\n",
    "    '''\n",
    "    YOUR properties go here!!\n",
    "    Variable properties should be assigned to the exact name of\n",
    "    the transformation as it appears in the Jupyter notebook filename.\n",
    "    '''\n",
    "    \n",
    "    col_1: str #This variable is for the brand/medication column. Used for identification purposes\n",
    "    col_2: str #This variable is the for the SP-ID column. Used for identification purposes\n",
    "    col_3: str #This variable is for the Long-ID column column. Used for identification purposes and to fill in null values where there is no Long-ID\n",
    "    col_fill: str #This variable is for the Status Date column. This is the column is used to fill null cells in Referral Date when no other Referral Date exists.\n",
    "    col_null: str #This variable is for the Referral Date column. This is the column where null cells are to be filled.\n",
    "    \n",
    "    def fill_null_ref_date(self,df):\n",
    "\n",
    "        # Creates a dictionary of all variables used as identifiers where Referral Date is null\n",
    "        unique_id_dict = (\n",
    "            df[df[self.col_null].isna()]\n",
    "            [[self.col_1,self.col_2,self.col_3]]\n",
    "            .dropna()\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "            .to_dict(orient='index')\n",
    "        )\n",
    "        \n",
    "        for key in unique_id_dict.keys():\n",
    "            \n",
    "            patient_journey_mask = (\n",
    "                (df[self.col_1] == unique_id_dict[key][self.col_1])\n",
    "                & (df[self.col_2] == unique_id_dict[key][self.col_2])\n",
    "                & (df[self.col_3] == unique_id_dict[key][self.col_3])\n",
    "            )\n",
    "            \n",
    "            patient_journey_df = df[patient_journey_mask]\n",
    "            \n",
    "            if not pd.isnull(min(patient_journey_df[self.col_null].unique())):\n",
    "                df.loc[(patient_journey_mask & df[self.col_null].isna()),self.col_null] = min(patient_journey_df[self.col_null].unique())\n",
    "                \n",
    "            elif pd.isnull(min(patient_journey_df[self.col_null].unique())):\n",
    "                df.loc[(patient_journey_mask & df[self.col_null].isna()),self.col_null] = min(patient_journey_df[self.col_fill].unique())\n",
    "                \n",
    "        return df\n",
    "\n",
    "transform = Transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please place your value assignments for development here!!\n",
    "## This cell will be turned off in production and Engineering will set to pull from the configuration application instead\n",
    "## For the last example, this could look like...\n",
    "## transform.some_ratio = 0.6\n",
    "## transform.site_name = \"WALGREENS\"\n",
    "\n",
    "transform.col_1 = 'medication'\n",
    "transform.col_2 = 'pharmacy_id'\n",
    "transform.col_3 = 'msa_patient_id'\n",
    "transform.col_fill = 'status_date'\n",
    "transform.col_null = 'ref_date'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "What does this transformation do? be specific.\n",
    "\n",
    "![what does your transform do](assets/what.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replaces Null values in Referral Date with either the minimum Referral Date for that Patient Journey or with the minimum Status Date for that Patient Journey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ FETCH DATA - TOUCH, BUT CAREFULLY **************\n",
    "This cell will be turned off in production, as the input_contract will be handled by the pipeline.\n",
    "\"\"\"\n",
    "\n",
    "if not input_branch:\n",
    "    input_branch = BRANCH_NAME\n",
    "input_contract = DatasetContract(branch=input_branch, state=input_state, parent=input_pharma, child=input_brand, dataset=input_name)\n",
    "run_filter = []\n",
    "# run_filter.append(dict(partition=\"run_id\", comparison=\"==\", values=[1]))\n",
    "# IF YOU HAVE PUBLISHED DATA MULTIPLE TIMES, uncomment the above line and change the int to the run_id to fetch.\n",
    "# Otherwise, you will have duplicate values in your fetched dataset!\n",
    "final_dataframe = input_contract.fetch(filters=run_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from s3parq import fetch\n",
    "from random import getrandbits\n",
    "\n",
    "run_id = 5\n",
    "\n",
    "ingest_contract = DatasetContract(branch='sun-extract-prod-vars',\n",
    "                                  parent='sun',\n",
    "                                  child='ilumya',\n",
    "                                  state='ingest',\n",
    "                                  dataset='symphony_health_association_ingest_column_mapping')\n",
    "\n",
    "run_filter = [{'partition':'__metadata_run_id', 'comparison':'==', 'values':[run_id]}]\n",
    "\n",
    "df = fetch(bucket=ingest_contract.bucket, key=ingest_contract.key, filters=run_filter)\n",
    "\n",
    "datetime = '%Y%m%d'\n",
    "df.status_date = df.status_date.str[:8].astype(str)\n",
    "df.ref_date = df.ref_date.str[:8].astype(str)\n",
    "\n",
    "df.status_date = pd.to_datetime(df.status_date, format=datetime, errors='coerce')\n",
    "df.ref_date = pd.to_datetime(df.ref_date, format=datetime, errors='coerce')\n",
    "\n",
    "def create_fake_data(df=df):\n",
    "    fake_id = getrandbits(19)\n",
    "    filter_col = [col for col in df.columns.values if not col.startswith('__metadata')]\n",
    "    null_df = df[df.ref_date.isna()]\n",
    "    null_df = null_df.dropna(subset=filter_col,how='all')\n",
    "    null_index = list(null_df.index[null_df.msa_patient_id.isna()])\n",
    "    null_df.loc[null_df.index.isin(null_index),'msa_patient_id'] = fake_id\n",
    "    df.loc[df.index.isin(null_index),'msa_patient_id'] = fake_id\n",
    "    return df\n",
    "\n",
    "df = create_fake_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "What does this transformation do? be specific.\n",
    "\n",
    "![what does your transform do](assets/what.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(clear out and replace with your description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please place your value assignments for development here!!\n",
    "## This cell will be turned off in production and Engineering will set to pull form the configuration application instead\n",
<<<<<<< HEAD
    "def populate_ref_date(df=df):\n",
||||||| merged common ancestors
<<<<<<< HEAD
    "def fill_null_ref_date(df=df):\n",
=======
    "def populate_ref_date(df=df):\n",
>>>>>>> 140455bc61a7858faddd838e7c09b0981933e405
=======
    "def fill_null_ref_date(df=df):\n",
>>>>>>> temp commit
    "    \"\"\"\n",
    "    Args:\n",
    "        df: pandas.DataFrame\n",
    "    Returns:\n",
    "        df: pandas.DataFrame - Filled in Null values found in ref_date column.\n",
    "    \"\"\"\n",
    "    def_df = df.copy()\n",
    "    \n",
    "    filter_col = [col for col in def_df.columns.values if not col.startswith('__metadata')]\n",
    "    \n",
    "    null_df = def_df[def_df.ref_date.isna()]\n",
    "    null_df = null_df.dropna(subset=filter_col,how='all')\n",
    "    \n",
    "    unique_id_dict = null_df[['pharmacy_id','medication','msa_patient_id']].drop_duplicates().to_dict(orient='index')\n",
    "    \n",
    "    for key in unique_id_dict.keys():\n",
    "        patient_journey_mask = (\n",
    "            (def_df.pharmacy_id == unique_id_dict[key]['pharmacy_id']) & \n",
    "            (def_df.medication == unique_id_dict[key]['medication']) & \n",
    "            (def_df.msa_patient_id == unique_id_dict[key]['msa_patient_id'])\n",
    "        ) \n",
    "        patient_journey_df = def_df[patient_journey_mask]\n",
    "        \n",
    "        if not pd.isnull(min(patient_journey_df.ref_date.unique())):\n",
    "            def_df.loc[(patient_journey_mask & df.ref_date.isna()),'ref_date'] = min(patient_journey_df.ref_date.unique())\n",
    "        \n",
    "        elif pd.isnull(min(patient_journey_df.ref_date.unique())):\n",
    "            def_df.loc[(patient_journey_mask & df.ref_date.isna()),'ref_date'] = min(patient_journey_df.status_date.unique())\n",
    "    \n",
    "    return def_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use the variables above to execute your transformation. the final output needs to be a variable named final_dataframe\n",
<<<<<<< HEAD
    "final_dataframe = populate_ref_date()"
||||||| merged common ancestors
<<<<<<< HEAD
    "final_dataframe = fill_null_ref_date()"
=======
    "final_dataframe = populate_ref_date()"
>>>>>>> 140455bc61a7858faddd838e7c09b0981933e405
=======
    "final_dataframe = fill_null_ref_date()"
>>>>>>> temp commit
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
   "source": [
    "def shape_status(final_dataframe,orig_df):\n",
    "    \"\"\"Make sure df shape doesn't change\n",
    "    This is a test:\n",
    "    >>> shape_status(final_dataframe,orig_df)\n",
    "    True\"\"\"\n",
    "    return final_dataframe.shape == orig_df.shape\n",
    "\n",
    "def nulls_removed(final_dataframe):\n",
    "    \"\"\"Make sure df does not contain Null Values\n",
    "    This is a test:\n",
    "    >>> shape_status(final_dataframe,orig_df)\n",
    "    True\"\"\"\n",
    "    final_dataframe[final_dataframe.ref_date.isna()].shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the below tests test_nulls_removed currently failing due to 5 rows of containing Null in all columns except \\_metadata column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_nulls_removed (__main__.TestNotebook) ... FAIL\n",
      "test_shape_status (__main__.TestNotebook) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_nulls_removed (__main__.TestNotebook)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-cea4f4733c0b>\", line 25, in test_nulls_removed\n",
      "    self.assertEqual(nulls_removed(final_dataframe),True)\n",
      "AssertionError: False != True\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.011s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f4b7b882f28>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
||||||| merged common ancestors
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_nulls_removed (__main__.TestNotebook) ... FAIL\n",
      "test_shape_status (__main__.TestNotebook) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_nulls_removed (__main__.TestNotebook)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-cea4f4733c0b>\", line 25, in test_nulls_removed\n",
      "    self.assertEqual(nulls_removed(final_dataframe),True)\n",
      "AssertionError: False != True\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.011s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f4b7b882f28>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "outputs": [],
>>>>>>> cleared outputs
   "source": [
    "import unittest\n",
    "\n",
    "def shape_status(final_dataframe,df):\n",
    "    \"\"\"\n",
    "    Make sure df shape doesn't change,\n",
    "    This is a test:\n",
    "    >>> shape_status(final_dataframe,df)\n",
    "    True\n",
    "    \"\"\"\n",
    "    return final_dataframe.shape == df.shape\n",
    "    \n",
    "def nulls_removed(final_dataframe):\n",
    "    \"\"\"\n",
    "    Make sure df does not contain Null Values\n",
    "    This is a test:\n",
    "    >>> shape_status(final_dataframe,df)\n",
    "    True\n",
    "    \"\"\"\n",
    "    return final_dataframe[final_dataframe.ref_date.isna()].shape[0] == 0\n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "\n",
    "    def test_shape_status(self):\n",
    "        self.assertEqual(shape_status(final_dataframe,df),True)\n",
    "    \n",
    "    def test_nulls_removed(self):\n",
    "        self.assertEqual(nulls_removed(final_dataframe),True)\n",
    "    \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## that's it - just provide the final dataframe to the var final_dataframe and we take it from there\n",
    "transform.publish_contract.publish(final_dataframe, run_id, session)\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
<<<<<<< HEAD
 "nbformat_minor": 2
||||||| merged common ancestors
<<<<<<< HEAD
 "nbformat_minor": 4
=======
 "nbformat_minor": 2
>>>>>>> 140455bc61a7858faddd838e7c09b0981933e405
=======
 "nbformat_minor": 4
>>>>>>> temp commit
}
||||||| merged common ancestors
=======
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-15 15:00:29,894 - core.helpers.session_helper.SessionHelper - INFO - Creating session for dev environment...\n",
      "2019-07-15 15:00:29,915 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating administrator mocks.\n",
      "2019-07-15 15:00:29,950 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating administrator mocks.\n",
      "2019-07-15 15:00:29,951 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pharmaceutical company mocks.\n",
      "2019-07-15 15:00:29,955 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pharmaceutical company mocks.\n",
      "2019-07-15 15:00:29,956 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating brand mocks.\n",
      "2019-07-15 15:00:29,962 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating brand mocks.\n",
      "2019-07-15 15:00:29,963 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating segment mocks.\n",
      "2019-07-15 15:00:29,966 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating segment mocks.\n",
      "2019-07-15 15:00:29,967 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline type mocks.\n",
      "2019-07-15 15:00:29,974 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline type mocks.\n",
      "2019-07-15 15:00:29,975 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline mocks.\n",
      "2019-07-15 15:00:29,979 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline mocks.\n",
      "2019-07-15 15:00:29,981 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline state type mocks.\n",
      "2019-07-15 15:00:29,987 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline state type mocks.\n",
      "2019-07-15 15:00:29,988 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline state mocks.\n",
      "2019-07-15 15:00:29,993 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline state mocks.\n",
      "2019-07-15 15:00:29,995 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating tag mocks.\n",
      "2019-07-15 15:00:29,998 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating tag mocks.\n",
      "2019-07-15 15:00:29,998 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation_template mocks.\n",
      "2019-07-15 15:00:30,002 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_template mocks.\n",
      "2019-07-15 15:00:30,003 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating bridge table mocks for transformation_templates <=> tags.\n",
      "2019-07-15 15:00:30,007 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_templates_tags mocks.\n",
      "2019-07-15 15:00:30,008 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation mocks.\n",
      "2019-07-15 15:00:30,012 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation mocks.\n",
      "2019-07-15 15:00:30,013 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation_variables mocks\n",
      "2019-07-15 15:00:30,020 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_variables mocks.\n",
      "2019-07-15 15:00:30,021 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating run_events mocks.\n",
      "2019-07-15 15:00:30,027 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating run_events mocks.\n",
      "2019-07-15 15:00:30,028 - core.helpers.session_helper.SessionHelper - INFO - Done. Created dev session with mock data.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "This section imports data from the configuration database\n",
    "and should not need to be altered, molested or otherwise messed with. \n",
    "~~These are not the droids you are looking for~~\n",
    "\"\"\"\n",
    "from core.constants import BRANCH_NAME, ENV_BUCKET\n",
    "from core.helpers.session_helper import SessionHelper\n",
    "from core.models.configuration import Transformation\n",
    "from dataclasses import dataclass\n",
    "from core.dataset_contract import DatasetContract\n",
    "\n",
    "db_transform = SessionHelper().session.query(Transformation).filter(Transformation.id == transform_id).one()\n",
    "\n",
    "@dataclass\n",
    "class DbTransform:\n",
    "    id: int = db_transform.id ## the instance id of the transform in the config app\n",
    "    name: str = db_transform.transformation_template.name ## the transform name in the config app\n",
    "    state: str = db_transform.pipeline_state.pipeline_state_type.name ## the pipeline state, one of raw, ingest, master, enhance, enrich, metrics, dimensional\n",
    "    branch:str = BRANCH_NAME ## the git branch for this execution \n",
    "    brand: str = db_transform.pipeline_state.pipeline.brand.name ## the pharma brand name\n",
    "    pharmaceutical_company: str = db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name # the pharma company name\n",
    "    publish_contract: DatasetContract = DatasetContract(branch=BRANCH_NAME,\n",
    "                            state=db_transform.pipeline_state.pipeline_state_type.name,\n",
    "                            parent=db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name,\n",
    "                            child=db_transform.pipeline_state.pipeline.brand.name,\n",
    "                            dataset=db_transform.transformation_template.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE Cartridge Notebook::[transform name here]\n",
    "![CORE Logo](assets/coreLogo.png) \n",
    "\n",
    "---\n",
    "## Keep in Mind\n",
    "Good Transforms Are...\n",
    "- **singular in purpose:** good transforms do one and only one thing, and handle all known cases for that thing. \n",
    "- **repeatable:** transforms should be written in a way that they can be run against the same dataset an infinate number of times and get the same result every time. \n",
    "- **easy to read:** 99 times out of 100, readable, clear code that runs a little slower is more valuable than a mess that runs quickly. \n",
    "- **No 'magic numbers':** if a variable or function is not instantly obvious as to what it is or does, without context, maybe consider renaming it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow - how to use this notebook to make science\n",
    "#### Data Science\n",
    "1. **Document your transform.** Fill out the _description_ cell below describing what it is this transform does; this will appear in the configuration application where Ops will create, configure and update pipelines. \n",
    "1. **Define your config object.** Fill out the _configuration_ cell below the commented-out guide to define the variables you want ops to set in the configuration application (these will populate here for every pipeline). \n",
    "2. **Build your transformation logic.** Use the transformation cell to do that magic that you do. \n",
    "![caution](assets/cautionTape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "********* CONFIGURATION - PLEASE TOUCH ********* \n",
    "This section defines what you expect to get from the configuration application \n",
    "in a single \"transform\" object. Define the vars you need here, and comment inline to the right of them \n",
    "for all-in-one documentation. \n",
    "Engineering will build a production \"transform\" object for every pipeline that matches what you define here.\n",
    "\n",
    "@@@ FORMAT OF THE DATA CLASS IS: @@@ \n",
    "\n",
    "<value_name>: <data_type> #<comment explaining what the value is to future us>\n",
    "\n",
    "~~These ARE the droids you are looking for~~\n",
    "\"\"\"\n",
    "\n",
    "class Transform(DbTransform):\n",
    "        ## YOUR properties go here!!\n",
    "        remote_path: str = db_transform.variables.filesystem_path # The path to follow on the remote server\n",
    "        prefix: str = db_transform.variables.prefix # The prefix of files to get on the remote server\n",
    "        secret_name: str = db_transform.variables.secret_name # The name of the secret in Secret Manager for the remote server\n",
    "        secret_type_of: str = db_transform.variables.secret_type_of # The type of the secret in Secret Manager for the remote server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please place your value assignments for development here!!\n",
    "## This cell will be turned off in production and Engineering will set to pull form the configuration application instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest\n",
    "import unittest\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from s3parq import fetch\n",
    "from s3fs import S3FileSystem\n",
    "from core.logging import get_logger\n",
    "from encodings.aliases import aliases\n",
    "\n",
    "transform = Transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 5\n",
    "ingest_contract = DatasetContract(branch='sun-extract-prod-vars',\n",
    "                                  parent='sun',\n",
    "                                  child='ilumya',\n",
    "                                  state='ingest',\n",
    "                                  dataset='symphony_health_association_ingest_column_mapping')\n",
    "\n",
    "run_filter = [{'partition':'__metadata_run_id', 'comparison':'==', 'values':[run_id]}]\n",
    "\n",
    "df = fetch(bucket=ingest_contract.bucket, key=ingest_contract.key, filters=run_filter)\n",
    "\n",
    "orig_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of after testing\n",
    "\n",
    "datetime = '%Y%m%d'\n",
    "\n",
    "df.status_date = df.status_date.str[:8].astype(str)\n",
    "df.ref_date = df.ref_date.str[:8].astype(str)\n",
    "\n",
    "df.status_date = pd.to_datetime(df.status_date, format=datetime, errors='coerce')\n",
    "df.ref_date = pd.to_datetime(df.ref_date, format=datetime, errors='coerce')\n",
    "\n",
    "filter_col = [col for col in df.columns.values if not col.startswith('__metadata')]\n",
    "null_df = df[df.ref_date.isna()]\n",
    "null_df = null_df.dropna(subset=filter_col,how='all')\n",
    "unique_id_dict = null_df[['pharmacy_id','medication','msa_patient_id']].drop_duplicates().to_dict(orient='index')\n",
    "\n",
    "from random import getrandbits\n",
    "\n",
    "def create_fake_data(df=df):\n",
    "    fake_id = getrandbits(19)\n",
    "    filter_col = [col for col in df.columns.values if not col.startswith('__metadata')]\n",
    "    null_df = df[df.ref_date.isna()]\n",
    "    null_df = null_df.dropna(subset=filter_col,how='all')\n",
    "    null_index = list(null_df.index[null_df.msa_patient_id.isna()])\n",
    "    null_df.loc[null_df.index.isin(null_index),'msa_patient_id'] = fake_id\n",
    "    df.loc[df.index.isin(null_index),'msa_patient_id'] = fake_id\n",
    "    return df\n",
    "\n",
    "df = create_fake_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use the variables above to execute your transformation. the final output needs to be a variable named final_dataframe\n",
    "final_dataframe = transform.fill_null_ref_date(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_nulls_removed (__main__.TestNotebook) ... FAIL\n",
      "test_shape_status (__main__.TestNotebook) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_nulls_removed (__main__.TestNotebook)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-fdbafb5f8376>\", line 7, in test_nulls_removed\n",
      "    self.assertEqual(nulls_removed(final_dataframe),True)\n",
      "AssertionError: None != True\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.011s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f83d00fc8d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestNotebook(unittest.TestCase):\n",
    "    \n",
    "    def test_shape_status(self):\n",
    "        self.assertEqual(shape_status(final_dataframe,orig_df),True)\n",
    "        \n",
    "    def test_nulls_removed(self):\n",
    "        self.assertEqual(nulls_removed(final_dataframe),True)\n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## that's it - just provide the final dataframe to the var final_dataframe and we take it from there\n",
    "transform.publish_contract.publish(final_dataframe, run_id)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
>>>>>>> updated from upstream
