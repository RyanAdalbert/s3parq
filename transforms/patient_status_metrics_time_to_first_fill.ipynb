{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE Cartridge Notebook::Time to First Fill Metric\n",
    "![CORE Logo](assets/coreLogo.png) \n",
    "\n",
    "---\n",
    "## Keep in Mind\n",
    "Good Transforms Are...\n",
    "- **singular in purpose:** good transforms do one and only one thing, and handle all known cases for that thing. \n",
    "- **repeatable:** transforms should be written in a way that they can be run against the same dataset an infinate number of times and get the same result every time. \n",
    "- **easy to read:** 99 times out of 100, readable, clear code that runs a little slower is more valuable than a mess that runs quickly. \n",
    "- **No 'magic numbers':** if a variable or function is not instantly obvious as to what it is or does, without context, maybe consider renaming it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow - how to use this notebook to make science\n",
    "#### Data Science\n",
    "1. **Document your transform.** Fill out the _description_ cell below describing what it is this transform does; this will appear in the configuration application where Ops will create, configure and update pipelines. \n",
    "1. **Define your config object.** Fill out the _configuration_ cell below the commented-out guide to define the variables you want ops to set in the configuration application (these will populate here for every pipeline). \n",
    "2. **Build your transformation logic.** Use the transformation cell to do that magic that you do. \n",
    "![caution](assets/cautionTape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "What does this transformation do? be specific.\n",
    "\n",
    "![what does your transform do](assets/what.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TTFF Metric prepares data to be utilized by Spotfire.  The actual TTFF metric is not calculated, but its components are calculated.  Spotfire handles the actual calculation of TTFF, using the output of this transform.\n",
    "\n",
    "The transform takes all patient journey data up-to and including the first active shipment.  If no active shipment is reported, that entire journey is dropped.\n",
    "It then calculates the time spent in each of those statuses as (next status_date) - (status_date).  The sum across all statuses for a patient journey would then give the the total TTFF (this part is done in Spotfire)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.helpers.session_helper import SessionHelper\n",
    "session = SessionHelper().session"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "************ CONFIGURATION - PLEASE TOUCH **************\n",
    "Pipeline Builder configuration: creates configurations from variables specified here!!\n",
    "This cell will be off in production as configurations will come from the configuration postgres DB.\n",
    "\"\"\"\n",
    "# config vars: this dataset\n",
    "config_pharma = \"sun\" # the pharmaceutical company which owns {brand}\n",
    "config_brand = \"ilumya\" # the brand this pipeline operates on\n",
    "config_state = \"enrich\" # the state this transform runs in\n",
    "config_name = \"enrich_cancel_before_active\" # the name of this transform, which is the name of this notebook without .ipynb\n",
    "\n",
    "# input vars: dataset to fetch. Recall that a contract published to S3 has a key format branch/pharma/brand/state/name\n",
    "# Note: this key is case sensitive!!\n",
    "input_pharma = \"sun\"\n",
    "input_brand = \"ilumya\"\n",
    "input_state = \"ingest\"\n",
    "input_name = \"symphony_health_association_ingest_column_mapping\"\n",
    "input_branch = \"sun-extract-validation\" # if None, input_branch is automagically set to your working branch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "Populating config mocker based on config parameters...\n",
    "\"\"\"\n",
    "import core.helpers.pipeline_builder as builder\n",
    "\n",
    "ids = builder.build(config_pharma, config_brand, config_state, config_name, session)\n",
    "transform_id = ids[0]\n",
    "run_id = ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "This section imports data from the configuration database\n",
    "and should not need to be altered or otherwise messed with. \n",
    "~~These are not the droids you are looking for~~\n",
    "\"\"\"\n",
    "from core.constants import BRANCH_NAME, ENV_BUCKET\n",
    "from core.helpers.session_helper import SessionHelper\n",
    "from core.models.configuration import Transformation\n",
    "from dataclasses import dataclass\n",
    "from core.dataset_contract import DatasetContract\n",
    "\n",
    "db_transform = session.query(Transformation).filter(Transformation.id == transform_id).one()\n",
    "\n",
    "@dataclass\n",
    "class DbTransform:\n",
    "    id: int = db_transform.id ## the instance id of the transform in the config app\n",
    "    name: str = db_transform.transformation_template.name ## the transform name in the config app\n",
    "    state: str = db_transform.pipeline_state.pipeline_state_type.name ## the pipeline state, one of raw, ingest, master, enhance, enrich, metrics, dimensional\n",
    "    branch:str = BRANCH_NAME ## the git branch for this execution \n",
    "    brand: str = db_transform.pipeline_state.pipeline.brand.name ## the pharma brand name\n",
    "    pharmaceutical_company: str = db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name # the pharma company name\n",
    "    publish_contract: DatasetContract = DatasetContract(branch=BRANCH_NAME,\n",
    "                            state=db_transform.pipeline_state.pipeline_state_type.name,\n",
    "                            parent=db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name,\n",
    "                            child=db_transform.pipeline_state.pipeline.brand.name,\n",
    "                            dataset=db_transform.transformation_template.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "********* VARIABLES - PLEASE TOUCH ********* \n",
    "This section defines what you expect to get from the configuration application \n",
    "in a single \"transform\" object. Define the vars you need here, and comment inline to the right of them \n",
    "for all-in-one documentation. \n",
    "Engineering will build a production \"transform\" object for every pipeline that matches what you define here.\n",
    "\n",
    "@@@ FORMAT OF THE DATA CLASS IS: @@@ \n",
    "\n",
    "<variable_name>: <data_type> #<comment explaining what the value is to future us>\n",
    "\n",
    "e.g.\n",
    "\n",
    "class Transform(DbTransform):\n",
    "    some_ratio: float\n",
    "    site_name: str\n",
    "\n",
    "~~These ARE the droids you are looking for~~\n",
    "\"\"\"\n",
    "\n",
    "class Transform(DbTransform):\n",
    "    '''\n",
    "    YOUR properties go here!!\n",
    "    Variable properties should be assigned to the exact name of\n",
    "    the transformation as it appears in the Jupyter notebook filename.\n",
    "    '''\n",
    "    input_transform: str = db_transform.variables.input_transform # The name of the dataset to pull from\n",
    "    input_hierarchy: str = db_transform.variables.hierarchy # Column header for Patient Journey Hierarchy from the input dataset\n",
    "    active_status_code: str = db_transform.variables.active_status_code # Active Shipment Status code, e.g. 'ACTIVE' (customer-specific)\n",
    "    active_substatus_code: str = db_transform.variables.active_substatus_code # Active Shipment Substatus code, e.g. 'SHIPMENT' (customer-specific)\n",
    "    fulfillment_hierarchy: str = db_transform.variables.fulfillment_hierarchy # Hierarchy used for statuses after the first fill, e.g. 'ACTIVE - SHIPMENT' (customer-specific)\n",
    "    discontinued_hierarchy: str = db_transform.variables.discontinued_hierarchy # Comma separated list (stored as string) of any hierarchies that we know should be excluded from TTFF, e.g. 'PATIENT'\n",
    "    discontinued_hierarchy = discontinued_hierarchy.split(',') # We reassign the string variable to be a list of strings by comma split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform.remove_from_ttff = \"REMOVE FROM TTFF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hardcoded variable values based on Ingest schema\n",
    "\n",
    "input_brand_col = 'brand'\n",
    "input_medication = 'medication'\n",
    "input_patient_id = 'longitudinal_patient_id'\n",
    "input_pharmacy = 'pharmacy_name'\n",
    "input_status_date = 'status_date'\n",
    "input_referral_date = 'referral_date'\n",
    "input_status =  'status'\n",
    "input_substatus =  'substatus'\n",
    "input_hierarchy = transform.input_hierarchy\n",
    "input_cust_status = 'customer_status'\n",
    "input_cust_status_desc = 'customer_status_description'\n",
    "input_payer = 'primary_payer'\n",
    "input_payer_type = 'primary_payer_type'\n",
    "input_hcp_first_name = 'hcp_first_name'\n",
    "input_hcp_last_name = 'hcp_last_name'\n",
    "input_hcp_npi = 'hcp_npi'\n",
    "input_hcp_state = 'hcp_state'\n",
    "input_hcp_zip = 'hcp_zip'\n",
    "input_dx_1 = 'dx_1'\n",
    "input_dx_2 = 'dx_2'\n",
    "input_referral_source = 'referral_source'\n",
    "input_remove_from_ttff = 'REMOVE FROM TTFF'\n",
    "input_cs_outlet_id = 'cs_outlet_id'\n",
    "input_cot = 'cot'\n",
    "\n",
    "if DbTransform.pharmaceutical_company.upper() == 'SUN':\n",
    "    input_trans_id = 'pharmacy_transaction_id'\n",
    "\n",
    "else:\n",
    "    input_trans_id = 'aggregator_transaction_id'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Please place your value assignments for development here!!\n",
    "## This cell will be turned off in production and Engineering will set to pull from the configuration application instead\n",
    "## For the last example, this could look like...\n",
    "## transform.some_ratio = 0.6\n",
    "## transform.site_name = \"WALGREENS\"\n",
    "\n",
    "transform.hierarchy = 'patient_journey_hierarchy'\n",
    "transform.active_status_code = 'ACTIVE'\n",
    "transform.active_substatus_code = 'SHIPMENT'\n",
    "transform.fulfillment_hierarchy = 'FULFILLMENT'\n",
    "transform.discontinued_hierarchy = 'PATIENT'\n",
    "transform.remove_from_ttff = 'REMOVE FROM TTFF'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "datetime = '%Y%m%d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "************ FETCH DATA - TOUCH, BUT CAREFULLY **************\n",
    "This cell will be turned off in production, as the input_contract will be handled by the pipeline.\n",
    "\"\"\"\n",
    "\n",
    "if not input_branch:\n",
    "    input_branch = BRANCH_NAME\n",
    "input_contract = DatasetContract(branch=input_branch, state=input_state, parent=input_pharma, child=input_brand, dataset=input_name)\n",
    "run_filter = []\n",
    "run_filter.append(dict(partition=\"__metadata_run_id\", comparison=\"==\", values=[4]))\n",
    "# IF YOU HAVE PUBLISHED DATA MULTIPLE TIMES, uncomment the above line and change the int to the run_id to fetch.\n",
    "# Otherwise, you will have duplicate values in your fetched dataset!\n",
    "df = input_contract.fetch(filters=run_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve current dataset from contract\n",
    "from core.dataset_diff import DatasetDiff\n",
    "\n",
    "diff = DatasetDiff(db_transform.id)\n",
    "df = diff.get_diff(transform_name=transform.input_transform, values=[run_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use the variables above to execute your transformation. the final output needs to be a variable named final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA CLEANING: ADDRESS THIS SECTION BEFORE PIPELINE INTEGRATION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# CLEAN DATA - This step should not be necessary once transform is integrated into pipeline.\n",
    "#    Extract and map relevant columns\n",
    "#    Replace null medications with \"NONE\" string\n",
    "#    Extract brand from medication\n",
    "#    Convert status/substatus to uppercase\n",
    "#    Convert dates to datetime format\n",
    "#    Populate null referral dates with the min(status_date) for that patient/pharmacy/brand.\n",
    "#    Drop null and duplicate records\n",
    "#    Sort data\n",
    "\n",
    "def clean_data(cust_input_df, datetime, transform):\n",
    "\n",
    "    clean_df = (\n",
    "        cust_input_df\n",
    "        .loc[:,\n",
    "             [transform.trans_id,\n",
    "              transform.patient_id,\n",
    "              transform.pharmacy,\n",
    "              transform.medication,\n",
    "              transform.payer,\n",
    "              transform.payer_type,\n",
    "              transform.hcp_first_name,\n",
    "              transform.hcp_last_name,\n",
    "              transform.hcp_npi,\n",
    "              transform.hcp_state,\n",
    "              transform.hcp_zip,\n",
    "              transform.dx_1,\n",
    "              transform.dx_2,\n",
    "              transform.referral_source,\n",
    "              transform.status_date,\n",
    "              transform.referral_date,\n",
    "              transform.status,\n",
    "              transform.substatus]\n",
    "            ]\n",
    "        .fillna(value={transform.medication:'NONE'})\n",
    "        .assign(**{\n",
    "            transform.brand_col : lambda x: (x[transform.medication].apply(lambda x: x.split()[0].strip())),\n",
    "            transform.status : lambda x: (x[transform.status].str.upper()),\n",
    "            transform.substatus : lambda x: (x[transform.substatus].str.upper()),\n",
    "            transform.cust_status : lambda x: (x[transform.status]),\n",
    "            transform.cust_status_desc: lambda x: (x[transform.substatus])\n",
    "        })\n",
    "        .assign(**{\n",
    "            transform.status_date : lambda x: (\n",
    "                pd.to_datetime(\n",
    "                    x[transform.status_date].str[:8].astype(str),\n",
    "                    format=datetime,\n",
    "                    errors='coerce'\n",
    "                )\n",
    "            )\n",
    "        })\n",
    "        .assign(**{\n",
    "            'min_status_date' : lambda x: (\n",
    "                x.groupby([transform.patient_id,transform.pharmacy,transform.brand_col])\n",
    "                [transform.status_date]\n",
    "                .transform(min)\n",
    "            )\n",
    "        })\n",
    "        .assign(**{\n",
    "            transform.referral_date : lambda x: (\n",
    "                pd.to_datetime(\n",
    "                    x[transform.referral_date].str[:8].astype(str),\n",
    "                    format=datetime,\n",
    "                    errors='coerce'\n",
    "                )\n",
    "                .fillna(x['min_status_date'])\n",
    "               )\n",
    "        })\n",
    "#        .dropna()\n",
    "        .drop(['min_status_date'],axis=1)\n",
    "        .drop_duplicates()\n",
    "        .sort_values(\n",
    "            by=[transform.patient_id, transform.pharmacy, transform.brand_col, transform.status_date, transform.status, transform.trans_id],\n",
    "            ascending=[True, True, True, True, False, True])\n",
    "        .reset_index(drop=True)\n",
    "        .assign(**{transform.hierarchy : lambda x: (np.random.choice(['Dummy', 'Fulfillment', 'Remove from TTFF'], size=len(x)))})\n",
    "        .assign(**{\n",
    "            transform.hierarchy : lambda x: (\n",
    "                np.where(\n",
    "                    x[transform.substatus] == transform.active_substatus_code,\n",
    "                    transform.fulfillment_hierarchy,\n",
    "                    x[transform.hierarchy]\n",
    "                )\n",
    "            ),\n",
    "            transform.cs_outlet_id : np.NaN,\n",
    "            transform.cot : None\n",
    "        })\n",
    "    )\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = clean_data(\n",
    "    df,\n",
    "    datetime,\n",
    "    transform\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPLY TRANSFORM LOGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert headers to standardized names for schema\n",
    "\n",
    "trans_id = 'transaction_id'\n",
    "patient_id = 'longitudinal_patient_id'\n",
    "payer = 'primary_payer'\n",
    "payer_type = 'primary_payer_type'\n",
    "hcp_first_name = 'hcp_first_name'\n",
    "hcp_last_name = 'hcp_last_name'\n",
    "hcp_npi = 'hcp_npi'\n",
    "hcp_state = 'hcp_state'\n",
    "hcp_zip = 'hcp_zip'\n",
    "medication = 'medication'\n",
    "brand = 'brand'\n",
    "pharmacy = 'pharmacy_name'\n",
    "cs_outlet_id = 'cs_outlet_id'\n",
    "cot = 'cot'\n",
    "status = 'status'\n",
    "substatus = 'substatus'\n",
    "cust_status = 'customer_status'\n",
    "cust_status_desc = 'customer_status_description'\n",
    "status_date = 'status_date'\n",
    "next_status_date = 'next_status_date'\n",
    "dx_1 = 'dx_1'\n",
    "dx_2 = 'dx_2'\n",
    "referral_source = 'referral_source'\n",
    "referral_date = 'referral_date'\n",
    "min_active_date = 'ship_date'\n",
    "hierarchy = 'accrual_bucket'\n",
    "time_in_status = 'tt_fill_days'\n",
    "\n",
    "#status_date_long = 'startDateLong'\n",
    "#next_status_date_long = 'statusDateLong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_columns = [\n",
    "    trans_id,\n",
    "    patient_id,\n",
    "    payer,\n",
    "    payer_type,\n",
    "    hcp_first_name,\n",
    "    hcp_last_name,\n",
    "    hcp_npi,\n",
    "    hcp_state,\n",
    "    hcp_zip,\n",
    "    medication,\n",
    "    brand,\n",
    "    pharmacy,\n",
    "    cs_outlet_id,\n",
    "    cot,\n",
    "    status,\n",
    "    substatus,\n",
    "    cust_status,\n",
    "    cust_status_desc,\n",
    "    status_date,\n",
    "    next_status_date,\n",
    "    dx_1,\n",
    "    dx_2,\n",
    "    referral_source,\n",
    "    referral_date,\n",
    "    min_active_date,\n",
    "    hierarchy,\n",
    "    time_in_status\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map column headers to match TTFF schema standardization\n",
    "\n",
    "def map_columns(df):\n",
    "    map_columns_df = (\n",
    "        df\n",
    "        .rename(columns = {\n",
    "            input_trans_id: trans_id,\n",
    "            input_patient_id: patient_id,\n",
    "            input_pharmacy: pharmacy,\n",
    "            input_payer: payer,\n",
    "            input_payer_type: payer_type,\n",
    "            input_hcp_first_name: hcp_first_name,\n",
    "            input_hcp_last_name: hcp_last_name,\n",
    "            input_hcp_npi: hcp_npi,\n",
    "            input_hcp_state: hcp_state,\n",
    "            input_hcp_zip: hcp_zip,\n",
    "            input_medication: medication,\n",
    "            input_brand_col: brand,\n",
    "            input_pharmacy: pharmacy,\n",
    "            input_cs_outlet_id: cs_outlet_id,\n",
    "            input_cot: cot,\n",
    "            input_status: status,\n",
    "            input_substatus: substatus,\n",
    "            input_cust_status: cust_status,\n",
    "            input_cust_status_desc: cust_status_desc,\n",
    "            input_status_date: status_date,\n",
    "            input_dx_1: dx_1,\n",
    "            input_dx_2: dx_2,\n",
    "            input_referral_source: referral_source,\n",
    "            input_referral_date: referral_date,\n",
    "            input_hierarchy: hierarchy\n",
    "        })\n",
    "    )\n",
    "    return map_columns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Patient Journey (pj_id) identifier\n",
    "# (This ID is used for calculation purposes only.  It will not be published)\n",
    "# Sort data\n",
    "\n",
    "def pj(map_columns_df):\n",
    "    pj_df = (\n",
    "        map_columns_df\n",
    "        .assign(**{\n",
    "            'pj_id' : lambda x: (\n",
    "                x.groupby([patient_id, pharmacy, brand]).grouper.group_info[0]\n",
    "            )\n",
    "        })\n",
    "        .sort_values(\n",
    "            by=['pj_id', status_date, status, trans_id],\n",
    "            ascending=[True, True, False, True])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return pj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to First Fill calculation:\n",
    "#    1) Convert all strings to uppercase\n",
    "#    2) Omit records with \"Remove from TTFF\" hierarchy\n",
    "#    3) Filter to only include patient journeys where active shipment occurs\n",
    "#    4) Get status date of first active shipment (min_active_date) for each patient journey\n",
    "#    5) Filter to only journeys on or before first active shipment date. And if date = first active shipment date, hierarchy must be fulfillment.\n",
    "#    6) Calculate time spent in status (only include business days and non-holidays) using np.busday_count\n",
    "#    7) Convert all dates to long type (calculated as number of seconds between 1/1/1970 and the status date) - NO LONGER INCLUDING THIS STEP\n",
    "#    8) Filter/reorder columns to match schema\n",
    "\n",
    "def ttff(pj_df):\n",
    "    ttff_df = (\n",
    "        pj_df\n",
    "        .apply(lambda x: (x.str.upper() if x.dtype == 'O' else x))\n",
    "        .loc[lambda x: (\n",
    "            (x[hierarchy] != transform.remove_from_ttff)\n",
    "            &\n",
    "            (x.pj_id.isin(\n",
    "                x.loc[\n",
    "                    (\n",
    "                        (x[status] == transform.active_status_code)\n",
    "                        &\n",
    "                        (x[substatus] == transform.active_substatus_code)\n",
    "                    ),\n",
    "                    'pj_id'\n",
    "                ].tolist()\n",
    "            ))\n",
    "        )]\n",
    "        .assign(**{\n",
    "            min_active_date : lambda x: (\n",
    "                x.loc[\n",
    "                    (x[status] == transform.active_status_code)\n",
    "                    &\n",
    "                    (x[substatus] == transform.active_substatus_code)\n",
    "                ]\n",
    "                .groupby(['pj_id'])[status_date]\n",
    "                .transform(min)\n",
    "            )\n",
    "        })\n",
    "        .assign(**{\n",
    "            min_active_date : lambda x: (\n",
    "                x.groupby(['pj_id'])[min_active_date]\n",
    "                .transform(lambda x: x.ffill().bfill())\n",
    "            )\n",
    "        })\n",
    "        .loc[lambda x: (\n",
    "            (x[status_date] < x[min_active_date])\n",
    "            |\n",
    "            (\n",
    "                (x[status_date] == x[min_active_date])\n",
    "                &\n",
    "                (x[hierarchy] == transform.fulfillment_hierarchy)\n",
    "            )\n",
    "        )]\n",
    "        .assign(**{\n",
    "            next_status_date : lambda x: (\n",
    "                x.groupby(['pj_id'])[status_date]\n",
    "                .transform(lambda x: x.shift(-1))\n",
    "                .fillna(x[status_date])\n",
    "            ),\n",
    "            time_in_status : lambda x: (\n",
    "                np.busday_count(\n",
    "                    x[status_date].apply(lambda x: x.strftime('%Y-%m-%d')),\n",
    "                    x[next_status_date].apply(lambda x: x.strftime('%Y-%m-%d')),\n",
    "                    holidays = pd.to_datetime(cal().holidays()).strftime(\"%Y-%m-%d\").tolist()\n",
    "                )\n",
    "            )\n",
    "        })\n",
    "#        .assign(**{\n",
    "#            status_date_long : lambda x: (\n",
    "#                (x[status_date] - pd.to_datetime('1970-01-01'))\n",
    "#                .dt\n",
    "#                .total_seconds()\n",
    "#                .astype(int)),\n",
    "#            min_active_date : lambda x: (\n",
    "#                (x[min_active_date] - pd.to_datetime('1970-01-01'))\n",
    "#                .dt\n",
    "#                .total_seconds()\n",
    "#                .astype(int)),\n",
    "#            next_status_date_long : lambda x: (\n",
    "#                (x[next_status_date] - pd.to_datetime('1970-01-01'))\n",
    "#                .dt\n",
    "#                .total_seconds()\n",
    "#                .astype(int))\n",
    "#        })\n",
    "        .loc[:,output_columns]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return ttff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe = (\n",
    "    df\n",
    "    .pipe(map_columns)\n",
    "    .pipe(pj)\n",
    "    .pipe(ttff)\n",
    ")\n",
    "\n",
    "final_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST TRANSFORM OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 1: Check that time in status = 0 for all active shipments\n",
    "\n",
    "test1 = len(\n",
    "    final_dataframe\n",
    "    .loc[lambda x:\n",
    "        (\n",
    "            (x[status].isin([transform.active_status_code]))\n",
    "            &\n",
    "            (x[substatus].isin([transform.active_substatus_code]))\n",
    "            &\n",
    "            (x[time_in_status] > 0)\n",
    "        )]\n",
    ") == 0\n",
    "\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 2: Check that next status date = status date for active shipments\n",
    "\n",
    "test2 = len(\n",
    "    final_dataframe\n",
    "    .loc[lambda x:\n",
    "        (\n",
    "            (x[status].isin([transform.active_status_code]))\n",
    "            &\n",
    "            (x[substatus].isin([transform.active_substatus_code]))\n",
    "            &\n",
    "            (x[next_status_date] != x[status_date])\n",
    "        )]\n",
    ") == 0\n",
    "\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 3: Check that results don't contain any \"discontinued\" hierarchies\n",
    "\n",
    "test3 = len(\n",
    "    final_dataframe\n",
    "    .loc[lambda x: (\n",
    "        x[hierarchy].isin(transform.discontinued_hierarchy)\n",
    "    )]\n",
    ") == 0\n",
    "\n",
    "test3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TEST 4: Create a \"test\" dataframe\n",
    "\n",
    "test_data = ([\n",
    "    [1, 0, 'PENDING', 'OTHER', 'PENDING - OTHER'],\n",
    "    [1, 70, 'PENDING', 'OTHER', 'PENDING - OTHER'],\n",
    "    [1, 72, 'CANCEL', 'INSURANCE DENIED', 'BVPA'],\n",
    "    [1, 72, 'CANCEL', 'OTHER', 'BVPA'],\n",
    "    [1, 72, 'ACTIVE', 'SHIPMENT', transform.fulfillment_hierarchy],\n",
    "    [2, 0, 'PENDING', 'OTHER', 'REMOVE FROM TTFF'],\n",
    "    [2, 70, 'PENDING', 'OTHER', 'NO STATUS CLARITY'],\n",
    "    [2, 72, 'PENDING', 'OTHER', 'PENDING - OTHER'],\n",
    "    [2, 72, 'CANCEL', 'OTHER', 'PENDING - OTHER'],\n",
    "    [2, 72, 'ACTIVE', 'READY', transform.fulfillment_hierarchy],\n",
    "    [2, 73, 'ACTIVE', 'SHIPMENT', transform.fulfillment_hierarchy],\n",
    "    [2, 73, 'DISCONTINUED', 'OTHER', 'PATIENT'],\n",
    "    [3, 0, 'PENDING', 'OTHER', 'REMOVE FROM TTFF'],\n",
    "    [3, 1, 'PENDING', 'OTHER', 'REMOVE FROM TTFF'],\n",
    "    [3, 2, 'CANCEL', 'OTHER', 'REMOVE FROM TTFF'],\n",
    "    [3, 70, 'CANCEL', 'OTHER', 'NO STATUS CLARITY'],\n",
    "    [3, 71, 'ACTIVE', 'READY', transform.fulfillment_hierarchy],\n",
    "    [3, 72, 'ACTIVE', 'READY', transform.fulfillment_hierarchy]\n",
    "])\n",
    "\n",
    "test_df = (\n",
    "    pd.DataFrame(test_data, columns = [transform.patient_id, transform.status_date, transform.status, transform.substatus, transform.hierarchy])\n",
    "    .assign(**{\n",
    "        transform.status_date : lambda x: (\n",
    "            pd.to_datetime('2019-01-01', format='%Y-%m-%d') + pd.to_timedelta(x[transform.status_date], unit='d')\n",
    "        ),\n",
    "        transform.pharmacy : 'ABC',\n",
    "        transform.brand_col : 'A',\n",
    "        transform.trans_id : 1,\n",
    "        transform.trans_id: trans_id,\n",
    "        transform.payer: None,\n",
    "        transform.payer_type: None,\n",
    "        transform.hcp_first_name: None,\n",
    "        transform.hcp_last_name: None,\n",
    "        transform.hcp_npi: None,\n",
    "        transform.hcp_state: None,\n",
    "        transform.hcp_zip: None,\n",
    "        transform.medication: None,\n",
    "        transform.cs_outlet_id: None,\n",
    "        transform.cot: None,\n",
    "        transform.cust_status: None,\n",
    "        transform.cust_status_desc: None,\n",
    "        transform.dx_1: None,\n",
    "        transform.dx_2: None,\n",
    "        transform.referral_source: None,\n",
    "        transform.referral_date: None\n",
    "    })\n",
    "    \n",
    ")\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create an \"expected\" dataframe with expected results of test transformaion.\n",
    "\n",
    "expected_data = ([\n",
    "    [1, 0, 70, 'PENDING - OTHER', 47],\n",
    "    [1, 70, 72, 'PENDING - OTHER', 2],\n",
    "    [1, 72, 72, transform.fulfillment_hierarchy, 0],\n",
    "    [2, 70, 72, 'NO STATUS CLARITY', 2],\n",
    "    [2, 72, 72, 'PENDING - OTHER', 0],\n",
    "    [2, 72, 72, 'PENDING - OTHER', 0],\n",
    "    [2, 72, 73, transform.fulfillment_hierarchy, 1],\n",
    "    [2, 73, 73, transform.fulfillment_hierarchy, 0]\n",
    "])\n",
    "\n",
    "expected_df = (\n",
    "    pd.DataFrame(expected_data, columns = [patient_id, status_date, next_status_date, hierarchy, time_in_status])\n",
    "    .assign(**{\n",
    "        status_date : lambda x: (\n",
    "            pd.to_datetime('2019-01-01', format='%Y-%m-%d') + pd.to_timedelta(x[status_date], unit='d')\n",
    "        ),\n",
    "        next_status_date : lambda x: (\n",
    "            pd.to_datetime('2019-01-01', format='%Y-%m-%d') + pd.to_timedelta(x[next_status_date], unit='d')\n",
    "        )\n",
    "    })    \n",
    ")\n",
    "\n",
    "expected_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Apply transform to test dataframe\n",
    "\n",
    "test_output = (\n",
    "    test_df\n",
    "    .pipe(map_columns)\n",
    "    .pipe(pj)\n",
    "    .pipe(ttff))\n",
    "\n",
    "test_output"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check that results of test transformation match expected dataframe\n",
    "\n",
    "test4 = (\n",
    "    expected_df == \n",
    "    test_output.loc[:,[patient_id, status_date, next_status_date, hierarchy, time_in_status]]\n",
    ").all().all()\n",
    "\n",
    "test4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TEST 5: Check that necessary fields do not have null values\n",
    "\n",
    "test5 = len(\n",
    "    final_dataframe\n",
    "    .loc[lambda x: (\n",
    "        (x[status_date].isnull())\n",
    "        |\n",
    "        (x[next_status_date].isnull())\n",
    "        |\n",
    "        (x[min_active_date].isnull())\n",
    "        |\n",
    "        (x[hierarchy].isnull())\n",
    "        |\n",
    "        (x[time_in_status].isnull())\n",
    "    )]\n",
    ") == 0\n",
    "\n",
    "test5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TEST 6: Check that next status date >= status date for all records\n",
    "\n",
    "test6 = len(\n",
    "    final_dataframe\n",
    "    .loc[lambda x: (\n",
    "        x[status_date] > x[next_status_date]\n",
    "    )]\n",
    ") == 0\n",
    "\n",
    "test6"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# FINAL TEST: Did all 6 tests pass?\n",
    "\n",
    "test1 & test2 & test3 & test4 & test5 & test6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST: Did the first 3 tests pass?\n",
    "\n",
    "test1 & test2 & test3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## that's it - just provide the final dataframe to the var final_dataframe and we take it from there\n",
    "transform.publish_contract.publish(final_dataframe, run_id, session)\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
