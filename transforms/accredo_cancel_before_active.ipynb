{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.helpers.session_helper import SessionHelper\n",
    "session = SessionHelper().session\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ CONFIGURATION - PLEASE TOUCH **************\n",
    "Pipeline Builder configuration: creates configurations from variables specified here!!\n",
    "This cell will be off in production as configurations will come from the configuration postgres DB.\n",
    "\"\"\"\n",
    "# config vars: this dataset\n",
    "config_pharma = \"sun\" # the pharmaceutical company which owns {brand}\n",
    "config_brand = \"ilumya\" # the brand this pipeline operates on\n",
    "config_state = \"enrich\" # the state this transform runs in\n",
    "config_name = \"accredo_cancel_before_active\" # the name of this transform, which is the name of this notebook without .ipynb\n",
    "\n",
    "# input vars: dataset to fetch. Recall that a contract published to S3 has a key format branch/pharma/brand/state/name\n",
    "input_pharma = \"sun\"\n",
    "input_brand = \"ilumya\"\n",
    "input_state = \"ingest\"\n",
    "input_name = \"symphony_health_association_ingest_column_mapping\"\n",
    "input_branch = \"sun-extract-validation\" # if None, input_branch is automagically set to your working branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "Populating config mocker based on config parameters...\n",
    "\"\"\"\n",
    "import core.helpers.pipeline_builder as builder\n",
    "\n",
    "ids = builder.build(config_pharma, config_brand, config_state, config_name, session)\n",
    "transform_id = ids[0]\n",
    "run_id = ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "This section imports data from the configuration database\n",
    "and should not need to be altered or otherwise messed with. \n",
    "~~These are not the droids you are looking for~~\n",
    "\"\"\"\n",
    "from core.constants import BRANCH_NAME, ENV_BUCKET\n",
    "from core.models.configuration import Transformation\n",
    "from dataclasses import dataclass\n",
    "from core.dataset_contract import DatasetContract\n",
    "\n",
    "db_transform = session.query(Transformation).filter(Transformation.id == transform_id).one()\n",
    "\n",
    "@dataclass\n",
    "class DbTransform:\n",
    "    id: int = db_transform.id ## the instance id of the transform in the config app\n",
    "    name: str = db_transform.transformation_template.name ## the transform name in the config app\n",
    "    state: str = db_transform.pipeline_state.pipeline_state_type.name ## the pipeline state, one of raw, ingest, master, enhance, enrich, metrics, dimensional\n",
    "    branch:str = BRANCH_NAME ## the git branch for this execution \n",
    "    brand: str = db_transform.pipeline_state.pipeline.brand.name ## the pharma brand name\n",
    "    pharmaceutical_company: str = db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name # the pharma company name\n",
    "    publish_contract: DatasetContract = DatasetContract(branch=BRANCH_NAME,\n",
    "                            state=db_transform.pipeline_state.pipeline_state_type.name,\n",
    "                            parent=db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name,\n",
    "                            child=db_transform.pipeline_state.pipeline.brand.name,\n",
    "                            dataset=db_transform.transformation_template.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE Cartridge Notebook::accredo_cancel_before_active\n",
    "![CORE Logo](assets/coreLogo.png) \n",
    "\n",
    "---\n",
    "## Keep in Mind\n",
    "Good Transforms Are...\n",
    "- **singular in purpose:** good transforms do one and only one thing, and handle all known cases for that thing. \n",
    "- **repeatable:** transforms should be written in a way that they can be run against the same dataset an infinate number of times and get the same result every time. \n",
    "- **easy to read:** 99 times out of 100, readable, clear code that runs a little slower is more valuable than a mess that runs quickly. \n",
    "- **No 'magic numbers':** if a variable or function is not instantly obvious as to what it is or does, without context, maybe consider renaming it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow - how to use this notebook to make science\n",
    "#### Data Science\n",
    "1. **Document your transform.** Fill out the _description_ cell below describing what it is this transform does; this will appear in the configuration application where Ops will create, configure and update pipelines. \n",
    "1. **Define your config object.** Fill out the _configuration_ cell below the commented-out guide to define the variables you want ops to set in the configuration application (these will populate here for every pipeline). \n",
    "2. **Build your transformation logic.** Use the transformation cell to do that magic that you do. \n",
    "![caution](assets/cautionTape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "********* VARIABLES - PLEASE TOUCH ********* \n",
    "This section defines what you expect to get from the configuration application \n",
    "in a single \"transform\" object. Define the vars you need here, and comment inline to the right of them \n",
    "for all-in-one documentation. \n",
    "Engineering will build a production \"transform\" object for every pipeline that matches what you define here.\n",
    "\n",
    "@@@ FORMAT OF THE DATA CLASS IS: @@@ \n",
    "\n",
    "<variable_name>: <data_type> #<comment explaining what the value is to future us>\n",
    "\n",
    "e.g.\n",
    "\n",
    "class Transform(DbTransform):\n",
    "    some_ratio: float\n",
    "    site_name: str\n",
    "\n",
    "~~These ARE the droids you are looking for~~\n",
    "\"\"\"\n",
    "\n",
    "class Transform(DbTransform):\n",
    "    '''\n",
    "    YOUR properties go here!!\n",
    "    Variable properties should be assigned to the exact name of\n",
    "    the transformation as it appears in the Jupyter notebook filename.\n",
    "    '''\n",
    "    # Column headers\n",
    "    status_date: str = 'status_date'# Status date column\n",
    "    ref_date: str = 'rec_date'# Referral date column\n",
    "    patient: str = 'msa_patient_id' # Patient ID column\n",
    "    pharm: str = 'pharm_code' # Pharmacy Name column\n",
    "    product: str = 'medication' # Medication Name column\n",
    "    status: str = 'status_code' # Status column\n",
    "    substatus: str = 'sub_status' # Substatus column\n",
    "    ic_status: str = 'integrichain_status' # IntegriChain Status column\n",
    "    ic_substatus: str = 'integrichain_sub_status' # IntegriChain Substatus column\n",
    "    pjh: str = 'Patient_Journey_Hierarchy' # Patient Journey Hierarchy column\n",
    "    brand: str = 'brand' # Brand column (medication without strength)\n",
    "        \n",
    "    # Possible status values\n",
    "    pending: str = 'PENDING' # Pending status 'PENDING'\n",
    "    active: str = 'ACTIVE' # Active status 'ACTIVE'\n",
    "    cancelled: str = 'CANCELLED' # Cancelled status 'CANCELLED'\n",
    "    discontinued: str = 'DISCONTINUED' # Discontinued status 'DISCONTINUED'\n",
    "        \n",
    "    # Possible substatus values\n",
    "    pending_new: str = 'NEW' # New substatus when status is 'PENDING'\n",
    "    active_shipped: str = 'SHIPMENT'# Shipment substatus when status is 'ACTIVE'\n",
    "    \n",
    "    # Possible PJH values\n",
    "    bvpa: str = 'BV/PA' # BV/PA pjh\n",
    "    intake: str = 'Intake' # Intake pjh\n",
    "    fulfillment: str = 'Fulfillment' # Fulfillment pjh\n",
    "    transfer: str = 'Transferred' # Transferred pjh\n",
    "    payer: str = 'Payer' # Payer pjh\n",
    "    no_clarity: str = 'NO STATUS CLARITY' # Final result of enrichment\n",
    "        \n",
    "    # Pharmacy this transform applies to\n",
    "    accredo: str = 'ACCREDO'\n",
    "\n",
    "transform = Transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please place your value assignments for development here!!\n",
    "## This cell will be turned off in production and Engineering will set to pull from the configuration application instead\n",
    "## For the last example, this could look like...\n",
    "## transform.some_ratio = 0.6\n",
    "## transform.site_name = \"WALGREENS\"\n",
    "\n",
    "# Vars\n",
    "patient = transform.patient\n",
    "pharm = transform.pharm\n",
    "product = transform.product\n",
    "ref_date = transform.ref_date\n",
    "status_date = transform.status_date\n",
    "status = transform.status\n",
    "substatus = transform.substatus\n",
    "ic_status = transform.ic_status\n",
    "ic_substatus = transform.ic_substatus\n",
    "pjh = transform.pjh\n",
    "brand = transform.brand\n",
    "\n",
    "# Values\n",
    "pending = transform.pending\n",
    "active = transform.active\n",
    "cancelled = transform.cancelled\n",
    "discontinued = transform.discontinued\n",
    "pending_new = transform.pending_new\n",
    "active_shipped = transform.active_shipped\n",
    "bvpa = transform.bvpa\n",
    "intake = transform.intake\n",
    "fulfillment = transform.fulfillment\n",
    "transfer = transform.transfer\n",
    "payer = transform.payer\n",
    "no_clarity = transform.no_clarity\n",
    "accredo = transform.accredo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "![what my transform does](assets/ds301_accredo_enrichment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ FETCH DATA - TOUCH, BUT CAREFULLY **************\n",
    "This cell will be turned off in production, as the input_contract will be handled by the pipeline.\n",
    "\"\"\"\n",
    "\n",
    "if not input_branch:\n",
    "    input_branch = BRANCH_NAME\n",
    "input_contract = DatasetContract(branch=input_branch, state=input_state, parent=input_pharma, child=input_brand, dataset=input_name)\n",
    "run_filter = []\n",
    "run_filter.append(dict(partition=\"__metadata_run_id\", comparison=\"==\", values=[3]))\n",
    "# IF YOU HAVE PUBLISHED DATA MULTIPLE TIMES, uncomment the above line and change the int to the run_id to fetch.\n",
    "# Otherwise, you will have duplicate values in your fetched dataset!\n",
    "final_dataframe = input_contract.fetch(filters=run_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy ingested data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_dataframe.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute PJH and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell brings in the Patient Journey Hierarchy information from a local file.\n",
    "This is just a placeholder to ensure data processes work as expected until the\n",
    "true ingest data can be used.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "pd.options.display.max_columns=999\n",
    "\n",
    "os.chdir('{}'.format(os.path.expanduser('~')))\n",
    "status_config = pd.read_csv('status_mapping.csv')\n",
    "\n",
    "status_config = (\n",
    "    status_config\n",
    "    .assign(**{\n",
    "        status: lambda x:(\n",
    "            x['statusCode'].str.upper()\n",
    "        ),\n",
    "        substatus: lambda x:(\n",
    "            x['subStatus'].str.upper()\n",
    "        ),\n",
    "        ic_status: lambda x:(\n",
    "            x['integrichain_status'].str.upper()\n",
    "        ),\n",
    "        ic_substatus: lambda x:(\n",
    "            x['integrichain_sub_status'].str.upper()\n",
    "        ),\n",
    "        pjh: lambda x:(\n",
    "            x['Patient_Journey_Hierarchy'].str.upper()\n",
    "        )\n",
    "    })\n",
    "    .drop(columns=[\n",
    "        'statusCode',\n",
    "        'subStatus'\n",
    "    ])\n",
    ")\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .loc[:,\n",
    "        [ref_date,\n",
    "         patient,\n",
    "         pharm,\n",
    "         product,\n",
    "         status_date,\n",
    "         status,\n",
    "         substatus]\n",
    "        ]\n",
    "    .fillna('NONE')\n",
    "    .assign(**{\n",
    "        brand: lambda x:(\n",
    "            x[product].apply(lambda x: x.split()[0].strip())\n",
    "        ),\n",
    "        status: lambda x:(\n",
    "            x[status].str.upper()\n",
    "        ),\n",
    "        substatus: lambda x:(\n",
    "            x[substatus].str.upper()\n",
    "        )\n",
    "    })\n",
    "    .assign(**{\n",
    "        status_date: lambda x:(\n",
    "            pd.to_datetime(\n",
    "                x[status_date].str[:8].astype(str),\n",
    "                errors='coerce'\n",
    "        )),\n",
    "        ref_date: lambda x:(\n",
    "            pd.to_datetime(\n",
    "                x[ref_date].str[:8].astype(str),\n",
    "                errors='coerce'\n",
    "        )),\n",
    "        'min_status_date': lambda x:(\n",
    "            x.groupby([patient, pharm, brand])[status_date].transform(min)\n",
    "        )\n",
    "    })\n",
    "    .fillna(value={ref_date: 'min_status_date'})\n",
    "    .drop(columns=['min_status_date'])\n",
    "    .merge(status_config, how='left', on=[status, substatus])\n",
    "    .drop_duplicates()\n",
    "    .sort_values(\n",
    "        [patient, pharm, brand, status_date]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and reset index to join on later\n",
    "df = (\n",
    "    df\n",
    "    .sort_values([patient, pharm, brand, status_date, ref_date, status])\n",
    "    .reset_index(drop=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create secondary df to find the first active shipment date for\n",
    "# each patient journey\n",
    "min_shipped_df = (\n",
    "    df\n",
    "    .loc[\n",
    "        (df[ic_status] == active) &\n",
    "        (df[ic_substatus] == active_shipped)\n",
    "    ]\n",
    "    .groupby([patient, pharm, brand])\n",
    "    [status_date]\n",
    "    .min()\n",
    "    .reset_index()\n",
    "    .rename(columns={status_date: 'first_shipped_date'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to main df to get first active shipment date for every\n",
    "# patient that has one\n",
    "df = (\n",
    "    df\n",
    "    .merge(\n",
    "        min_shipped_df,\n",
    "        how='left'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column that can be grouped on later to determine the\n",
    "# first date where a desired care occurs\n",
    "df = (\n",
    "    df\n",
    "    .assign(status_spree=(\n",
    "    ~(\n",
    "        # Check to make sure patient journey for row above is the same\n",
    "        (df[patient].eq(df[patient].shift(1))) &\n",
    "        (df[pharm].eq(df[pharm].shift(1))) &\n",
    "        (df[brand].eq(df[brand].shift(1))) &\n",
    "        # Check to make sure statuses in row above are the same\n",
    "        (df[ic_status].eq(df[ic_status].shift(1))) &\n",
    "        (df[ic_substatus].eq(df[ic_substatus].shift(1)))\n",
    "    )\n",
    "    ).cumsum())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column denoting the min status date of each spree\n",
    "df = (\n",
    "    df\n",
    "    .assign(min_status_date=lambda x:(\n",
    "        x.groupby('status_spree')[status_date].transform(min)\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bool column to denote row being right above active shipment row\n",
    "df = (\n",
    "    df\n",
    "    .assign(above_shipment_step=lambda x:(\n",
    "        # Check to make sure patient journey in row below is the same\n",
    "        (x[patient].eq(x[patient].shift(-1))) &\n",
    "        (x[pharm].eq(x[pharm].shift(-1))) &\n",
    "        (x[brand].eq(x[brand].shift(-1))) &\n",
    "        # Check to make sure status in row below is active/shipped\n",
    "        (x[ic_status].shift(-1) == active) &\n",
    "        (x[ic_substatus].shift(-1) == active_shipped)\n",
    "    ))\n",
    ")\n",
    "\n",
    "# Pick out sprees that are above a shipment\n",
    "above_shipment_series = df.groupby('status_spree').above_shipment_step.any()\n",
    "\n",
    "# Assign True to those sprees\n",
    "df = (\n",
    "    df\n",
    "    .assign(above_shipment=lambda x:(\n",
    "        x.status_spree.isin(above_shipment_series[above_shipment_series].index)\n",
    "    ))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denote how many days between min_status_date and first_shipped_date\n",
    "df = (\n",
    "    df\n",
    "    .assign(day_diff=(df['min_status_date'] - df['first_shipped_date']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\n",
    "    (df[patient] == accredo)\n",
    "    # Negative for day diff, because we only want status sprees that\n",
    "    # occur BEFORE the first active shipment\n",
    "    (df.day_diff < np.timedelta64(-2, 'D')) &\n",
    "    (df.above_shipment == True) &\n",
    "    (\n",
    "        (df[ic_status].isin([cancelled, discontinued])) |\n",
    "        (\n",
    "            (df[ic_status] == pending) &\n",
    "            (df[ic_substatus] == pending_new)\n",
    "        )\n",
    "    ),\n",
    "    # Change pjh to 'NO STATUS CLARITY' for the records that pass\n",
    "    pjh\n",
    "] = no_clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe = (\n",
    "    df\n",
    "    .set_index('index')\n",
    "    .sort_index()\n",
    "    .drop(columns=['first_shipped_date', 'status_spree', 'min_status_date', 'above_shipment_step', 'above_shipment', 'day_diff'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## that's it - just provide the final dataframe to the var final_dataframe and we take it from there\n",
    "transform.publish_contract.publish(final_dataframe, run_id, session)\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
