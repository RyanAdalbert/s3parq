{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"CELL1\"></a>\n",
    "## CELL 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-07 11:01:23,749 - core.helpers.session_helper.SessionHelper - INFO - Creating session for dev environment...\n",
      "2019-08-07 11:01:23,843 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating administrator mocks.\n",
      "2019-08-07 11:01:23,856 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating administrator mocks.\n",
      "2019-08-07 11:01:23,858 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pharmaceutical company mocks.\n",
      "2019-08-07 11:01:23,870 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pharmaceutical company mocks.\n",
      "2019-08-07 11:01:23,871 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating brand mocks.\n",
      "2019-08-07 11:01:23,876 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating brand mocks.\n",
      "2019-08-07 11:01:23,877 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating segment mocks.\n",
      "2019-08-07 11:01:23,887 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating segment mocks.\n",
      "2019-08-07 11:01:23,888 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline type mocks.\n",
      "2019-08-07 11:01:23,892 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline type mocks.\n",
      "2019-08-07 11:01:23,894 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline mocks.\n",
      "2019-08-07 11:01:23,901 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline mocks.\n",
      "2019-08-07 11:01:23,903 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline state type mocks.\n",
      "2019-08-07 11:01:23,908 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline state type mocks.\n",
      "2019-08-07 11:01:23,909 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline state mocks.\n",
      "2019-08-07 11:01:23,913 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline state mocks.\n",
      "2019-08-07 11:01:23,916 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating tag mocks.\n",
      "2019-08-07 11:01:23,920 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating tag mocks.\n",
      "2019-08-07 11:01:23,922 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation_template mocks.\n",
      "2019-08-07 11:01:23,928 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_template mocks.\n",
      "2019-08-07 11:01:23,931 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating bridge table mocks for transformation_templates <=> tags.\n",
      "2019-08-07 11:01:23,937 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_templates_tags mocks.\n",
      "2019-08-07 11:01:23,938 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation mocks.\n",
      "2019-08-07 11:01:23,943 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation mocks.\n",
      "2019-08-07 11:01:23,944 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation_variables mocks\n",
      "2019-08-07 11:01:23,948 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_variables mocks.\n",
      "2019-08-07 11:01:23,950 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating run_events mocks.\n",
      "2019-08-07 11:01:23,955 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating run_events mocks.\n",
      "2019-08-07 11:01:23,956 - core.helpers.session_helper.SessionHelper - INFO - Done. Created dev session with mock data.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"CELL 1\n",
    "builds and returns a database session\n",
    "local assumes a psql instance in a local docker container\n",
    "only postgres database is supported for configuration_application at this time\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "gets env-based configuration secret\n",
    "returns a session to the configuration db\n",
    "for dev env it pre-populates the database with helper and seed data\n",
    "\"\"\"\n",
    "from core.helpers.session_helper import SessionHelper\n",
    "session = SessionHelper().session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIGURATION - PLEASE TOUCH\n",
    "### <font color=pink>This cell will be off in production as configurations will come from the configuration postgres DB</color>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ CONFIGURATION - PLEASE TOUCH **************\n",
    "Pipeline Builder configuration: creates configurations from variables specified here!!\n",
    "This cell will be off in production as configurations will come from the configuration postgres DB.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "PIPELINE STATE:\n",
    "\n",
    "raw-->ingest-->master-->enhance-->enrich-->metrics-->dimensional\n",
    "\n",
    "\"\"\"\n",
    "# config vars: this dataset\n",
    "config_pharma = \"sun\" # the pharmaceutical company which owns {brand}\n",
    "config_brand = \"ilumya\" # the brand this pipeline operates on\n",
    "config_state = \"enrich\" # the state this transform runs in\n",
    "config_name = \"enrich_patient_journey_hierarchy\" # the name of this transform, which is the name of this notebook without .ipynb\n",
    "\n",
    "# input vars: dataset to fetch. \n",
    "# Recall that a contract published to S3 has a key format branch/pharma/brand/state/name\n",
    "input_branch = \"sun-extract-validation\"\n",
    "# None\n",
    "# if None, input_branch is automagically set to your working branch\n",
    "input_pharma = \"sun\"\n",
    "input_brand = \"ilumya\"\n",
    "input_state = \"ingest\"\n",
    "input_name = \"symphony_health_association_ingest_column_mapping\"\n",
    "\n",
    "#This contract defines the base of the output structure of data into S3.\n",
    "#\n",
    "#contract structure in s3: \n",
    "#s3:// {ENV} / {BRANCH} / {PARENT} / {CHILD} / {STATE} / {name of input}\n",
    "#\n",
    "#ENV - environment Must be one of development, uat, production.\n",
    "#Prefixed with integrichain- due to global unique reqirement\n",
    "#BRANCH - the software branch for development this will be the working pull request (eg pr-225)\n",
    "#in uat this will be edge, in production this will be master\n",
    "#PARENT - The top level source identifier\n",
    "#this is generally the customer (and it is aliased as such) but can be IntegriChain for internal sources,\n",
    "#or another aggregator for future-proofing\n",
    "#CHILD - The sub level source identifier, generally the brand (and is aliased as such)\n",
    "#STATE - One of: raw, ingest, master, enhance, enrich, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=orange>SETUP - DON'T TOUCH </font>\n",
    "Populating config mocker based on config parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-07 11:01:46,754 - core.logging - DEBUG - Adding/getting mocks for specified configurations...\n",
      "2019-08-07 11:01:46,790 - core.logging - DEBUG - Done. Creating mock run event and committing results to configuration mocker.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "Populating config mocker based on config parameters...\n",
    "\"\"\"\n",
    "import core.helpers.pipeline_builder as builder\n",
    "\n",
    "ids = builder.build(config_pharma, config_brand, config_state, config_name, session)\n",
    "\"\"\"\n",
    "RETURNS: A list of 2 items: [transformation_id, run_id] where transformation_id corresponds\n",
    "to the configuration created/found for {transformation} and run_id is a randomly generated 6 digit\n",
    "number (to avoid publishing to the same place with the same dataset)\n",
    "\"\"\"\n",
    "transform_id = ids[0]\n",
    "run_id = ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-07 12:09:32,453 - root - DEBUG - Transform Id:6 Run Id:104404\n"
     ]
    }
   ],
   "source": [
    "# debug only\n",
    "# e.g.\n",
    "# 6\n",
    "# 136126\n",
    "log.debug(f'Transform Id:{transform_id} Run Id:{run_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=orange>SETUP - DON'T TOUCH </font>\n",
    "This section imports data from the configuration database\n",
    "and should not need to be altered or otherwise messed with. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"************ SETUP - DON'T TOUCH **************\n",
    "This section imports data from the configuration database\n",
    "and should not need to be altered or otherwise messed with. \n",
    "~~These are not the droids you are looking for~~\n",
    "\"\"\"\n",
    "from core.constants import BRANCH_NAME, ENV_BUCKET, BATCH_JOB_QUEUE\n",
    "from core.helpers.session_helper import SessionHelper\n",
    "from core.models.configuration import Transformation\n",
    "from dataclasses import dataclass\n",
    "from core.dataset_contract import DatasetContract\n",
    "\n",
    "\n",
    "db_transform = session.query(Transformation).filter(Transformation.id == transform_id).one()\n",
    "\n",
    "@dataclass\n",
    "class DbTransform:\n",
    "    id: int = db_transform.id ## the instance id of the transform in the config app\n",
    "    name: str = db_transform.transformation_template.name ## the transform name in the config app\n",
    "    state: str = db_transform.pipeline_state.pipeline_state_type.name ## the pipeline state, one of raw, ingest, master, enhance, enrich, metrics, dimensional\n",
    "    branch:str = BRANCH_NAME ## the git branch for this execution \n",
    "    brand: str = db_transform.pipeline_state.pipeline.brand.name ## the pharma brand name\n",
    "    pharmaceutical_company: str = db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name # the pharma company name\n",
    "    publish_contract: DatasetContract = DatasetContract(branch=BRANCH_NAME,\n",
    "                            state=db_transform.pipeline_state.pipeline_state_type.name,\n",
    "                            parent=db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name,\n",
    "                            child=db_transform.pipeline_state.pipeline.brand.name,\n",
    "                            dataset=db_transform.transformation_template.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-07 12:10:54,216 - root - DEBUG - Branch name:DC-546_ENRICH_PATIENT_JOURNEY_HIERARCHY Env Bucket:ichain-dev Batch Job Queue:dev-core\n"
     ]
    }
   ],
   "source": [
    "#debug only\n",
    "# e.g:\n",
    "#DC-578_PatientStatus\n",
    "#ichain-dev\n",
    "#dev-core\n",
    "log.debug(f'Branch name:{BRANCH_NAME} Env Bucket:{ENV_BUCKET} Batch Job Queue:{BATCH_JOB_QUEUE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# CORE Cartridge Notebook::[master_patient_status]\n",
    "![CORE Logo](assets/coreLogo.png) \n",
    "\n",
    "---\n",
    "## Keep in Mind\n",
    "Good Transforms Are...\n",
    "- **singular in purpose:** good transforms do one and only one thing, and handle all known cases for that thing. \n",
    "- **repeatable:** transforms should be written in a way that they can be run against the same dataset an infinate number of times and get the same result every time. \n",
    "- **easy to read:** 99 times out of 100, readable, clear code that runs a little slower is more valuable than a mess that runs quickly. \n",
    "- **No 'magic numbers':** if a variable or function is not instantly obvious as to what it is or does, without context, maybe consider renaming it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow - how to use this notebook to make science\n",
    "#### Data Science\n",
    "1. **Document your transform.** Fill out the _description_ cell below describing what it is this transform does; this will appear in the configuration application where Ops will create, configure and update pipelines. \n",
    "1. **Define your config object.** Fill out the _configuration_ cell below the commented-out guide to define the variables you want ops to set in the configuration application (these will populate here for every pipeline). \n",
    "2. **Build your transformation logic.** Use the transformation cell to do that magic that you do. \n",
    "![caution](assets/cautionTape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIGURATION - VARIABLES - PLEASE TOUCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "CONFIGURATION ********* VARIABLES - PLEASE TOUCH ********* \n",
    "This section defines what you expect to get from the configuration application \n",
    "in a single \"transform\" object. Define the vars you need here, and comment inline to the right of them \n",
    "for all-in-one documentation. \n",
    "Engineering will build a production \"transform\" object for every pipeline that matches what you define here.\n",
    "\n",
    "@@@ FORMAT OF THE DATA CLASS IS: @@@ \n",
    "\n",
    "<variable_name>: <data_type> #<comment explaining what the value is to future us>\n",
    "e.g.\n",
    "class Transform(DbTransform):\n",
    "    some_ratio: float\n",
    "    site_name: str\n",
    "\n",
    "~~These ARE the droids you are looking for~~\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "imports\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from core.logging import get_logger\n",
    " \n",
    "class Transform(DbTransform):\n",
    "    '''\n",
    "    YOUR properties go here!!\n",
    "    Variable properties should be assigned to the exact name of\n",
    "    the transformation as it appears in the Jupyter notebook filename.\n",
    "    ''' \n",
    "\n",
    "    ## cols of interest\n",
    "    ## Field\t     Description\tData Type / Format\tComments\tRequired\t            OLD Name in Data Model\tNew Name in Data Model\t<-- Map to\tBIPI\tSun\tAlkermes - Status\n",
    "    ## Status Code\tStatus Code\t    X(20)\t\t                    Required if Available\tstatus\t                status\t\t\t\t\n",
    "    col_status: str \n",
    "    col_substatus: str \n",
    "    customer_name: str        \n",
    "    \n",
    "    \n",
    "    def enrich_patient_journey_hierarchy(self,df):\n",
    "        try:        \n",
    "            go = False # assume things are not working YET.\n",
    "           \n",
    "            dffail = pd.DataFrame() # initialize df for fails\n",
    "            \n",
    "            # master data golden status\n",
    "            #working draft] Gold Domain of patient status\n",
    "            logger.info('try:')\n",
    "            status_dict = {}\n",
    "            #status_dict[1]='ACTIVE'\n",
    "            #status_dict[2]='CANCELLED'\n",
    "            #status_dict[3]='DISCONTINUED'\n",
    "            #status_dict[4]='DENIED'\n",
    "            #status_dict[5]='PENDING'\n",
    "            \n",
    "            # store the golden values in a list\n",
    "            #status_list = list(status_dict.values())           \n",
    "            \n",
    "            # log metadata \n",
    "            \n",
    "            logger.info(f'Gold Domain List:{status_list}')\n",
    "            \n",
    "            # df in\n",
    "            dfShape = df.shape\n",
    "            logger.info(f'df in  shape: {dfShape[0]} {dfShape[1]}')\n",
    "            logger.info(f'df in {df.head()}') \n",
    "            \n",
    "            # are we expecting certain column names? YES \n",
    "            statusColNameExpected = transform.col_status\n",
    "            substatusColNameExpected = transform.col_substatus\n",
    "            \n",
    "            logger.info(f'expecting column name  referral as:{referralColNameExpected}')\n",
    "            columnNamesArr = df.columns.values.tolist()\n",
    "            logger.info(f'df column names:{columnNamesArr}')\n",
    "            \n",
    "            if statusColNameExpected in columnNamesArr and substatusColNameExpected in columnNamesArr:\n",
    " \n",
    "                # apply Upper Case to col(s) values of interest\n",
    "                # apply strip  to col(s) values of interest\n",
    "                df[statusColNameExpected]= df[statusColNameExpected].apply(lambda x: x.upper() if x is not None else x)   \n",
    "                df[statusColNameExpected]= df[statusColNameExpected].apply(lambda x: x.strip() if x is not None else x)\n",
    "                df[substatusColNameExpected]= df[substatusColNameExpected].apply(lambda x: x.upper() if x is not None else x)   \n",
    "                df[substatusColNameExpected]= df[substatusColNameExpected].apply(lambda x: x.strip() if x is not None else x)\n",
    "   \n",
    "    \n",
    "                # what fails\n",
    "                dffail =  pd.DataFrame()\n",
    "                # dffail = df[~df[statusColNameExpected].isin(status_list)]\n",
    "                # apply master selection for the column of interest\n",
    "                # what passes\n",
    "                df = pd.DataFrame()\n",
    "                #df = df[df[statusColNameExpected].isin(status_list)]\n",
    "                \n",
    "                # meta data log for what comes out of the function pass df\n",
    "                # meta data log for what comes out of the function pass and fail df\n",
    "                dfOutSize = df.size\n",
    "                dfOutShape = df.shape\n",
    "                dffailSize = dffail.size\n",
    "                dffailShape = dffail.shape\n",
    "                logger.info(f'df in   shape: {dfShape[0]} {dfShape[1]}')                \n",
    "                logger.info(f'df pass shape: {dfOutShape[0]} {dfOutShape[1]}')\n",
    "                logger.info(f'df fail shape: {dffailShape[0]} {dffailShape[1]}')\n",
    "                logger.info(f'df pass {df.head()}')\n",
    "                logger.info(f'df fail {dffail.head()}')  \n",
    "                go = True\n",
    "            else:\n",
    "                go = False # something did not work\n",
    "                logger.exception('expecting column names for patient status substatus if/else exception raise')\n",
    "                raise Exception('expecting column names for patient status substatus if/else exception raise')              \n",
    "        except Exception as e:\n",
    "            go = False # something did not work\n",
    "            logger.exception(f'exception:{e}')\n",
    "            raise Exception(str(e))\n",
    "        else:\n",
    "            pass\n",
    "        finally:\n",
    "            pass\n",
    "        return df.copy(),dffail.copy(),go\n",
    "                \n",
    "\n",
    "transform = Transform()\n",
    "logger = get_logger(f\"core.transforms.{transform.state}.{transform.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Please place your value assignments for development below*\n",
    "### <font color=pink>This cell will be turned off in production, Engineering will set to pull from the configuration</color>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please place your value assignments for development here!!\n",
    "## This cell will be turned off in production and Engineering will set to pull from the configuration application instead\n",
    "## For the last example, this could look like...\n",
    "## transform.some_ratio = 0.6\n",
    "## transform.site_name = \"WALGREENS\"\n",
    "\n",
    "transform.customer_name = 'sun'\n",
    "transform.col_status = 'status_code'\n",
    "transform.col_substatus = 'sub_status'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "What does this transformation do? be specific.\n",
    "\n",
    "![what does your transform do](assets/what.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planned\n",
    "\n",
    "We map status/sub-status combinations to a patient journey model in order to provide actionable insights to our customers.\n",
    "\n",
    "Definition of Done:\n",
    "\n",
    "- by status/sub-status combination - map to a patient journey bucket defined in a bridge table.\n",
    "- process to kick out any statuses/sub-statuses that are NOT mapped. \n",
    "\n",
    "**Transform assumes MASTER patient status and MASTER substatus transforms at a minimum have been run.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FETCH DATA - TOUCH, BUT CAREFULLY\n",
    "### <font color=pink>This cell will be turned off in production, as the input_contract will be handled by the pipeline</color>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ FETCH DATA - TOUCH, BUT CAREFULLY **************\n",
    "This cell will be turned off in production, as the input_contract will be handled by the pipeline.\n",
    "\"\"\"\n",
    "logger.info(\"FETCH DATA CELL - TOUCH - This cell will be turned off in production, as the input_contract will be handled by the pipeline. \")\n",
    "\n",
    "# for testing / development only\n",
    "run_id = 3\n",
    "\n",
    "if not input_branch:\n",
    "    input_branch = BRANCH_NAME\n",
    "input_contract = DatasetContract(branch=input_branch,\n",
    "                                 state=input_state, \n",
    "                                 parent=input_pharma, \n",
    "                                 child=input_brand, \n",
    "                                 dataset=input_name)\n",
    "run_filter = []\n",
    "run_filter.append(dict(partition=\"__metadata_run_id\", comparison=\"==\", values=[run_id]))\n",
    "# IF YOU HAVE PUBLISHED DATA MULTIPLE TIMES, uncomment the above line and change the int to the run_id to fetch.\n",
    "# Otherwise, you will have duplicate values in your fetched dataset!\n",
    "\n",
    "# bypass/comment out when unit testing individual parquet files\n",
    "df = input_contract.fetch(filters=run_filter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *<font color=grey>unit test development only*</font>\n",
    "*<font color=grey>The next **5** cells will be deleted in production.* </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyarrow.parquet as pq\n",
    "#import s3fs\n",
    "\n",
    "#def pandas_from_parquet_s3(file_path):  \n",
    "#    s3 = s3fs.S3FileSystem()\n",
    "#    df = (\n",
    "#        pq\n",
    "#        .ParquetDataset(file_path, filesystem=s3)\n",
    "#        .read_pandas()\n",
    "#        .to_pandas()\n",
    "#    )    \n",
    "#    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test/development \n",
    "# isolate on individual parquet files\n",
    "#TEST 1\n",
    "#df = pandas_from_parquet_s3('ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping/__metadata_run_id=3/90ca3aa7b0bb4246a281591b013ff54e.parquet')\n",
    "# TEST 2\n",
    "#df = pandas_from_parquet_s3('ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping/__metadata_run_id=3/d7ad974cef284e19aa7b5ac410220b96.parquet')\n",
    "# TEST 3\n",
    "#df = pandas_from_parquet_s3('ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping/__metadata_run_id=3/1a6ffd3598d442e38fbba66ea85a55a2.parquet')\n",
    "# TEST 4\n",
    "#df = pandas_from_parquet_s3('ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping/__metadata_run_id=3/5c00059d9fc04b0e8bc4ce764c50f3fb.parquet')\n",
    "# TEST 5\n",
    "# df = pandas_from_parquet_s3('ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping/__metadata_run_id=3/6eceb7ce59bd4dec8720316b4209b0e3.parquet')\n",
    "# THEN ALL TEST use \n",
    "# then use the FETCH DATA - TOUCH, BUT CAREFULLY CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test/development only\n",
    "# before shot unit testing only\n",
    "# dfSize = df.size\n",
    "# dfShape = df.shape\n",
    "# print('shape: {} {}'.format(dfShape[0],dfShape[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test/development only\n",
    "# needed to see the col(s) of interest\n",
    "#pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unit test/development only\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>**CALL**</font> THE TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use the variables above to execute your transformation.\n",
    "### the final output needs to be a variable named final_dataframe\n",
    "logger.info(\"CALL THE TRANSFORM - execute your transformation - the final output needs to be a variable named final_dataframe\")\n",
    "\n",
    "final_dataframe, final_fail, go = transform.enrich_patient_journey_hierarchy(df)\n",
    "\n",
    "if go==True:\n",
    "    logger.info(\"CALL THE TRANSFORM -  go no go = GO\")\n",
    "elif go==False:\n",
    "    logger.info(\"CALL THE TRANSFORM -  go no go = NO go\")\n",
    "else:\n",
    "    go=False\n",
    "    logger.info(\"CALL THE TRANSFORM -  go no go = unknown make it NO go\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *<font color=grey>unittest python*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "def ut_shape(final_dataframe,df):\n",
    "    \"\"\"\n",
    "    assertion will change based on coding state\n",
    "    \"\"\"\n",
    "    return final_dataframe.shape == df.shape\n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "    \n",
    "    def test_ut_shape(self):\n",
    "        \n",
    "        self.assertEqual(ut_shape(final_dataframe,df),True)\n",
    "                \n",
    "\"\"\"\n",
    "expect ok     9:45 am 2019 23 july since transform function\n",
    "is doing nothing yet\n",
    "expect FAIL   UNIT TEST 1 as input\n",
    "expect ok for UNIT TEST 4 file, no fails\n",
    "expect ok for UNIT TEST 5 file, no fails\n",
    "expect FAIL for run_id = 3 (all parquet files from the individual unit testing)\n",
    "\"\"\"\n",
    "# for development only\n",
    "unittest.main(argv=[''], verbosity= 2, exit=False)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untit test/development only look at the fails\n",
    "#final_fail.head()\n",
    "#final_fail[transform.col_status]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untit test/development only look at the pass(es)\n",
    "#final_dataframe.head()\n",
    "#final_dataframe[transform.col_status]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **publish**\n",
    "### Writing to S3\n",
    "Invoke the `publish()` command to write to a given contract. Some things to know:\n",
    "- To invoke publish a contract must be at the grain of dataset. This is because file names will be set by the dataframe=\\>parquet conversion. \n",
    "- publish only accepts a pandas dataframe.\n",
    "- publish does not allow for timedelta data types at this time (this is missing functionality in pyarrow).\n",
    "- publish handles partitioning the data as per contract, creating file paths, and creating the binary parquet files in S3, as well as the needed metadata. <br>\n",
    "**- by default, all datasets include a single partition, \\_\\_metadata\\_run\\_id, the RunEvent ID of an executed pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## that's it - just provide the final dataframe to the var final_dataframe and we take it from there\n",
    "if go==True:\n",
    "    logger.info(\"PUBLISH - that's it - its a GO - just provide the final dataframe to the var final_dataframe and we take it from there\")\n",
    "    transform.publish_contract.publish(final_dataframe, run_id, session)\n",
    "elif go==False:\n",
    "    logger.info(\"PUBLISH -  go no go = NO go -  so DONT publish\")\n",
    "else:\n",
    "    go=False\n",
    "    logger.info(\"PUBLISH -  go no go = unknown make it NO go - so DONT publish\")    \n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
