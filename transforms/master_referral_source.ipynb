{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"CELL1\"></a>\n",
    "## CELL 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-06 11:57:47,221 - core.helpers.session_helper.SessionHelper - INFO - Creating session for dev environment...\n",
      "2019-08-06 11:57:47,246 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating administrator mocks.\n",
      "2019-08-06 11:57:47,284 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating administrator mocks.\n",
      "2019-08-06 11:57:47,285 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pharmaceutical company mocks.\n",
      "2019-08-06 11:57:47,289 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pharmaceutical company mocks.\n",
      "2019-08-06 11:57:47,290 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating brand mocks.\n",
      "2019-08-06 11:57:47,297 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating brand mocks.\n",
      "2019-08-06 11:57:47,298 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating segment mocks.\n",
      "2019-08-06 11:57:47,305 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating segment mocks.\n",
      "2019-08-06 11:57:47,306 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline type mocks.\n",
      "2019-08-06 11:57:47,311 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline type mocks.\n",
      "2019-08-06 11:57:47,313 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline mocks.\n",
      "2019-08-06 11:57:47,318 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline mocks.\n",
      "2019-08-06 11:57:47,319 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline state type mocks.\n",
      "2019-08-06 11:57:47,324 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline state type mocks.\n",
      "2019-08-06 11:57:47,326 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating pipeline state mocks.\n",
      "2019-08-06 11:57:47,331 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating pipeline state mocks.\n",
      "2019-08-06 11:57:47,332 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating tag mocks.\n",
      "2019-08-06 11:57:47,335 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating tag mocks.\n",
      "2019-08-06 11:57:47,336 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation_template mocks.\n",
      "2019-08-06 11:57:47,341 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_template mocks.\n",
      "2019-08-06 11:57:47,342 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating bridge table mocks for transformation_templates <=> tags.\n",
      "2019-08-06 11:57:47,347 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_templates_tags mocks.\n",
      "2019-08-06 11:57:47,348 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation mocks.\n",
      "2019-08-06 11:57:47,354 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation mocks.\n",
      "2019-08-06 11:57:47,355 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating transformation_variables mocks\n",
      "2019-08-06 11:57:47,363 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating transformation_variables mocks.\n",
      "2019-08-06 11:57:47,364 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Generating run_events mocks.\n",
      "2019-08-06 11:57:47,368 - core.helpers.configuration_mocker.ConfigurationMocker - DEBUG - Done generating run_events mocks.\n",
      "2019-08-06 11:57:47,370 - core.helpers.session_helper.SessionHelper - INFO - Done. Created dev session with mock data.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"CELL 1\n",
    "builds and returns a database session\n",
    "local assumes a psql instance in a local docker container\n",
    "only postgres database is supported for configuration_application at this time\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "gets env-based configuration secret\n",
    "returns a session to the configuration db\n",
    "for dev env it pre-populates the database with helper and seed data\n",
    "\"\"\"\n",
    "from core.helpers.session_helper import SessionHelper\n",
    "session = SessionHelper().session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIGURATION - PLEASE TOUCH\n",
    "### <font color=pink>This cell will be off in production as configurations will come from the configuration postgres DB</color>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************ CONFIGURATION - PLEASE TOUCH **************\n",
    "Pipeline Builder configuration: creates configurations from variables specified here!!\n",
    "This cell will be off in production as configurations will come from the configuration postgres DB.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "PIPELINE STATE:\n",
    "\n",
    "raw-->ingest-->master-->enhance-->enrich-->metrics-->dimensional\n",
    "\n",
    "\"\"\"\n",
    "# config vars: this dataset\n",
    "config_pharma = \"sun\" # the pharmaceutical company which owns {brand}\n",
    "config_brand = \"ilumya\" # the brand this pipeline operates on\n",
    "config_state = \"master\" # the state this transform runs in\n",
    "config_name = \"master_referral_source\" # the name of this transform, which is the name of this notebook without .ipynb\n",
    "\n",
    "# input vars: dataset to fetch. \n",
    "# Recall that a contract published to S3 has a key format branch/pharma/brand/state/name\n",
    "input_branch = \"sun-extract-validation\"\n",
    "# None\n",
    "# if None, input_branch is automagically set to your working branch\n",
    "input_pharma = \"sun\"\n",
    "input_brand = \"ilumya\"\n",
    "input_state = \"ingest\"\n",
    "input_name = \"symphony_health_association_ingest_column_mapping\"\n",
    "\n",
    "#This contract defines the base of the output structure of data into S3.\n",
    "#\n",
    "#contract structure in s3: \n",
    "#s3:// {ENV} / {BRANCH} / {PARENT} / {CHILD} / {STATE} / {name of input}\n",
    "#\n",
    "#ENV - environment Must be one of development, uat, production.\n",
    "#Prefixed with integrichain- due to global unique reqirement\n",
    "#BRANCH - the software branch for development this will be the working pull request (eg pr-225)\n",
    "#in uat this will be edge, in production this will be master\n",
    "#PARENT - The top level source identifier\n",
    "#this is generally the customer (and it is aliased as such) but can be IntegriChain for internal sources,\n",
    "#or another aggregator for future-proofing\n",
    "#CHILD - The sub level source identifier, generally the brand (and is aliased as such)\n",
    "#STATE - One of: raw, ingest, master, enhance, enrich, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=orange>SETUP - DON'T TOUCH </font>\n",
    "Populating config mocker based on config parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-06 11:59:49,196 - core.logging - DEBUG - Adding/getting mocks for specified configurations...\n",
      "2019-08-06 11:59:49,221 - core.logging - DEBUG - Done. Creating mock run event and committing results to configuration mocker.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "************ SETUP - DON'T TOUCH **************\n",
    "Populating config mocker based on config parameters...\n",
    "\"\"\"\n",
    "import core.helpers.pipeline_builder as builder\n",
    "\n",
    "ids = builder.build(config_pharma, config_brand, config_state, config_name, session)\n",
    "\"\"\"\n",
    "RETURNS: A list of 2 items: [transformation_id, run_id] where transformation_id corresponds\n",
    "to the configuration created/found for {transformation} and run_id is a randomly generated 6 digit\n",
    "number (to avoid publishing to the same place with the same dataset)\n",
    "\"\"\"\n",
    "transform_id = ids[0]\n",
    "run_id = ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-06 12:00:02,075 - root - DEBUG - Transform Id:6 Run Id:300953\n"
     ]
    }
   ],
   "source": [
    "# debug only\n",
    "# e.g.\n",
    "# 6\n",
    "# 136126\n",
    "log.debug('Transform Id:{} Run Id:{}'.format(transform_id,run_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=orange>SETUP - DON'T TOUCH </font>\n",
    "This section imports data from the configuration database\n",
    "and should not need to be altered or otherwise messed with. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"************ SETUP - DON'T TOUCH **************\n",
    "This section imports data from the configuration database\n",
    "and should not need to be altered or otherwise messed with. \n",
    "~~These are not the droids you are looking for~~\n",
    "\"\"\"\n",
    "from core.constants import BRANCH_NAME, ENV_BUCKET, BATCH_JOB_QUEUE\n",
    "from core.helpers.session_helper import SessionHelper\n",
    "from core.models.configuration import Transformation\n",
    "from dataclasses import dataclass\n",
    "from core.dataset_contract import DatasetContract\n",
    "\n",
    "\n",
    "db_transform = session.query(Transformation).filter(Transformation.id == transform_id).one()\n",
    "\n",
    "@dataclass\n",
    "class DbTransform:\n",
    "    id: int = db_transform.id ## the instance id of the transform in the config app\n",
    "    name: str = db_transform.transformation_template.name ## the transform name in the config app\n",
    "    state: str = db_transform.pipeline_state.pipeline_state_type.name ## the pipeline state, one of raw, ingest, master, enhance, enrich, metrics, dimensional\n",
    "    branch:str = BRANCH_NAME ## the git branch for this execution \n",
    "    brand: str = db_transform.pipeline_state.pipeline.brand.name ## the pharma brand name\n",
    "    pharmaceutical_company: str = db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name # the pharma company name\n",
    "    publish_contract: DatasetContract = DatasetContract(branch=BRANCH_NAME,\n",
    "                            state=db_transform.pipeline_state.pipeline_state_type.name,\n",
    "                            parent=db_transform.pipeline_state.pipeline.brand.pharmaceutical_company.name,\n",
    "                            child=db_transform.pipeline_state.pipeline.brand.name,\n",
    "                            dataset=db_transform.transformation_template.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-06 12:00:32,962 - root - DEBUG - Branch name:DC-580_ReferralSource Env Bucket:ichain-dev Batch Job Queue:dev-core\n"
     ]
    }
   ],
   "source": [
    "#debug only\n",
    "# e.g:\n",
    "#DC-578_PatientStatus\n",
    "#ichain-dev\n",
    "#dev-core\n",
    "log.debug('Branch name:{} Env Bucket:{} Batch Job Queue:{}'.format(BRANCH_NAME,ENV_BUCKET,BATCH_JOB_QUEUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# CORE Cartridge Notebook::[master_referral_source]\n",
    "![CORE Logo](assets/coreLogo.png) \n",
    "\n",
    "---\n",
    "## Keep in Mind\n",
    "Good Transforms Are...\n",
    "- **singular in purpose:** good transforms do one and only one thing, and handle all known cases for that thing. \n",
    "- **repeatable:** transforms should be written in a way that they can be run against the same dataset an infinate number of times and get the same result every time. \n",
    "- **easy to read:** 99 times out of 100, readable, clear code that runs a little slower is more valuable than a mess that runs quickly. \n",
    "- **No 'magic numbers':** if a variable or function is not instantly obvious as to what it is or does, without context, maybe consider renaming it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow - how to use this notebook to make science\n",
    "#### Data Science\n",
    "1. **Document your transform.** Fill out the _description_ cell below describing what it is this transform does; this will appear in the configuration application where Ops will create, configure and update pipelines. \n",
    "1. **Define your config object.** Fill out the _configuration_ cell below the commented-out guide to define the variables you want ops to set in the configuration application (these will populate here for every pipeline). \n",
    "2. **Build your transformation logic.** Use the transformation cell to do that magic that you do. \n",
    "![caution](assets/cautionTape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIGURATION - VARIABLES - PLEASE TOUCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "CONFIGURATION ********* VARIABLES - PLEASE TOUCH ********* \n",
    "This section defines what you expect to get from the configuration application \n",
    "in a single \"transform\" object. Define the vars you need here, and comment inline to the right of them \n",
    "for all-in-one documentation. \n",
    "Engineering will build a production \"transform\" object for every pipeline that matches what you define here.\n",
    "\n",
    "@@@ FORMAT OF THE DATA CLASS IS: @@@ \n",
    "\n",
    "<variable_name>: <data_type> #<comment explaining what the value is to future us>\n",
    "e.g.\n",
    "class Transform(DbTransform):\n",
    "    some_ratio: float\n",
    "    site_name: str\n",
    "\n",
    "~~These ARE the droids you are looking for~~\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "imports\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from core.logging import get_logger\n",
    " \n",
    "class Transform(DbTransform):\n",
    "    '''\n",
    "    YOUR properties go here!!\n",
    "    Variable properties should be assigned to the exact name of\n",
    "    the transformation as it appears in the Jupyter notebook filename.\n",
    "    ''' \n",
    "\n",
    "    col_referral_source: str           \n",
    "    \n",
    "    \n",
    "    def master_referral_source(self,df):\n",
    "        try:        \n",
    "            go = False # assume things are not working YET.\n",
    "           \n",
    "            dffail = pd.DataFrame() # initialize df for fails\n",
    "            \n",
    "            # master data golden referral\n",
    "            #working draft] Gold Domain of referral source\n",
    "            logger.info('try:')\n",
    "            referral_dict = {}\n",
    "            referral_dict[1]='DIRECT'\n",
    "            referral_dict[2]='HUB'\n",
    "            referral_dict[3]='PHARM'\n",
    "            \n",
    "            # store the golden values in a list\n",
    "            referral_list = list(referral_dict.values())           \n",
    "            \n",
    "            # log metadata \n",
    "            \n",
    "            logger.info('Gold Domain List:{}'.format(referral_list))  \n",
    "            \n",
    "            # df in\n",
    "            dfShape = df.shape\n",
    "            logger.info('df in  shape: {} {}'.format(dfShape[0],dfShape[1])) \n",
    "            logger.info('df in {}'.format(df.head()))   \n",
    "            \n",
    "            # am I expecting certain column names? YES \n",
    "            referralColNameExpected = transform.col_referral_source\n",
    "            \n",
    "            logger.info('expecting column name  referral as:{}'.format(referralColNameExpected))\n",
    "            columnNamesArr = df.columns.values.tolist()\n",
    "            logger.info('df column names:{}'.format(columnNamesArr))\n",
    "            \n",
    "            if referralColNameExpected in columnNamesArr:\n",
    " \n",
    "                # apply Upper Case to col values of interest\n",
    "                df[referralColNameExpected]= df[referralColNameExpected].apply(lambda x: x.upper() if x is not None else x)   \n",
    "                # apply strip  to col values of interest\n",
    "                df[referralColNameExpected]= df[referralColNameExpected].apply(lambda x: x.strip() if x is not None else x)\n",
    "                # what fails\n",
    "                dffail = df[~df[referralColNameExpected].isin(referral_list)]\n",
    "                # apply master selection for the column of interest\n",
    "                # what passes\n",
    "                df = df[df[referralColNameExpected].isin(referral_list)]\n",
    "                \n",
    "                # meta data log for what comes out of the function pass and fail df\n",
    "                dfOutSize = df.size\n",
    "                dfOutShape = df.shape\n",
    "                dffailSize = dffail.size\n",
    "                dffailShape = dffail.shape\n",
    "                logger.info('df in   shape: {} {}'.format(dfShape[0],dfShape[1]))                 \n",
    "                logger.info('df pass shape: {} {}'.format(dfOutShape[0],dfOutShape[1]))\n",
    "                logger.info('df fail shape: {} {}'.format(dffailShape[0],dffailShape[1]))\n",
    "                logger.info('df pass {}'.format(df.head()))\n",
    "                logger.info('df fail {}'.format(dffail.head()))   \n",
    "                # end meta data\n",
    "                go = True\n",
    "            else:\n",
    "                go = False # something did not work\n",
    "                logger.exception('expecting column name for referral_source if/else exception raise')\n",
    "                raise Exception(\"master_referral_source try if/else referralColNameExpected in columnNamesArr\")              \n",
    "        except Exception as e:\n",
    "            go = False # something did not work\n",
    "            logger.exception(\"exception:\".format(e))\n",
    "            raise Exception(str(e))\n",
    "        else:\n",
    "            pass\n",
    "        finally:\n",
    "            pass\n",
    "        return df.copy(),dffail.copy(),go\n",
    "                \n",
    "transform = Transform()\n",
    "logger = get_logger(f\"core.transforms.{transform.state}.{transform.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Please place your value assignments for development below*\n",
    "### <font color=pink>This cell will be turned off in production, Engineering will set to pull from the configuration</color>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please place your value assignments for development here!!\n",
    "## This cell will be turned off in production and Engineering will set to pull from the configuration application instead\n",
    "## For the last example, this could look like...\n",
    "## transform.some_ratio = 0.6\n",
    "## transform.site_name = \"WALGREENS\"\n",
    "\n",
    "transform.col_referral_source = 'ref_source'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "What does this transformation do? be specific.\n",
    "\n",
    "![what does your transform do](assets/what.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planned\n",
    "Need a transform that is able to map multiple distinct instances of a referral source to a cleansed referral source data model.\n",
    "\n",
    "Definition of Done:\n",
    "- Collect all unique raw referral source instances\n",
    "- Auto-map as many raw referral source instances to a defined cleansed data model.\n",
    "- Process for identifying and manually mapping where auto-map fails.\n",
    "= Do not publish un-mapped instances. Drop them, give us the ability to triage and map to gold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FETCH DATA - TOUCH, BUT CAREFULLY\n",
    "### <font color=pink>This cell will be turned off in production, as the input_contract will be handled by the pipeline</color>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-06 12:32:39,706 - core.transforms.master.master_referral_source - INFO - FETCH DATA CELL - TOUCH - This cell will be turned off in production, as the input_contract will be handled by the pipeline. \n",
      "2019-08-06 12:32:39,710 - core.dataset_contract.DatasetContract - INFO - Fetching dataframe from s3 location s3://ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping.\n",
      "2019-08-06 12:32:39,937 - urllib3.util.retry - DEBUG - Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n",
      "2019-08-06 12:32:39,939 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): ichain-dev.s3.amazonaws.com:443\n",
      "2019-08-06 12:32:40,413 - urllib3.connectionpool - DEBUG - https://ichain-dev.s3.amazonaws.com:443 \"GET /?prefix=sun-extract-validation%2Fsun%2Filumya%2Fingest%2Fsymphony_health_association_ingest_column_mapping&encoding-type=url HTTP/1.1\" 200 None\n",
      "2019-08-06 12:32:40,436 - urllib3.util.retry - DEBUG - Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n",
      "2019-08-06 12:32:40,438 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): ichain-dev.s3.amazonaws.com:443\n",
      "2019-08-06 12:32:40,753 - urllib3.connectionpool - DEBUG - https://ichain-dev.s3.amazonaws.com:443 \"HEAD /sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping/__metadata_run_id%3D3/1a6ffd3598d442e38fbba66ea85a55a2.parquet HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "************ FETCH DATA - TOUCH, BUT CAREFULLY **************\n",
    "This cell will be turned off in production, as the input_contract will be handled by the pipeline.\n",
    "\"\"\"\n",
    "logger.info(\"FETCH DATA CELL - TOUCH - This cell will be turned off in production, as the input_contract will be handled by the pipeline. \")\n",
    "\n",
    "# for testing / development only !!! picking run id based on core pipeline datasets available\n",
    "run_id = 3\n",
    "\n",
    "if not input_branch:\n",
    "    input_branch = BRANCH_NAME\n",
    "input_contract = DatasetContract(branch=input_branch,\n",
    "                                 state=input_state, \n",
    "                                 parent=input_pharma, \n",
    "                                 child=input_brand, \n",
    "                                 dataset=input_name)\n",
    "run_filter = []\n",
    "run_filter.append(dict(partition=\"__metadata_run_id\", comparison=\"==\", values=[run_id]))\n",
    "# IF YOU HAVE PUBLISHED DATA MULTIPLE TIMES, uncomment the above line and change the int to the run_id to fetch.\n",
    "# Otherwise, you will have duplicate values in your fetched dataset!\n",
    "\n",
    "# bypass/comment out when unit testing individual parquet files\n",
    "df = input_contract.fetch(filters=run_filter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *<font color=grey>unit test development only*</font>\n",
    "*<font color=grey>The next **5** cells will be deleted in production.* </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyarrow.parquet as pq\n",
    "#import s3fs\n",
    "\n",
    "#def pandas_from_parquet_s3(file_path):  \n",
    "#    s3 = s3fs.S3FileSystem()\n",
    "#    df = (\n",
    "#        pq\n",
    "#        .ParquetDataset(file_path, filesystem=s3)\n",
    "#        .read_pandas()\n",
    "#        .to_pandas()\n",
    "#    )    \n",
    "#    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test/development \n",
    "# isolate on individual parquet files\n",
    "#TEST 1\n",
    "#df = pandas_from_parquet_s3('ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping/__metadata_run_id=3/90ca3aa7b0bb4246a281591b013ff54e.parquet')\n",
    "# TEST 2\n",
    "#df = pandas_from_parquet_s3('ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping/__metadata_run_id=3/d7ad974cef284e19aa7b5ac410220b96.parquet')\n",
    "# TEST 3\n",
    "#df = pandas_from_parquet_s3('ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping/__metadata_run_id=3/1a6ffd3598d442e38fbba66ea85a55a2.parquet')\n",
    "# TEST 4\n",
    "#df = pandas_from_parquet_s3('ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping/__metadata_run_id=3/5c00059d9fc04b0e8bc4ce764c50f3fb.parquet')\n",
    "# TEST 5\n",
    "# df = pandas_from_parquet_s3('ichain-dev/sun-extract-validation/sun/ilumya/ingest/symphony_health_association_ingest_column_mapping/__metadata_run_id=3/6eceb7ce59bd4dec8720316b4209b0e3.parquet')\n",
    "# THEN ALL TEST use \n",
    "# then use the FETCH DATA - TOUCH, BUT CAREFULLY CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test/development only\n",
    "# before shot unit testing only\n",
    "# dfSize = df.size\n",
    "# dfShape = df.shape\n",
    "# print('shape: {} {}'.format(dfShape[0],dfShape[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test/development only\n",
    "# needed to see the col(s) of interest\n",
    "#pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DIRECT    15186\n",
       "HUB        8689\n",
       "PHARM       572\n",
       "NaN          10\n",
       "Name: ref_source, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## unit test/development only\n",
    "#df.head()\n",
    "#df[transform.col_referral_source].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>**CALL**</font> THE TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-06 13:19:14,983 - core.transforms.master.master_referral_source - INFO - CALL THE TRANSFORM - execute your transformation - the final output needs to be a variable named final_dataframe\n",
      "2019-08-06 13:19:14,987 - core.transforms.master.master_referral_source - INFO - try:\n",
      "2019-08-06 13:19:14,989 - core.transforms.master.master_referral_source - INFO - Gold Domain List:['DIRECT', 'HUB', 'PHARM']\n",
      "2019-08-06 13:19:14,990 - core.transforms.master.master_referral_source - INFO - df in  shape: 24457 72\n",
      "2019-08-06 13:19:15,026 - core.transforms.master.master_referral_source - INFO - df in          rec_date pharm_code   pharm_npi transtype pharm_transaction_id  \\\n",
      "0  20181024115959    ACCREDO  1346208949       COM   279133432018102401   \n",
      "1  20181025115959    ACCREDO  1346208949       COM   278370982018102502   \n",
      "2  20181029115959    ACCREDO  1346208949       COM   279181482018102903   \n",
      "3  20181102115959    ACCREDO  1346208949       COM   267244982018110204   \n",
      "4  20181106115959    ACCREDO  1346208949       COM   160618142018110605   \n",
      "\n",
      "  trans_seq ref_source        ref_date program_id pharmacy_id  ...  \\\n",
      "0         0     DIRECT  20181019120000       None    27913343  ...   \n",
      "1         0     DIRECT  20181022120000       None    27837098  ...   \n",
      "2         0     DIRECT  20181024120000       None    27918148  ...   \n",
      "3         0     DIRECT  20181030120000       None    26724498  ...   \n",
      "4         0     DIRECT  20181102120000       None    16061814  ...   \n",
      "\n",
      "  copay_assist_amount oth_payer_amt xfer_pharmname msa_patient_id  \\\n",
      "0                None          None           None           None   \n",
      "1                None          None           None           None   \n",
      "2                None          None           None           None   \n",
      "3                None          None           None           None   \n",
      "4                None          None           None           None   \n",
      "\n",
      "  msa_patient_bmap __metadata_run_timestamp __metadata_app_version  \\\n",
      "0            NNNNV      2019-07-01 13:25:07                 0.0.11   \n",
      "1            NNNVV      2019-07-01 13:25:07                 0.0.11   \n",
      "2            NNNVV      2019-07-01 13:25:07                 0.0.11   \n",
      "3            NNNVV      2019-07-01 13:25:07                 0.0.11   \n",
      "4            NNNVV      2019-07-01 13:25:07                 0.0.11   \n",
      "\n",
      "                          __metadata_output_contract  \\\n",
      "0  s3://ichain-dev/sun-extract-validation/sun/ilu...   \n",
      "1  s3://ichain-dev/sun-extract-validation/sun/ilu...   \n",
      "2  s3://ichain-dev/sun-extract-validation/sun/ilu...   \n",
      "3  s3://ichain-dev/sun-extract-validation/sun/ilu...   \n",
      "4  s3://ichain-dev/sun-extract-validation/sun/ilu...   \n",
      "\n",
      "  __metadata_transform_timestamp __metadata_run_id  \n",
      "0            2019-07-01 13:35:22                 3  \n",
      "1            2019-07-01 13:35:22                 3  \n",
      "2            2019-07-01 13:35:22                 3  \n",
      "3            2019-07-01 13:35:22                 3  \n",
      "4            2019-07-01 13:35:22                 3  \n",
      "\n",
      "[5 rows x 72 columns]\n",
      "2019-08-06 13:19:15,031 - core.transforms.master.master_referral_source - INFO - expecting column name  referral as:ref_source\n",
      "2019-08-06 13:19:15,035 - core.transforms.master.master_referral_source - INFO - df column names:['rec_date', 'pharm_code', 'pharm_npi', 'transtype', 'pharm_transaction_id', 'trans_seq', 'ref_source', 'ref_date', 'program_id', 'pharmacy_id', 'pat_last_name', 'pat_first_name', 'pat_dob', 'pat_gender', 'pat_addr1', 'pat_addr2', 'pat_city', 'pat_state', 'pat_zip', 'dx1_code', 'dx2_code', 'status_date', 'status_code', 'sub_status', 'pres_last_name', 'pres_first_name', 'pres_addr1', 'pres_addr2', 'pres_city', 'pres_state', 'pres_zip', 'pres_phone', 'pres_npi', 'pres_dea', 'facility_name', 'rxdate', 'rxnumber', 'rxrefills', 'rxfill', 'refill_remaining', 'prev_disp', 'rx_ndc_number', 'medication', 'quantity', 'day_supply', 'ship_date', 'ship_carrier', 'shiptracking_num', 'ship_location', 'ship_address', 'ship_city', 'ship_state', 'ship_zip', 'has_medical', 'primary_coverage_type', 'primary_payer_name', 'primary_payer_type', 'secondary_coverage_type', 'secondary_payer_name', 'secondary_payer_type', 'plan_paid_amt', 'pat_copay', 'copay_assist_amount', 'oth_payer_amt', 'xfer_pharmname', 'msa_patient_id', 'msa_patient_bmap', '__metadata_run_timestamp', '__metadata_app_version', '__metadata_output_contract', '__metadata_transform_timestamp', '__metadata_run_id']\n",
      "2019-08-06 13:19:15,089 - core.transforms.master.master_referral_source - INFO - df in   shape: 24457 72\n",
      "2019-08-06 13:19:15,090 - core.transforms.master.master_referral_source - INFO - df pass shape: 24447 72\n",
      "2019-08-06 13:19:15,093 - core.transforms.master.master_referral_source - INFO - df fail shape: 10 72\n",
      "2019-08-06 13:19:15,104 - core.transforms.master.master_referral_source - INFO - df in count of values:DIRECT    15186\n",
      "HUB        8689\n",
      "PHARM       572\n",
      "Name: ref_source, dtype: int64\n",
      "2019-08-06 13:19:15,120 - core.transforms.master.master_referral_source - INFO - df pass          rec_date pharm_code   pharm_npi transtype pharm_transaction_id  \\\n",
      "0  20181024115959    ACCREDO  1346208949       COM   279133432018102401   \n",
      "1  20181025115959    ACCREDO  1346208949       COM   278370982018102502   \n",
      "2  20181029115959    ACCREDO  1346208949       COM   279181482018102903   \n",
      "3  20181102115959    ACCREDO  1346208949       COM   267244982018110204   \n",
      "4  20181106115959    ACCREDO  1346208949       COM   160618142018110605   \n",
      "\n",
      "  trans_seq ref_source        ref_date program_id pharmacy_id  ...  \\\n",
      "0         0     DIRECT  20181019120000       None    27913343  ...   \n",
      "1         0     DIRECT  20181022120000       None    27837098  ...   \n",
      "2         0     DIRECT  20181024120000       None    27918148  ...   \n",
      "3         0     DIRECT  20181030120000       None    26724498  ...   \n",
      "4         0     DIRECT  20181102120000       None    16061814  ...   \n",
      "\n",
      "  copay_assist_amount oth_payer_amt xfer_pharmname msa_patient_id  \\\n",
      "0                None          None           None           None   \n",
      "1                None          None           None           None   \n",
      "2                None          None           None           None   \n",
      "3                None          None           None           None   \n",
      "4                None          None           None           None   \n",
      "\n",
      "  msa_patient_bmap __metadata_run_timestamp __metadata_app_version  \\\n",
      "0            NNNNV      2019-07-01 13:25:07                 0.0.11   \n",
      "1            NNNVV      2019-07-01 13:25:07                 0.0.11   \n",
      "2            NNNVV      2019-07-01 13:25:07                 0.0.11   \n",
      "3            NNNVV      2019-07-01 13:25:07                 0.0.11   \n",
      "4            NNNVV      2019-07-01 13:25:07                 0.0.11   \n",
      "\n",
      "                          __metadata_output_contract  \\\n",
      "0  s3://ichain-dev/sun-extract-validation/sun/ilu...   \n",
      "1  s3://ichain-dev/sun-extract-validation/sun/ilu...   \n",
      "2  s3://ichain-dev/sun-extract-validation/sun/ilu...   \n",
      "3  s3://ichain-dev/sun-extract-validation/sun/ilu...   \n",
      "4  s3://ichain-dev/sun-extract-validation/sun/ilu...   \n",
      "\n",
      "  __metadata_transform_timestamp __metadata_run_id  \n",
      "0            2019-07-01 13:35:22                 3  \n",
      "1            2019-07-01 13:35:22                 3  \n",
      "2            2019-07-01 13:35:22                 3  \n",
      "3            2019-07-01 13:35:22                 3  \n",
      "4            2019-07-01 13:35:22                 3  \n",
      "\n",
      "[5 rows x 72 columns]\n",
      "2019-08-06 13:19:15,142 - core.transforms.master.master_referral_source - INFO - df fail    rec_date pharm_code pharm_npi transtype pharm_transaction_id trans_seq  \\\n",
      "6      None       None      None      None                 None      None   \n",
      "7      None       None      None      None                 None      None   \n",
      "8      None       None      None      None                 None      None   \n",
      "9      None       None      None      None                 None      None   \n",
      "10     None       None      None      None                 None      None   \n",
      "\n",
      "   ref_source ref_date program_id pharmacy_id  ... copay_assist_amount  \\\n",
      "6        None     None       None        None  ...                None   \n",
      "7        None     None       None        None  ...                None   \n",
      "8        None     None       None        None  ...                None   \n",
      "9        None     None       None        None  ...                None   \n",
      "10       None     None       None        None  ...                None   \n",
      "\n",
      "   oth_payer_amt xfer_pharmname msa_patient_id msa_patient_bmap  \\\n",
      "6           None           None           None             None   \n",
      "7           None           None           None             None   \n",
      "8           None           None           None             None   \n",
      "9           None           None           None             None   \n",
      "10          None           None           None             None   \n",
      "\n",
      "   __metadata_run_timestamp __metadata_app_version  \\\n",
      "6       2019-07-01 13:25:07                 0.0.11   \n",
      "7       2019-07-01 13:25:07                 0.0.11   \n",
      "8       2019-07-01 13:25:07                 0.0.11   \n",
      "9       2019-07-01 13:25:07                 0.0.11   \n",
      "10      2019-07-01 13:25:07                 0.0.11   \n",
      "\n",
      "                           __metadata_output_contract  \\\n",
      "6   s3://ichain-dev/sun-extract-validation/sun/ilu...   \n",
      "7   s3://ichain-dev/sun-extract-validation/sun/ilu...   \n",
      "8   s3://ichain-dev/sun-extract-validation/sun/ilu...   \n",
      "9   s3://ichain-dev/sun-extract-validation/sun/ilu...   \n",
      "10  s3://ichain-dev/sun-extract-validation/sun/ilu...   \n",
      "\n",
      "   __metadata_transform_timestamp __metadata_run_id  \n",
      "6             2019-07-01 13:35:26                 3  \n",
      "7             2019-07-01 13:35:26                 3  \n",
      "8             2019-07-01 13:35:26                 3  \n",
      "9             2019-07-01 13:35:26                 3  \n",
      "10            2019-07-01 13:35:26                 3  \n",
      "\n",
      "[5 rows x 72 columns]\n",
      "2019-08-06 13:19:15,172 - core.transforms.master.master_referral_source - INFO - CALL THE TRANSFORM -  go no go = GO\n"
     ]
    }
   ],
   "source": [
    "### Use the variables above to execute your transformation.\n",
    "### the final output needs to be a variable named final_dataframe\n",
    "logger.info(\"CALL THE TRANSFORM - execute your transformation - the final output needs to be a variable named final_dataframe\")\n",
    "\n",
    "final_dataframe, final_fail, go = transform.master_referral_source(df)\n",
    "\n",
    "if go==True:\n",
    "    logger.info(\"CALL THE TRANSFORM -  go no go = GO\")\n",
    "elif go==False:\n",
    "    logger.info(\"CALL THE TRANSFORM -  go no go = NO go\")\n",
    "else:\n",
    "    go=False\n",
    "    logger.info(\"CALL THE TRANSFORM -  go no go = unknown make it NO go\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *<font color=grey>unittest python*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_ut_shape (__main__.TestNotebook) ... FAIL\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_ut_shape (__main__.TestNotebook)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-33-f771f8d9cda6>\", line 13, in test_ut_shape\n",
      "    self.assertEqual(ut_shape(final_dataframe,df),True)\n",
      "AssertionError: False != True\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.008s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f9c6d6197f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "def ut_shape(final_dataframe,df):\n",
    "    \"\"\"\n",
    "    assertion will change based on coding state\n",
    "    \"\"\"\n",
    "    return final_dataframe.shape == df.shape\n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "    \n",
    "    def test_ut_shape(self):\n",
    "        \n",
    "        self.assertEqual(ut_shape(final_dataframe,df),True)\n",
    "                \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# for development only\n",
    "unittest.main(argv=[''], verbosity= 2, exit=False)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6      None\n",
       "7      None\n",
       "8      None\n",
       "9      None\n",
       "10     None\n",
       "712    None\n",
       "713    None\n",
       "714    None\n",
       "715    None\n",
       "716    None\n",
       "Name: ref_source, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# untit test/development only look at the fails\n",
    "#final_fail.head()\n",
    "#final_fail[transform.col_referral_source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untit test/development only look at the pass(es)\n",
    "#final_dataframe.head()\n",
    "#final_dataframe[transform.col_referral_source]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **publish**\n",
    "### Writing to S3\n",
    "Invoke the `publish()` command to write to a given contract. Some things to know:\n",
    "- To invoke publish a contract must be at the grain of dataset. This is because file names will be set by the dataframe=\\>parquet conversion. \n",
    "- publish only accepts a pandas dataframe.\n",
    "- publish does not allow for timedelta data types at this time (this is missing functionality in pyarrow).\n",
    "- publish handles partitioning the data as per contract, creating file paths, and creating the binary parquet files in S3, as well as the needed metadata. <br>\n",
    "**- by default, all datasets include a single partition, \\_\\_metadata\\_run\\_id, the RunEvent ID of an executed pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-06 13:02:39,203 - core.transforms.master.master_referral_source - INFO - PUBLISH - that's it - its a GO - just provide the final dataframe to the var final_dataframe and we take it from there\n",
      "2019-08-06 13:02:39,205 - core.dataset_contract.DatasetContract - INFO - Publishing dataframe to s3 location s3://ichain-dev/dc-580_referralsource/sun/ilumya/master/master_referral_source with run ID 3.\n",
      "2019-08-06 13:02:39,214 - core.dataset_contract.DatasetContract - DEBUG - Publishing dataframe to Redshift Spectrum database ichain_core to schema.table                 data_core.sun_ilumya_master_referral_source...\n",
      "2019-08-06 13:02:39,217 - s3parq.publish_parq - DEBUG - Found redshift parameters. Checking validity of params...\n",
      "2019-08-06 13:02:39,222 - s3parq.publish_parq - DEBUG - Checking redshift params are correctly formatted\n",
      "2019-08-06 13:02:39,227 - s3parq.publish_parq - DEBUG - Done checking redshift params\n",
      "2019-08-06 13:02:39,232 - s3parq.publish_parq - DEBUG - Redshift parameters valid. Opening Session helper.\n",
      "2019-08-06 13:02:39,312 - urllib3.util.retry - DEBUG - Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n",
      "2019-08-06 13:02:39,314 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): iam.amazonaws.com:443\n",
      "2019-08-06 13:02:39,397 - urllib3.connectionpool - DEBUG - https://iam.amazonaws.com:443 \"POST / HTTP/1.1\" 200 523\n",
      "2019-08-06 13:02:39,431 - urllib3.util.retry - DEBUG - Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n",
      "2019-08-06 13:02:39,433 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): redshift.us-east-1.amazonaws.com:443\n",
      "2019-08-06 13:02:39,541 - urllib3.connectionpool - DEBUG - https://redshift.us-east-1.amazonaws.com:443 \"POST / HTTP/1.1\" 200 480\n",
      "2019-08-06 13:02:39,585 - s3parq.publish_redshift - INFO - Running query to create schema: CREATE EXTERNAL SCHEMA IF NOT EXISTS data_core                 FROM DATA CATALOG                 database 'ichain_core'                 iam_role 'arn:aws:iam::265991248033:role/mySpectrumRole';\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "(psycopg2.ProgrammingError) permission denied for database ichain_core\n\n[SQL: CREATE EXTERNAL SCHEMA IF NOT EXISTS data_core                 FROM DATA CATALOG                 database 'ichain_core'                 iam_role 'arn:aws:iam::265991248033:role/mySpectrumRole';]\n(Background on this error at: http://sqlalche.me/e/f405)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1243\u001b[0m                     self.dialect.do_execute(\n\u001b[0;32m-> 1244\u001b[0;31m                         \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m                     )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProgrammingError\u001b[0m: permission denied for database ichain_core\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-08701c48828c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PUBLISH - that's it - its a GO - just provide the final dataframe to the var final_dataframe and we take it from there\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_contract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_dataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PUBLISH -  go no go = NO go -  so DONT publish\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/src/app/core/dataset_contract.py\u001b[0m in \u001b[0;36mpublish\u001b[0;34m(self, dataframe, run_id, session, publish_to_redshift)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mdataframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mpartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mredshift_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mredshift_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/s3parq/publish_parq.py\u001b[0m in \u001b[0;36mpublish\u001b[0;34m(bucket, key, partitions, dataframe, redshift_params)\u001b[0m\n\u001b[1;32m    252\u001b[0m         )\n\u001b[1;32m    253\u001b[0m         \u001b[0msession_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_session_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mpublish_redshift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredshift_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'schema_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredshift_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'db_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredshift_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iam_role'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_helper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Schema {redshift_params['schema_name']} created. Creating table {redshift_params['table_name']}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/s3parq/publish_redshift.py\u001b[0m in \u001b[0;36mcreate_schema\u001b[0;34m(schema_name, db_name, iam_role, session_helper)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Running query to create schema: {new_schema_query}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_schema_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_helper\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSessionHelper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/session.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, clause, params, mapper, bind, **kw)\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m         return self._connection_for_bind(bind, close_with_result=True).execute(\n\u001b[0;32m-> 1269\u001b[0;31m             \u001b[0mclause\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m         )\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, object_, *multiparams, **params)\u001b[0m\n\u001b[1;32m    986\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectNotExecutableError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sqlalchemy/sql/elements.py\u001b[0m in \u001b[0;36m_execute_on_connection\u001b[0;34m(self, connection, multiparams, params)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_on_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_execution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_clauseelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectNotExecutableError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_clauseelement\u001b[0;34m(self, elem, multiparams, params)\u001b[0m\n\u001b[1;32m   1105\u001b[0m             \u001b[0mdistilled_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0mcompiled_sql\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             \u001b[0mdistilled_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m         )\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1246\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m             self._handle_dbapi_exception(\n\u001b[0;32m-> 1248\u001b[0;31m                 \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m             )\n\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewraise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mshould_wrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1466\u001b[0;31m                 \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlalchemy_exception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1467\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m                 \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mraise_from_cause\u001b[0;34m(exception, exc_info)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0mcause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexc_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexception\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc_tb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb, cause)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1242\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m                     self.dialect.do_execute(\n\u001b[0;32m-> 1244\u001b[0;31m                         \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m                     )\n\u001b[1;32m   1246\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute_no_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProgrammingError\u001b[0m: (psycopg2.ProgrammingError) permission denied for database ichain_core\n\n[SQL: CREATE EXTERNAL SCHEMA IF NOT EXISTS data_core                 FROM DATA CATALOG                 database 'ichain_core'                 iam_role 'arn:aws:iam::265991248033:role/mySpectrumRole';]\n(Background on this error at: http://sqlalche.me/e/f405)"
     ]
    }
   ],
   "source": [
    "## that's it - just provide the final dataframe to the var final_dataframe and we take it from there\n",
    "if go==True:\n",
    "    logger.info(\"PUBLISH - that's it - its a GO - just provide the final dataframe to the var final_dataframe and we take it from there\")\n",
    "    transform.publish_contract.publish(final_dataframe, run_id, session)\n",
    "elif go==False:\n",
    "    logger.info(\"PUBLISH -  go no go = NO go -  so DONT publish\")\n",
    "else:\n",
    "    go=False\n",
    "    logger.info(\"PUBLISH -  go no go = unknown make it NO go - so DONT publish\")    \n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
